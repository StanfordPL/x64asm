Copyright 2013 eric schkufza														
														
Licensed under the Apache License, Version 2.0 (the "License");														
you may not use this file except in compliance with the License.														
You may obtain a copy of the License at														
														
    http://www.apache.org/licenses/LICENSE-2.0														
														
Unless required by applicable law or agreed to in writing, software														
distributed under the License is distributed on an "AS IS" BASIS,														
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.														
See the License for the specific language governing permissions and														
limitations under the License.														
														
Opcode	Instruction	Op/En	Properties	Implicit Read	Implicit Write	Implicit Undef	Useful	Protected	64-bit Mode	Compat/32-bit-Legacy Mode	CPUID Feature Flags	AT&T Mnemonic	Preferred 	Description
														
37	AAA								I	V				ASCII adjust AL after addition.
														
D5 0A	AAD								I	V				ASCII adjust AX before division.
D5 ib	AAD imm8								I	V				Adjust AX before division to number base imm8.
														
D4 0A	AAM								I	V				ASCII adjust AX after multiply.
D4 ib	AAM imm8								I	V				Adjust AX after multiply to number base imm8.
														
3F	AAS								I	V				ASCII adjust AL after subtraction.
														
14 ib	ADC AL, imm8	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcb		Add with carry imm8 to AL
15 iw	ADC AX, imm16	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcw		Add with carry imm16 to AX.
15 id	ADC EAX, imm32	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcl		Add with carry imm32 to EAX.
REX.W+ 15 id	ADC RAX, imm32	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcq		Add with carry imm32 sign extended to 64- bits to RAX.
80 /2 ib	ADC r/m8, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcb		Add with carry imm8 to r/m8.
REX+ 80 /2 ib	ADC r/m8, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcb		Add with carry imm8 to r/m8.
81 /2 iw	ADC r/m16, imm16	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcw		Add with carry imm16 to r/m16.
81 /2 id	ADC r/m32, imm32	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcl		Add with CF imm32 to r/m32.
REX.W+ 81 /2 id	ADC r/m64, imm32	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcq		Add with CF imm32 sign extended to 64-bits to r/m64.
83 /2 ib	ADC r/m16, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcw	YES	Add with CF sign-extended imm8 to r/m16.
83 /2 ib	ADC r/m32, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcl	YES	Add with CF sign-extended imm8 into r/m32.
REX.W+ 83 /2 ib	ADC r/m64, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcq	YES	Add with CF sign-extended imm8 into r/m64.
10 /r	ADC r/m8, r8	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcb	YES	Add with carry byte register to r/m8.
REX+ 10 /r	ADC r/m8, r8	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcb	YES	Add with carry byte register to r/m8.
11 /r	ADC r/m16, r16	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcw	YES	Add with carry r16 to r/m16.
11 /r	ADC r/m32, r32	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcl	YES	Add with CF r32 to r/m32.
REX.W+ 11 /r	ADC r/m64, r64	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcq	YES	Add with CF r64 to r/m64.
12 /r	ADC r8, r/m8	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcb		Add with carry r/m8 to byte register.
REX+ 12 /r	ADC r8, r/m8	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcb		Add with carry r/m64 to byte register.
13 /r	ADC r16, r/m16	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcw		Add with carry r/m16 to r16.
13 /r	ADC r32, r/m32	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		adcl		Add with CF r/m32 to r32.
REX.W+ 13 /r	ADC r64, r/m64	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		adcq		Add with CF r/m64 to r64.
														
04 ib	ADD AL, imm8	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addb		Add imm8 to AL.
05 iw	ADD AX, imm16 	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addw		Add imm16 to AX.
05 id	ADD EAX, imm32 	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addl		Add imm32 to EAX.
REX.W+ 05 id 	ADD RAX, imm32 	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addq		Add imm32 sign-extended to 64-bits to RAX.  
80 /0 ib	ADD r/m8, imm8 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addb		Add imm8 to r/m8.
REX+ 80 /0 ib 	ADD r/m8, imm8 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addb		Add sign-extended imm8 to r/m64.
81 /0 iw	ADD r/m16, imm16 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addw		Add imm16 to r/m16.
81 /0 id	ADD r/m32, imm32 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addl		Add imm32 to r/m32.
REX.W+ 81 /0 id	ADD r/m64, imm32	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addq		Add imm32 sign-extended to 64-bits to r/m64.
83 /0 ib	ADD r/m16, imm8 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addw	YES	Add sign-extended imm8 to r/m16. 
83 /0 ib	ADD r/m32, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addl	YES	Add sign-extended imm8 to r/m32. 
REX.W+ 83 /0 ib 	ADD r/m64, imm8 	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addq	YES	Add sign-extended imm8 to r/m64. 
00 /r 	ADD r/m8, r8 	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addb	YES	Add r8 to r/m8.
REX+ 00 /r	ADD r/m8, r8	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addb	YES	Add r8 to r/m8. 
01 /r	ADD r/m16, r16 	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addw	YES	Add r16 to r/m16. 
01 /r	ADD r/m32, r32 	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addl	YES	Add r32 to r/m32. 
REX.W+ 01 /r 	ADD r/m64, r64 	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addq	YES	Add r64 to r/m64. 
02 /r 	ADD r8, r/m8 	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addb		Add r/m8 to r8. 
REX+ 02 /r	ADD r8, r/m8	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addb		Add r/m8 to r8. 
03 /r	ADD r16, r/m16 	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addw		Add r/m16 to r16. 
03 /r	ADD r32, r/m32 	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		addl		Add r/m32 to r32. 
REX.W+ 03 /r	ADD r64, r/m64	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		addq		Add r/m64 to r64.
														
66 0F 58 /r	ADDPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	addpd		Add packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 58 /r	VADDPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vaddpd		Add packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 58 /r	VADDPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vaddpd		Add packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
0F 58 /r	ADDPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	addps		Add packed single-precision floating-point values from xmm2/m128 to xmm1 and stores result in xmm1.
VEX.NDS.128.0F.WIG 58 /r	VADDPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vaddps		Add packed single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 58 /r	VADDPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vaddps		Add packed single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
F2 0F 58 /r	ADDSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	addsd		Add the low double-precision floating-point value from xmm2/m64 to xmm1.
VEX.NDS.LIG.F2.0F.WIG 58 /r	VADDSD xmm1, xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vaddsd		Add the low double-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
														
F3 0F 58 /r	ADDSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	addss		Add the low single-precision floating-point value from xmm2/m32 to xmm1.
VEX.NDS.LIG.F3.0F.WIG 58 /r	VADDSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vaddss		Add the low single-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
														
66 0F D0 /r	ADDSUBPD xmm1, xmm2/m128	RM	RW, R						V	V	PNI	addsubpd		Add/subtract double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG D0 /r	VADDSUBPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vaddsubpd		Add/subtract packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG D0 /r	VADDSUBPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vaddsubpd		Add / subtract packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
F2 0F D0 /r	ADDSUBPS xmm1, xmm2/m128	RM	RW, R						V	V	PNI	addsubps		Add/subtract single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG D0 /r	VADDSUBPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vaddsubps		Add/subtract single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.F2.0F.WIG D0 /r	VADDSUBPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vaddsubps		Add / subtract single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
66 0F 38 DE /r	AESDEC xmm1, xmm2/m128	RM	RW, R						V	V	AES	aesdec		Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DE /r	VAESDEC xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AES AVX	vaesdec		Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
														
66 0F 38 DF /r	AESDECLAST xmm1, xmm2/m128	RM	RW, R						V	V	AES	aesdeclast		Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DF /r	VAESDECLAST xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AES AVX	vaesdeclast		Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
														
66 0F 38 DC /r	AESENC xmm1, xmm2/m128	RM	RW, R						V	V	AES	aesenc		Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DC /r	VAESENC xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AES AVX	vaesenc		Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1.
														
66 0F 38 DD /r	AESENCLAST xmm1, xmm2/m128	RM	RW, R						V	V	AES	aesenclast		Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
VEX.NDS.128.66.0F38.WIG DD /r	VAESENCLAST xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AES AVX	vaesenclast		Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1.
														
66 0F 38 DB /r	AESIMC xmm1, xmm2/m128	RM	W, R						V	V	AES	aesimc		Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
VEX.128.66.0F38.WIG DB /r	VAESIMC xmm1, xmm2/m128	RM	Z, R						V	V	AES AVX	vaesimc		Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
														
66 0F 3A DF /r ib	AESKEYGENASSIST xmm1, xmm2/m128, imm8	RMI	W, R, R						V	V	AES	aeskeygenassist		Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
VEX.128.66.0F3A.WIG DF /r ib	VAESKEYGENASSIST xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AES AVX	vaeskeygenassist		Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
														
24 ib	AND AL, imm8	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andb		AL AND imm8.
25 iw	AND AX, imm16	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andw		AX AND imm16.
25 id	AND EAX, imm32	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andl		EAX AND imm32.
REX.W+ 25 id	AND RAX, imm32	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andq		RAX AND imm32 sign-extended to 64-bits.
80 /4 ib	AND r/m8, imm8	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andb		r/m8 AND imm8.
REX+ 80 /4 ib	AND r/m8, imm8	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andb		r/m8 AND imm8.
81 /4 iw	AND r/m16, imm16	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andw		r/m16 AND imm16.
81 /4 id	AND r/m32, imm32	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andl		r/m32 AND imm32.
REX.W+ 81 /4 id	AND r/m64, imm32	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andq		r/m64 AND imm32 sign extended to 64-bits.
83 /4 ib	AND r/m16, imm8	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andw	YES	r/m16 AND imm8 (sign-extended).
83 /4 ib	AND r/m32, imm8	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andl	YES	r/m32 AND imm8 (sign-extended).
REX.W+ 83 /4 ib	AND r/m64, imm8	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andq	YES	r/m64 AND imm8 (sign-extended).
20 /r	AND r/m8, r8	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andb	YES	r/m8 AND r8.
REX+ 20 /r	AND r/m8, r8	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andb	YES	r/m64 AND r8 (sign-extended).
21 /r	AND r/m16, r16	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andw	YES	r/m16 AND r16.
21 /r	AND r/m32, r32	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andl	YES	r/m32 AND r32.
REX.W+ 21 /r	AND r/m64, r64	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andq	YES	r/m64 AND r32.
22 /r	AND r8, r/m8	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andb		r8 AND r/m8.
REX+ 22 /r	AND r8, r/m8	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andb		r/m64 AND r8 (sign-extended).
23 /r	AND r16, r/m16	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andw		r16 AND r/m16.
23 /r	AND r32, r/m32	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		andl		r32 AND r/m32.
REX.W+ 23 /r	AND r64, r/m64	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		andq		r64 AND r/m64.
														
VEX.NDS.LZ.0F38.W0 F2 /r	ANDN r32a, r32b, r/m32	RVM	W, R, R		E.SF E.ZF E.OF E.CF	E.AF E.PF			V	V	BMI1	andnl		Bitwise AND of inverted r32b with r/m32, store result in r32a
VEX.NDS.LZ.0F38.W1 F2 /r	ANDN r64a, r64b, r/m64	RVM	W, R, R		E.SF E.ZF E.OF E.CF	E.AF E.PF			V	NE	BMI1	andnq		Bitwise AND of inverted r64b with r/m64, store result in r64a
														
66 0F 54 /r	ANDPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	andpd		Return the bitwise logical AND of packed double-precision floating-point values in xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 54 /r	VANDPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vandpd		Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 54 /r	VANDPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vandpd		Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/mem.
														
0F 54 /r	ANDPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	andps		Bitwise logical AND of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 54 /r	VANDPS xmm1,xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vandps		Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 54 /r	VANDPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vandps		Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/mem.
														
66 0F 55 /r	ANDNPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	andnpd		Bitwise logical AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 55 /r	VANDNPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vandnpd		Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 55 /r	VANDNPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vandnpd		Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/mem.
														
0F 55 /r	ANDNPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	andnps		Bitwise logical AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 55 /r	VANDNPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vandnps		Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 55 /r	VANDNPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vandnps		Return the bitwise logical AND NOT of packed single-precision floating-point values in ymm2 and ymm3/mem.
														
63 /r	ARPL r/m16, r16						YES	NO	NE	V				Adjust RPL of r/m16 to not less than RPL of r16.
														
66 0F 3A 0D /r ib	BLENDPD xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE4_1	blendpd		Select packed DP-FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG 0D /r ib	VBLENDPD xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vblendpd		Select packed double-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1. 
VEX.NDS.256.66.0F3A.WIG 0D /r ib	VBLENDPD ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vblendpd		Select packed double-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
														
VEX.NDS.LZ.0F38.W0 F7 /r	BEXTR r32a, r/m32, r32b	RMV	W, R, R		E.ZF E.OF E.CF	E.AF E.SF E.PF			V	V	BMI1	bextrl		Contiguous bitwise extract from r/m32 using r32b as control; store result in r32a.
VEX.NDS.LZ.0F38.W1 F7 /r	BEXTR r64a, r/m64, r64b	RMV	W, R, R		E.ZF E.OF E.CF	E.AF E.SF E.PF			V	NE	BMI1	bextrq		Contiguous bitwise extract from r/m64 using r64b as control; store result in r64a
														
66 0F 3A 0C /r ib	BLENDPS xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE4_1	blendps		Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG 0C /r ib	VBLENDPS xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vblendps		Select packed single-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VEX.NDS.256.66.0F3A.WIG 0C /r ib	VBLENDPS ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vblendps		Select packed single-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
														
66 0F 38 15 /r	BLENDVPD xmm1, xmm2/m128, <XMM0>	RM0	RW, R, R						V	V	SSE4_1	blendvpd		Select packed DP FP values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1.
VEX.NDS.128.66.0F3A.W0 4B /r /is4	VBLENDVPD xmm1, xmm2, xmm3/m128, xmm4	RVMR	Z, R, R, R						V	V	AVX	vblendvpd		Conditionally copy double-precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4.
VEX.NDS.256.66.0F3A.W0 4B /r /is4	VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4	RVMR	W, R, R, R						V	V	AVX	vblendvpd		Conditionally copy double-precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4.
														
66 0F 38 14 /r	BLENDVPS xmm1, xmm2/m128, <XMM0>	RM0	RW, R, R						V	V	SSE4_1	blendvps		Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1.
VEX.NDS.128.66.0F3A.W0 4A /r /is4	VBLENDVPS xmm1, xmm2, xmm3/m128, xmm4	RVMR	Z, R, R, R						V	V	AVX	vblendvps		Conditionally copy single-precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4.
VEX.NDS.256.66.0F3A.W0 4A /r /is4	VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4	RVMR	W, R, R, R						V	V	AVX	vblendvps		Conditionally copy single-precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4.
														
VEX.NDD.LZ.0F38.W0 F3 /3	BLSI r32, r/m32	VM	RW, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	V	BMI1	blsil		Extract lowest set bit from r/m32 and set that bit in r32.
VEX.NDD.LZ.0F38.W1 F3 /3	BLSI r64, r/m64	VM	RW, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	NE	BMI1	blsiq		Extract lowest set bit from r/m64, and set that bit in r64.
														
VEX.NDD.LZ.0F38.W0 F3 /2	BLSMSK r32, r/m32	VM	RW, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	V	BMI1	blsmskl		Set all lower bits in r32 to "1" starting from bit 0 to lowest set bit in r/m32
VEX.NDD.LZ.0F38.W1 F3 /2	BLSMSK r64, r/m64	VM	RW, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	NE	BMI1	blsmskq		Set all lower bits in r64 to "1" starting from bit 0 to lowest set bit in r/m64
														
VEX.NDD.LZ.0F38.W0 F3 /1	BLSR r32, r/m32	VM	W, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	V	BMI1	blsrl		Reset lowest set bit of r/m32, keep all other bits of r/m32 and write result to r32.
VEX.NDD.LZ.0F38.W1 F3 /1	BLSR r64, r/m64	VM	W, R		E.ZF E.SF E.OF e.cf	E.AF E.PF			V	NE	BMI1	blsrq		Reset lowest set bit of r/m64, keep all other bits of r/m64 and write result to r64.
														
62 /r	BOUND r16, m16&16								I	V				Check if r16 (array index) is within bounds specified by m16&16.
62 /r	BOUND r32, m32&32								I	V				Check if r32 (array index) is within bounds specified by m16&16.
														
0F BC /r	BSF r16, r/m16	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			a	V		bsfw		Bit scan forward on r/m16.
0F BC /r	BSF r32, r/m32	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			V	V		bsfl		Bit scan forward on r/m32.
REX.W+ 0F BC	BSF r64, r/m64	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			V	NE		bsfq		Bit scan forward on r/m64.
														
0F BD /r	BSR r16, r/m16	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			V	V		bsrw		Bit scan reverse on r/m16.
0F BD /r	BSR r32, r/m32	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			V	V		bsrl		Bit scan reverse on r/m32.
REX.W+ 0F BD	BSR r64, r/m64	RM	uw, R		E.ZF	E.CF E.OF E.SF E.AF E.PF			V	NE		bsrq		Bit scan reverse on r/m64.
														
0F C8 +rd	BSWAP r32	O	RW						V	V		bswap		Reverses the byte order of a 32-bit register.
REX.W+ 0F C8 +rd	BSWAP r64	O	RW						V	NE		bswap		Reverses the byte order of a 64-bit register.
														
0F A3	BT r/m16, r16	MR	R, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btw		Store selected bit in CF flag.
0F A3	BT r/m32, r32	MR	R, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btl		Store selected bit in CF flag.
REX.W+ 0F A3	BT r/m64, r64	MR	R, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btq		Store selected bit in CF flag.
0F BA /4 ib	BT r/m16, imm8	MI	R, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btw		Store selected bit in CF flag.
0F BA /4 ib	BT r/m32, imm8	MI	R, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btl		Store selected bit in CF flag.
REX.W+ 0F BA /4 ib	BT r/m64, imm8	MI	R, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btq		Store selected bit in CF flag.
														
0F BB	BTC r/m16, r16	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btcw		Store selected bit in CF flag and complement.
0F BB	BTC r/m32, r32	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btcl		Store selected bit in CF flag and complement.
REX.W+ 0F BB	BTC r/m64, r64	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btcq		Store selected bit in CF flag and complement.
0F BA /7 ib	BTC r/m16, imm8	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btcw		Store selected bit in CF flag and complement.
0F BA /7 ib	BTC r/m32, imm8	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btcl		Store selected bit in CF flag and complement.
REX.W+ 0F BA /7 ib	BTC r/m64, imm8	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btcq		Store selected bit in CF flag and complement.
														
0F B3	BTR r/m16, r16	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btrw		Store selected bit in CF flag and clear.
0F B3	BTR r/m32, r32 	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btrl		Store selected bit in CF flag and clear.
REX.W+ 0F B3	BTR r/m64, r64 	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btrq		Store selected bit in CF flag and clear.
0F BA /6 ib	BTR r/m16, imm8 	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btrw		Store selected bit in CF flag and clear.
0F BA /6 ib	BTR r/m32, imm8 	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btrl		Store selected bit in CF flag and clear.
REX.W+ 0F BA /6 ib	BTR r/m64, imm8	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btrq		Store selected bit in CF flag and clear.
														
0F AB	BTS r/m16, r16 	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btsw		Store selected bit in CF flag and set.
0F AB	BTS r/m32, r32 	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btsl		Store selected bit in CF flag and set.
REX.W+ 0F AB	BTS r/m64, r64 	MR	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btsq		Store selected bit in CF flag and set.
0F BA /5 ib	BTS r/m16, imm8 	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btsw		Store selected bit in CF flag and set.
0F BA /5 ib	BTS r/m32, imm8 	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	V		btsl		Store selected bit in CF flag and set.
REX.W+ 0F BA /5 ib	BTS r/m64, imm8	MI	RW, R		E.CF	E.OF E.SF E.AF E.PF			V	NE		btsq		Store selected bit in CF flag and set.
														
VEX.NDS.LZ.0F38.W0 F5 /r	BZHI r32a, r/m32, r32b	RMV	W, R, R		E.ZF E.CF E.SF E.OF	E.AF E.PF			V	V	BMI2	bzhil		Zero bits in r/m32 starting with the position in r32b, write result to r32a.
VEX.NDS.LZ.0F38.W1 F5 /r	BZHI r64a, r/m64, r64b	RMV	W, R, R		E.ZF E.CF E.SF E.OF	E.AF E.PF			V	NE	BMI2	bzhiq		Zero bits in r/m64 starting with the position in r64b, write result to r64a.
														
E8 cw 	CALL rel16 								NS	V				Call near, relative, displacement relative to next instruction.
E8 cd	CALL rel32	D	R	???	???	???			V	V		callq		Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode.
FF /2	CALL r/m16								NE	V				Call near, absolute indirect, address given in r/m16.
FF /2	CALL r/m32								NE	V				Call near, absolute indirect, address given in r/m32.
FF /2	CALL r/m64	M	R	???	???	???			V	NE		callq		Call near, absolute indirect, address given in r/m64.
9A cd 	CALL ptr16:16 								I	V				Call far, absolute, address given in operand.
9A cp 	CALL ptr16:32 								I	V				Call far, absolute, address given in operand.
FF /3	CALL m16:16	M	R	???	???	???			V	V		callq		Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16- bit offset from far pointer referenced in the instruction.
FF /3	CALL m16:32	M	R	???	???	???			V	V		callq		In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction.
REX.W+ FF /3	CALL m16:64	M	R	???	???	???			V	NE		callq		In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction.
														
PREF.66+ 98	CBW	NP		AL	AX				V	V		cbtw		AX = sign-extend of AL.
98	CWDE	NP		AX	EAX				V	V		cwtl		EAX = sign-extend of AX.
REX.W+ 98	CDQE	NP		EAX	RAX				V	NE		cltq		RAX = sign-extend of EAX.
														
F8	CLC	NP			E.CF				V	V		clc		Clear CF flag.
														
FC	CLD	NP			E.DF				V	V		cld		Clear DF flag.
														
0F AE /7	CLFLUSH m8	M	I						V	V	CLFLUSH	clflush		Flushes cache line containing m8.
														
FA	CLI	NP		E.VM E.IOPL E.VIP	e.if e.vif				V	V		cli		Clear interrupt flag; interrupts disabled when interrupt flag cleared.
														
0F 06	CLTS						NO	YES	V	V				Clears TS flag in CR0.
														
F5	CMC	NP		E.CF	E.CF				V	V		cmc		Complement CF flag.
														
0F 47 /r	CMOVA r16, r/m16 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovaw		Move if above (CF=0 and ZF=0).
0F 47 /r 	CMOVA r32, r/m32 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmoval		Move if above (CF=0 and ZF=0).
REX.W+ 0F 47 /r 	CMOVA r64, r/m64 	RM	w, R	E.CF E.ZF					V	NE	CMOV	cmovaq		Move if above (CF=0 and ZF=0).
0F 43 /r	CMOVAE r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovaew		Move if above or equal (CF=0).
0F 43 /r 	CMOVAE r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovael		Move if above or equal (CF=0).
REX.W+ 0F 43 /r 	CMOVAE r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovaeq		Move if above or equal (CF=0).
0F 42 /r	CMOVB r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovbw		Move if below (CF=1).
0F 42 /r 	CMOVB r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovbl		Move if below (CF=1).
REX.W+ 0F 42 /r 	CMOVB r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovbq		Move if below (CF=1).
0F 46 /r	CMOVBE r16, r/m16 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovbew		Move if below or equal (CF=1 or ZF=1).
0F 46 /r 	CMOVBE r32, r/m32 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovbel		Move if below or equal (CF=1 or ZF=1).
REX.W+ 0F 46 /r 	CMOVBE r64, r/m64	RM	w, R	E.CF E.ZF					V	NE	CMOV	cmovbeq		Move if below or equal (CF=1 or ZF=1).
0F 42 /r	CMOVC r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovcw		Move if carry (CF=1).
0F 42 /r 	CMOVC r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovcl		Move if carry (CF=1).
REX.W+ 0F 42 /r 	CMOVC r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovcq		Move if carry (CF=1).
0F 44 /r	CMOVE r16, r/m16 	RM	w, R	E.ZF					V	V	CMOV	cmovew		Move if equal (ZF=1).
0F 44 /r 	CMOVE r32, r/m32 	RM	w, R	E.ZF					V	V	CMOV	cmovel		Move if equal (ZF=1).
REX.W+ 0F 44 /r 	CMOVE r64, r/m64 	RM	w, R	E.ZF					V	NE	CMOV	cmoveq		Move if equal (ZF=1).
0F 4F /r	CMOVG r16, r/m16 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovgw		Move if greater (ZF=0 and SF=OF).
0F 4F /r 	CMOVG r32, r/m32 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovgl		Move if greater (ZF=0 and SF=OF).
REX.W+ 0F 4F /r 	CMOVG r64, r/m64 	RM	w, R	E.ZF E.SF E.OF					V	NE	CMOV	cmovgq		Move if greater (ZF=0 and SF=OF).
0F 4D /r	CMOVGE r16, r/m16 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovgew		Move if greater or equal (SF=OF).
0F 4D /r 	CMOVGE r32, r/m32 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovgel		Move if greater or equal (SF=OF).
REX.W+ 0F 4D /r 	CMOVGE r64, r/m64 	RM	w, R	E.SF E.OF					V	NE	CMOV	cmovgeq		Move if greater or equal (SF=OF).
0F 4C /r	CMOVL r16, r/m16 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovlw		Move if less (SF != OF).
0F 4C /r 	CMOVL r32, r/m32 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovll		Move if less (SF!= OF).
REX.W+ 0F 4C /r 	CMOVL r64, r/m64 	RM	w, R	E.SF E.OF					V	NE	CMOV	cmovlq		Move if less (SF!= OF).
0F 4E /r	CMOVLE r16, r/m16 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovlew		Move if less or equal (ZF=1 or SF!= OF).
0F 4E /r 	CMOVLE r32, r/m32 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovlel		Move if less or equal (ZF=1 or SF!= OF).
REX.W+ 0F 4E /r 	CMOVLE r64, r/m64 	RM	w, R	E.ZF E.SF E.OF					V	NE	CMOV	cmovleq		Move if less or equal (ZF=1 or SF!= OF).
0F 46 /r	CMOVNA r16, r/m16 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovnaw		Move if not above (CF=1 or ZF=1).
0F 46 /r 	CMOVNA r32, r/m32 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovnal		Move if not above (CF=1 or ZF=1).
REX.W+ 0F 46 /r	CMOVNA r64, r/m64 	RM	w, R	E.CF E.ZF					V	NE	CMOV	cmovnaq		Move if not above (CF=1 or ZF=1).
0F 42 /r	CMOVNAE r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovnaew		Move if not above or equal (CF=1).
0F 42 /r 	CMOVNAE r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovnael		Move if not above or equal (CF=1).
REX.W+ 0F 42 /r 	CMOVNAE r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovnaeq		Move if not above or equal (CF=1).
0F 43 /r	CMOVNB r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovnbw		Move if not below (CF=0).
0F 43 /r 	CMOVNB r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovnbl		Move if not below (CF=0).
REX.W+ 0F 43 /r 	CMOVNB r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovnbq		Move if not below (CF=0).
0F 47 /r	CMOVNBE r16, r/m16	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovnbew		Move if not below or equal (CF=0 and ZF=0).
0F 47 /r 	CMOVNBE r32, r/m32 	RM	w, R	E.CF E.ZF					V	V	CMOV	cmovnbel		Move if not below or equal (CF=0 and ZF=0).
REX.W+ 0F 47 /r 	CMOVNBE r64, r/m64 	RM	w, R	E.CF E.ZF					V	NE	CMOV	cmovnbeq		Move if not below or equal (CF=0 and ZF=0).
0F 43 /r	CMOVNC r16, r/m16 	RM	w, R	E.CF					V	V	CMOV	cmovncw		Move if not carry (CF=0).
0F 43 /r 	CMOVNC r32, r/m32 	RM	w, R	E.CF					V	V	CMOV	cmovncl		Move if not carry (CF=0).
REX.W+ 0F 43 /r 	CMOVNC r64, r/m64 	RM	w, R	E.CF					V	NE	CMOV	cmovncq		Move if not carry (CF=0).
0F 45 /r	CMOVNE r16, r/m16 	RM	w, R	E.ZF					V	V	CMOV	cmovnew		Move if not equal (ZF=0).
0F 45 /r 	CMOVNE r32, r/m32 	RM	w, R	E.ZF					V	V	CMOV	cmovnel		Move if not equal (ZF=0).
REX.W+ 0F 45 /r 	CMOVNE r64, r/m64 	RM	w, R	E.ZF					V	NE	CMOV	cmovneq		Move if not equal (ZF=0).
0F 4E /r	CMOVNG r16, r/m16 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovngw		Move if not greater (ZF=1 or SF!= OF).
0F 4E /r 	CMOVNG r32, r/m32 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovngl		Move if not greater (ZF=1 or SF!= OF).
REX.W+ 0F 4E /r 	CMOVNG r64, r/m64 	RM	w, R	E.ZF E.SF E.OF					V	NE	CMOV	cmovngq		Move if not greater (ZF=1 or SF!= OF).
0F 4C /r	CMOVNGE r16, r/m16 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovngew		Move if not greater or equal (SF!= OF).
0F 4C /r 	CMOVNGE r32, r/m32 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovngel		Move if not greater or equal (SF!= OF).
REX.W+ 0F 4C /r 	CMOVNGE r64, r/m64 	RM	w, R	E.SF E.OF					V	NE	CMOV	cmovngeq		Move if not greater or equal (SF!= OF).
0F 4D /r	CMOVNL r16, r/m16 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovnlw		Move if not less (SF=OF).
0F 4D /r 	CMOVNL r32, r/m32 	RM	w, R	E.SF E.OF					V	V	CMOV	cmovnll		Move if not less (SF=OF).
REX.W+ 0F 4D /r 	CMOVNL r64, r/m64 	RM	w, R	E.SF E.OF					V	NE	CMOV	cmovnlq		Move if not less (SF=OF).
0F 4F /r	CMOVNLE r16, r/m16 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovnlew		Move if not less or equal (ZF=0 and SF=OF).
0F 4F /r 	CMOVNLE r32, r/m32 	RM	w, R	E.ZF E.SF E.OF					V	V	CMOV	cmovnlel		Move if not less or equal (ZF=0 and SF=OF).
REX.W+ 0F 4F /r 	CMOVNLE r64, r/m64 	RM	w, R	E.ZF E.SF E.OF					V	NE	CMOV	cmovnleq		Move if not less or equal (ZF=0 and SF=OF).
0F 41 /r	CMOVNO r16, r/m16 	RM	w, R	E.OF					V	V	CMOV	cmovnow		Move if not overflow (OF=0).
0F 41 /r 	CMOVNO r32, r/m32 	RM	w, R	E.OF					V	V	CMOV	cmovnol		Move if not overflow (OF=0).
REX.W+ 0F 41 /r 	CMOVNO r64, r/m64 	RM	w, R	E.OF					V	NE	CMOV	cmovnoq		Move if not overflow (OF=0).
0F 4B /r	CMOVNP r16, r/m16 	RM	w, R	E.PF					V	V	CMOV	cmovnpw		Move if not parity (PF=0).
0F 4B /r 	CMOVNP r32, r/m32 	RM	w, R	E.PF					V	V	CMOV	cmovnpl		Move if not parity (PF=0).
REX.W+ 0F 4B /r 	CMOVNP r64, r/m64 	RM	w, R	E.PF					V	NE	CMOV	cmovnpq		Move if not parity (PF=0).
0F 49 /r	CMOVNS r16, r/m16 	RM	w, R	E.SF					V	V	CMOV	cmovnsw		Move if not sign (SF=0).
0F 49 /r 	CMOVNS r32, r/m32 	RM	w, R	E.SF					V	V	CMOV	cmovnsl		Move if not sign (SF=0).
REX.W+ 0F 49 /r 	CMOVNS r64, r/m64 	RM	w, R	E.SF					V	NE	CMOV	cmovnsq		Move if not sign (SF=0).
0F 45 /r	CMOVNZ r16, r/m16 	RM	w, R	E.ZF					V	V	CMOV	cmovnzw		Move if not zero (ZF=0).
0F 45 /r 	CMOVNZ r32, r/m32 	RM	w, R	E.ZF					V	V	CMOV	cmovnzl		Move if not zero (ZF=0).
REX.W+ 0F 45 /r 	CMOVNZ r64, r/m64 	RM	w, R	E.ZF					V	NE	CMOV	cmovnzq		Move if not zero (ZF=0).
0F 40 /r	CMOVO r16, r/m16 	RM	w, R	E.OF					V	V	CMOV	cmovow		Move if overflow (OF=1).
0F 40 /r 	CMOVO r32, r/m32 	RM	w, R	E.OF					V	V	CMOV	cmovol		Move if overflow (OF=1).
REX.W+ 0F 40 /r 	CMOVO r64, r/m64 	RM	w, R	E.OF					V	NE	CMOV	cmovoq		Move if overflow (OF=1).
0F 4A /r	CMOVP r16, r/m16 	RM	w, R	E.PF					V	V	CMOV	cmovpw		Move if parity (PF=1).
0F 4A /r 	CMOVP r32, r/m32 	RM	w, R	E.PF					V	V	CMOV	cmovpl		Move if parity (PF=1).
REX.W+ 0F 4A /r 	CMOVP r64, r/m64 	RM	w, R	E.PF					V	NE	CMOV	cmovpq		Move if parity (PF=1).
0F 4A /r	CMOVPE r16, r/m16 	RM	w, R	E.PF					V	V	CMOV	cmovpew		Move if parity even (PF=1).
0F 4A /r 	CMOVPE r32, r/m32 	RM	w, R	E.PF					V	V	CMOV	cmovpel		Move if parity even (PF=1).
REX.W+ 0F 4A /r	CMOVPE r64, r/m64	RM	w, R	E.PF					V	NE	CMOV	cmovpeq		Move if parity even (PF=1).
0F 4B /r	CMOVPO r16, r/m16 	RM	w, R	E.PF					V	V	CMOV	cmovpow		Move if parity odd (PF=0).
0F 4B /r	CMOVPO r32, r/m32 	RM	w, R	E.PF					V	V	CMOV	cmovpol		Move if parity odd (PF=0).
REX.W+ 0F 4B /r 	CMOVPO r64, r/m64 	RM	w, R	E.PF					V	NE	CMOV	cmovpoq		Move if parity odd (PF=0).
0F 48 /r	CMOVS r16, r/m16 	RM	w, R	E.SF					V	V	CMOV	cmovsw		Move if sign (SF=1).
0F 48 /r	CMOVS r32, r/m32 	RM	w, R	E.SF					V	V	CMOV	cmovsl		Move if sign (SF=1).
REX.W+ 0F 48 /r 	CMOVS r64, r/m64 	RM	w, R	E.SF					V	NE	CMOV	cmovsq		Move if sign (SF=1).
0F 44 /r	CMOVZ r16, r/m16 	RM	w, R	E.ZF					V	V	CMOV	cmovzw		Move if zero (ZF=1).
0F 44 /r	CMOVZ r32, r/m32 	RM	w, R	E.ZF					V	V	CMOV	cmovzl		Move if zero (ZF=1).
REX.W+ 0F 44 /r	CMOVZ r64, r/m64	RM	w, R	E.ZF					V	NE	CMOV	cmovzq		Move if zero (ZF=1).
														
3C ib	CMP AL, imm8 	I	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpb		Compare imm8 with AL.
3D iw	CMP AX, imm16	I	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpw		Compare imm16 with AX.
3D id	CMP EAX, imm32 	I	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpl		Compare imm32 with EAX.
REX.W+ 3D id	CMP RAX, imm32	I	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpq		Compare imm32 sign-extended to 64-bits with RAX.
80 /7 ib	CMP r/m8, imm8 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpb		Compare imm8 with r/m8.
REX+ 80 /7 ib	CMP r/m8, imm8	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpb		Compare imm8 with r/m8.
81 /7 iw	CMP r/m16, imm16 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpw		Compare imm16 with r/m16.
81 /7 id	CMP r/m32, imm32 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpl		Compare imm32 with r/m32.
REX.W+ 81 /7 id	CMP r/m64, imm32	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpq		Compare imm32 sign-extended to 64-bits with r/m64.
83 /7 ib	CMP r/m16, imm8 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpw	YES	Compare imm8 with r/m16.
83 /7 ib	CMP r/m32, imm8 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpl	YES	Compare imm8 with r/m32.
REX.W+ 83 /7 ib	CMP r/m64, imm8 	MI	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpq	YES	Compare imm8 with r/m64.
38 /r	CMP r/m8, r8 	MR	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpb	YES	Compare r8 with r/m8.
REX+ 38 /r	CMP r/m8, r8	MR	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpb	YES	Compare r8 with r/m8.
39 /r	CMP r/m16, r16 	MR	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpw	YES	Compare r16 with r/m16.
39 /r	CMP r/m32, r32 	MR	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpl	YES	Compare r32 with r/m32.
REX.W+ 39 /r	CMP r/m64, r64 	MR	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpq	YES	Compare r64 with r/m64.
3A /r	CMP r8, r/m8 	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpb		Compare r/m8 with r8.
REX+ 3A /r	CMP r8, r/m8	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpb		Compare r/m8 with r8.
3B /r	CMP r16, r/m16 	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpw		Compare r/m16 with r16.
3B /r	CMP r32, r/m32 	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpl		Compare r/m32 with r32.
REX.W+ 3B /r	CMP r64, r/m64	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpq		Compare r/m64 with r64.
														
66 0F C2 /r ib	CMPPD xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE2	cmppd		Compare packed double-precision floating- point values in xmm2/m128 and xmm1 using imm8 as comparison predicate.
VEX.NDS.128.66.0F.WIG C2 /r ib	VCMPPD xmm1, xmm2, xmm3/m128, imm8	RVMI	W, R, R, R						V	V	AVX	vcmppd		Compare packed double-precision floating- point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VEX.NDS.256.66.0F.WIG C2 /r ib	VCMPPD ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vcmppd		Compare packed double-precision floating- point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
														
0F C2 /r ib	CMPPS xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE	cmpps		Compare packed single-precision floating- point values in xmm2/mem and xmm1 using imm8 as comparison predicate.
VEX.NDS.128.0F.WIG C2 /r ib	VCMPPS xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vcmpps		Compare packed single-precision floating- point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VEX.NDS.256.0F.WIG C2 /r ib	VCMPPS ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vcmpps		Compare packed single-precision floating- point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
														
A6	CMPS m8, m8	NP	I, I	E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmps		For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI to byte at address (R|E)DI. The status flags are set accordingly.
A7	CMPS m16, m16	NP	I, I	E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmps		For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
A7	CMPS m32, m32	NP	I, I	E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmps		For legacy mode, compare dword at address DS:(E)SI at dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI at dword at address (R|E)DI. The status flags are set accordingly.
REX.W+ A7 	CMPS m64, m64 	NP	I, I	E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmps		Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
A6	CMPSB	NP		E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpsb		For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64- bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly.
PREF.66+ A7	CMPSW	NP		E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpsw		For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64- bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
A7	CMPSD	NP		E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		cmpsl		For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly.
REX.W+ A7	CMPSQ	NP		E.DF ESI rsi EDI rdi	E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		cmpsq		Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
														
F2 0F C2 /r ib	CMPSD xmm1, xmm2/m64, imm8	RMI	RW, R, R						V	V	SSE2	cmpsd		Compare low double-precision floating-point value in xmm2/m64 and xmm1 using imm8 as comparison predicate.
VEX.NDS.LIG.F2.0F.WIG C2 /r ib	VCMPSD xmm1, xmm2, xmm3/m64, imm8	RVMI	W, R, R, R						V	V	AVX	vcmpsd		Compare low double precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate.
														
F3 0F C2 /r ib	CMPSS xmm1, xmm2/m32, imm8	RMI	RW, R, R						V	V	SSE	cmpss		Compare low single-precision floating-point value in xmm2/m32 and xmm1 using imm8 as comparison predicate.
VEX.NDS.LIG.F3.0F.WIG C2 /r ib	VCMPSS xmm1, xmm2, xmm3/m32, imm8	RVMI	W, R, R, R						V	V	AVX	vcmpss		Compare low single precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate.
														
0F B0 /r	CMPXCHG r/m8, r8	MR	Rw, R	AL	al E.ZF E.CF E.PF E.AF E.SF E.OF				V	V		cmpxchgb		Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
REX+ 0F B0 /r	CMPXCHG r/m8,r8	MR	Rw, R	AL	al E.ZF E.CF E.PF E.AF E.SF E.OF				V	NE		cmpxchgb		Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
0F B1 /r	CMPXCHG r/m16, r16	MR	Rw, R	AX	ax E.ZF E.CF E.PF E.AF E.SF E.OF				V	V		cmpxchgw		Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX.
0F B1 /r	CMPXCHG r/m32, r32	MR	Rw, R	EAX	eax E.ZF E.CF E.PF E.AF E.SF E.OF				V	V		cmpxchgl		Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX.
REX.W+ 0F B1 /r	CMPXCHG r/m64, r64	MR	Rw, R	RAX	Rax E.ZF E.CF E.PF E.AF E.SF E.OF				V	NE		cmpxchgq		Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX.
														
0F C7 /1	CMPXCHG8B m64	M	Rw	EAX EDX	eax ebx ecx edx E.ZF				V	V	CX8	cmpxchg8b		Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX.
REX.W+ 0F C7 /1	CMPXCHG16B m128	M	Rw	RAX RDX	rax rbx rcx rdx E.ZF				V	NE	CX16	cmpxchg16b		Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX.
														
66 0F 2F /r	COMISD xmm1, xmm2/m64	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	SSE2	comisd		Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VEX.LIG.66.0F.WIG 2F /r	VCOMISD xmm1, xmm2/m64	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	AVX	vcomisd		Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
														
0F 2F /r	COMISS xmm1, xmm2/m32	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	SSE	comiss		Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VEX.LIG.0F.WIG 2F /r	VCOMISS xmm1, xmm2/m32	RM	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	AVX	vcomiss		Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
														
0F A2	CPUID	NP		EAX ecx	EAX EBX ECX EDX				V	V		cpuid		Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well).
														
F2 0F 38 F0 /r	CRC32 r32, r/m8	RM	RW, R						V	V		crc32b		Accumulate CRC32 on r/m8.
F2 REX+ 0F 38 F0 /r	CRC32 r32, r/m8	RM	RW, R						V	NE		crc32b		Accumulate CRC32 on r/m8.
F2 0F 38 F1 /r	CRC32 r32, r/m16	RM	RW, R						V	V		crc32w		Accumulate CRC32 on r/m16.
F2 0F 38 F1 /r	CRC32 r32, r/m32	RM	RW, R						V	V		crc32l		Accumulate CRC32 on r/m32.
F2 REX.W+ 0F 38 F0 /r	CRC32 r64, r/m8	RM	RW, R						V	NE		crc32b		Accumulate CRC32 on r/m8.
F2 REX.W+ 0F 38 F1 /r	CRC32 r64, r/m64	RM	RW, R						V	NE		crc32q		Accumulate CRC32 on r/m64.
														
F3 0F E6	CVTDQ2PD xmm1, xmm2/m64	RM	W, R						V	V	SSE2	cvtdq2pd		Convert two packed signed doubleword integers from xmm2/m128 to two packed double-precision floating-point values in xmm1.
VEX.128.F3.0F.WIG E6 /r	VCVTDQ2PD xmm1, xmm2/m64	RM	Z, R						V	V	AVX	vcvtdq2pd		Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1.
VEX.256.F3.0F.WIG E6 /r	VCVTDQ2PD ymm1, ymm2/m128	RM	W, R						V	V	AVX	vcvtdq2pd		Convert four packed signed doubleword integers from ymm2/mem to four packed double-precision floating-point values in ymm1.
														
0F 5B /r	CVTDQ2PS xmm1, xmm2/m128	RM	W, R						V	V	SSE2	cvtdq2ps		Convert four packed signed doubleword integers from xmm2/m128 to four packed single-precision floating-point values in xmm1.
VEX.128.0F.WIG 5B /r	VCVTDQ2PS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vcvtdq2ps		Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1.
VEX.256.0F.WIG 5B /r	VCVTDQ2PS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vcvtdq2ps		Convert eight packed signed doubleword integers from ymm2/mem to eight packed single-precision floating-point values in ymm1.
														
F2 0F E6	CVTPD2DQ xmm1, xmm2/m128	RM	W, R	M.RC					V	V	SSE2	cvtpd2dq		Convert two packed double-precision floating- point values from xmm2/m128 to two packed signed doubleword integers in xmm1.
VEX.128.F2.0F.WIG E6 /r	VCVTPD2DQ xmm1, xmm2/m128	RM	Z, R	M.RC					V	V	AVX	vcvtpd2dqx		Convert two packed double-precision floating- point values in xmm2/mem to two signed doubleword integers in xmm1.
VEX.256.F2.0F.WIG E6 /r	VCVTPD2DQ xmm1, ymm2/m256	RM	Z, R	M.RC					V	V	AVX	vcvtpd2dq		Convert four packed double-precision floating- point values in ymm2/mem to four signed doubleword integers in xmm1.
														
66 0F 2D /r	CVTPD2PI mm, xmm/m128	RM	W, R	M.RC					V	V		cvtpd2pi		Convert two packed double-precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm.
														
66 0F 5A /r	CVTPD2PS xmm1, xmm2/m128	RM	W, R	M.RC					V	V	SSE2	cvtpd2ps		Convert two packed double-precision floating- point values in xmm2/m128 to two packed single-precision floating-point values in xmm1.
VEX.128.66.0F.WIG 5A /r	VCVTPD2PS xmm1, xmm2/m128	RM	Z, R	M.RC					V	V	AVX	vcvtpd2ps		Convert two packed double-precision floating- point values in xmm2/mem to two single- precision floating-point values in xmm1.
VEX.256.66.0F.WIG 5A /r	VCVTPD2PS xmm1, ymm2/m256	RM	Z, R	M.RC					V	V	AVX	vcvtpd2ps		Convert four packed double-precision floating- point values in ymm2/mem to four single- precision floating-point values in xmm1.
														
66 0F 2A /r	CVTPI2PD xmm, mm/m64	RM	W, R	M.RC					V	V		cvtpi2pd		Convert two packed signed doubleword integers from mm/mem64 to two packed double-precision floating-point values in xmm.
														
0F 2A /r	CVTPI2PS xmm, mm/m64	RM	W, R	M.RC					V	V		cvtpi2ps		Convert two signed doubleword integers from mm/m64 to two single-precision floating-point values in xmm.
														
66 0F 5B /r	CVTPS2DQ xmm1, xmm2/m128	RM	W, R	M.RC					V	V	SSE2	cvtps2dq		Convert four packed single-precision floating- point values from xmm2/m128 to four packed signed doubleword integers in xmm1.
VEX.128.66.0F.WIG 5B /r	VCVTPS2DQ xmm1, xmm2/m128	RM	Z, R	M.RC					V	V	AVX	vcvtps2dq		Convert four packed single precision floating- point values from xmm2/mem to four packed signed doubleword values in xmm1.
VEX.256.66.0F.WIG 5B /r	VCVTPS2DQ ymm1, ymm2/m256	RM	W, R	M.RC					V	V	AVX	vcvtps2dq		Convert eight packed single precision floating- point values from ymm2/mem to eight packed signed doubleword values in ymm1.
														
0F 5A /r	CVTPS2PD xmm1, xmm2/m64	RM	W, R	M.RC					V	V	SSE2	cvtps2pd		Convert two packed single-precision floating- point values in xmm2/m64 to two packed double-precision floating-point values in xmm1.
VEX.128.0F.WIG 5A /r	VCVTPS2PD xmm1, xmm2/m64	RM	Z, R	M.RC					V	V	AVX	vcvtps2pd		Convert two packed single-precision floating- point values in xmm2/mem to two packed double-precision floating-point values in xmm1.
VEX.256.0F.WIG 5A /r	VCVTPS2PD ymm1, xmm2/m128	RM	W, R	M.RC					V	V	AVX	vcvtps2pd		Convert four packed single-precision floating- point values in xmm2/mem to four packed double-precision floating-point values in ymm1.
														
0F 2D /r	CVTPS2PI mm, xmm/m64	RM	W, R	M.RC					V	V		cvtps2pi		Convert two packed single-precision floating- point values from xmm/m64 to two packed signed doubleword integers in mm.
														
F2 0F 2D /r	CVTSD2SI r32, xmm/m64	RM	W, R	M.RC					V	V	SSE2	cvtsd2si		Convert one double-precision floating-point value from xmm/m64 to one signed doubleword integer r32.
F2 REX.W+ 0F 2D /r	CVTSD2SI r64, xmm/m64	RM	W, R	M.RC					V	NE	SSE2	cvtsd2si		Convert one double-precision floating-point value from xmm/m64 to one signed quadword integer sign-extended into r64.
VEX.LIG.F2.0F.W0 2D /r	VCVTSD2SI r32, xmm1/m64	RM	W, R	M.RC					V	V	AVX	vcvtsd2sil		Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
VEX.LIG.F2.0F.W1 2D /r	VCVTSD2SI r64, xmm1/m64	RM	W, R	M.RC					V	NE	AVX	vcvtsd2siq		Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer sign-extended into r64.
														
F2 0F 5A /r	CVTSD2SS xmm1, xmm2/m64	RM	W, R	M.RC					V	V	SSE2	cvtsd2ss		Convert one double-precision floating-point value in xmm2/m64 to one single-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.WIG 5A /r	VCVTSD2SS xmm1,xmm2, xmm3/m64	RVM	Z, R, R	M.RC					V	V	AVX	vcvtsd2ss		Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2.
														
F2 0F 2A /r	CVTSI2SD xmm, r/m32	RM	W, R	M.RC					V	V	SSE2	cvtsi2sdl		Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm.
F2 REX.W+ 0F 2A /r	CVTSI2SD xmm, r/m64	RM	W, R	M.RC					V	NE	SSE2	cvtsi2sdq		Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm.
VEX.NDS.LIG.F2.0F.W0 2A /r	VCVTSI2SD xmm1, xmm2, r/m32	RVM	Z, R, R	M.RC					V	V	AVX	vcvtsi2sdl		Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.W1 2A /r	VCVTSI2SD xmm1, xmm2, r/m64	RVM	Z, R, R	M.RC					V	NE	AVX	vcvtsi2sdq		Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
														
F3 0F 2A /r	CVTSI2SS xmm, r/m32	RM	W, R	M.RC					V	V	SSE	cvtsi2ssl		Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm.
F3 REX.W+ 0F 2A /r	CVTSI2SS xmm, r/m64	RM	W, R	M.RC					V	NE	SSE	cvtsi2ssq		Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm.
VEX.NDS.LIG.F3.0F.W0 2A /r	VCVTSI2SS xmm1, xmm2, r/m32	RVM	Z, R, R	M.RC					V	V	AVX	vcvtsi2ssl		Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.W1 2A /r	VCVTSI2SS xmm1, xmm2, r/m64	RVM	Z, R, R	M.RC					V	NE	AVX	vcvtsi2ssq		Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
														
F3 0F 5A /r	CVTSS2SD xmm1, xmm2/m32	RM	W, R	M.RC					V	V	SSE2	cvtss2sd		Convert one single-precision floating-point value in xmm2/m32 to one double-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.WIG 5A /r	VCVTSS2SD xmm1, xmm2, xmm3/m32	RVM	Z, R, R	M.RC					V	V	AVX	vcvtss2sd		Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2.
														
F3 0F 2D /r	CVTSS2SI r32, xmm/m32	RM	W, R						V	V	SSE	cvtss2sil		Convert one single-precision floating-point value from xmm/m32 to one signed doubleword integer in r32.
F3 REX.W+ 0F 2D /r	CVTSS2SI r64, xmm/m32	RM	W, R						V	NE	SSE	cvtss2siq		Convert one single-precision floating-point value from xmm/m32 to one signed quadword integer in r64.
VEX.LIG.F3.0F.W0 2D /r	VCVTSS2SI r32, xmm1/m32	RM	W, R						V	V	AVX	vcvtss2sil		Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
VEX.LIG.F3.0F.W1 2D /r	VCVTSS2SI r64, xmm1/m32	RM	W, R						V	NE	AVX	vcvtss2siq		Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
														
66 0F E6	CVTTPD2DQ xmm1, xmm2/m128	RM	W, R						V	V	SSE2	cvttpd2dq		Convert two packed double-precision floating- point values from xmm2/m128 to two packed signed doubleword integers in xmm1 using truncation.
VEX.128.66.0F.WIG E6 /r	VCVTTPD2DQ xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vcvttpd2dq		Convert two packed double-precision floating- point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
VEX.256.66.0F.WIG E6 /r	VCVTTPD2DQ xmm1, ymm2/m256	RM	Z, R						V	V	AVX	vcvttpd2dq		Convert four packed double-precision floating- point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation.
														
66 0F 2C /r	CVTTPD2PI mm, xmm/m128	RM	W, R						V	V		cvttpd2pi		Convert two packer double-precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm using truncation.
														
F3 0F 5B /r	CVTTPS2DQ xmm1, xmm2/m128	RM	W, R						V	V	SSE2	cvttps2dq		Convert four single-precision floating-point values from xmm2/m128 to four signed doubleword integers in xmm1 using truncation.
VEX.128.F3.0F.WIG 5B /r	VCVTTPS2DQ xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vcvttps2dq		Convert four packed single precision floating- point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
VEX.256.F3.0F.WIG 5B /r	VCVTTPS2DQ ymm1, ymm2/m256	RM	W, R						V	V	AVX	vcvttps2dq		Convert eight packed single precision floating- point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation.
														
0F 2C /r	CVTTPS2PI mm, xmm/m64	RM	W, R						V	V		cvttps2pi		Convert two single-precision floating-point values from xmm/m64 to two signed doubleword signed integers in mm using truncation.
														
F2 0F 2C /r	CVTTSD2SI r32, xmm/m64	RM	W, R						V	V	SSE2	cvttsd2si		Convert one double-precision floating-point value from xmm/m64 to one signed doubleword integer in r32 using truncation.
F2 REX.W+ 0F 2C /r	CVTTSD2SI r64, xmm/m64	RM	W, R						V	NE	SSE2	cvttsd2siq		Convert one double precision floating-point value from xmm/m64 to one signedquadword integer in r64 using truncation.
VEX.LIG.F2.0F.W0 2C /r	VCVTTSD2SI r32, xmm1/m64	RM	W, R						V	V	AVX	vcvttsd2si		Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VEX.LIG.F2.0F.W1 2C /r	VCVTTSD2SI r64, xmm1/m64	RM	W, R						V	NE	AVX	vcvttsd2siq		Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
														
F3 0F 2C /r	CVTTSS2SI r32, xmm/m32	RM	W, R						V	V	SSE	cvttss2si		Convert one single-precision floating-point value from xmm/m32 to one signed doubleword integer in r32 using truncation.
F3 REX.W+ 0F 2C /r	CVTTSS2SI r64, xmm/m32	RM	W, R						V	NE	SSE	cvttss2siq		Convert one single-precision floating-point value from xmm/m32 to one signed quadword integer in r64 using truncation.
VEX.LIG.F3.0F.W0 2C /r	VCVTTSS2SI r32, xmm1/m32	RM	W, R						V	V	AVX	vcvttss2si		Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VEX.LIG.F3.0F.W1 2C /r	VCVTTSS2SI r64, xmm1/m32	RM	W, R						V	NE	AVX	vcvttss2siq		Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
														
PREF.66+ 99	CWD	NP		AX	DX				V	V		cwtd		DX:AX = sign-extend of AX.
99	CDQ	NP		EAX	EDX				V	V		cltd		EDX:EAX = sign-extend of EAX.
REX.W+ 99	CQO	NP		RAX	RDX				V	NE		cqto		RDX:RAX = sign-extend of RAX.
														
27	DAA								I	V				Decimal adjust AL after addition.
														
2F	DAS								I	V				Decimal adjust AL after subtraction.
														
FE /1	DEC r/m8 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		decb		Decrement r/m8 by 1.
REX+ FE /1	DEC r/m8	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	NE		decb		Decrement r/m8 by 1.
FF /1	DEC r/m16 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		decw		Decrement r/m16 by 1.
FF /1	DEC r/m32 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		decl		Decrement r/m32 by 1.
REX.W+ FF /1	DEC r/m64 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	NE		decq		Decrement r/m64 by 1.
48 +rw	DEC r16 								NE	V				Decrement r16 by 1.
48 +rd	DEC r32								NE	V				Decrement r32 by 1.
														
F6 /6	DIV r/m8	M	R	AX	AX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		divb		Unsigned divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
REX+ F6 /6	DIV r/m8	M	R	AX	AX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	NE		divb		Unsigned divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
F7 /6	DIV r/m16 	M	R	AX DX	AX DX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		divw		Unsigned divide DX:AX by r/m16, with result stored in AX = Quotient, DX = Remainder.
F7 /6	DIV r/m32	M	R	EAX EDX	EAX EDX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		divl		Unsigned divide EDX:EAX by r/m32, with result stored in EAX = Quotient, EDX = Remainder.
REX.W+ F7 /6	DIV r/m64	M	R	RAX RDX	RAX RDX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	NE		divq		Unsigned divide RDX:RAX by r/m64, with result stored in RAX = Quotient, RDX = Remainder.
														
66 0F 5E /r	DIVPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	divpd		Divide packed double-precision floating-point values in xmm1 by packed double-precision floating-point values xmm2/m128.
VEX.NDS.128.66.0F.WIG 5E /r	VDIVPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vdivpd		Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem.
VEX.NDS.256.66.0F.WIG 5E /r	VDIVPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vdivpd		Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem.
														
0F 5E /r	DIVPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	divps		Divide packed single-precision floating-point values in xmm1 by packed single-precision floating-point values xmm2/m128.
VEX.NDS.128.0F.WIG 5E /r	VDIVPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vdivps		Divide packed single-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem.
VEX.NDS.256.0F.WIG 5E /r	VDIVPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vdivps		Divide packed single-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem.
														
F2 0F 5E /r	DIVSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	divsd		Divide low double-precision floating-point value in xmm1 by low double-precision floating-point value in xmm2/mem64.
VEX.NDS.LIG.F2.0F.WIG 5E /r	VDIVSD xmm1, xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vdivsd		Divide low double-precision floating point values in xmm2 by low double precision floating-point value in xmm3/mem64.
														
F3 0F 5E /r	DIVSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	divss		Divide low single-precision floating-point value in xmm1 by low single-precision floating-point value in xmm2/m32.
VEX.NDS.LIG.F3.0F.WIG 5E /r	VDIVSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vdivss		Divide low single-precision floating point value in xmm2 by low single precision floating-point value in xmm3/m32.
														
66 0F 3A 41 /r ib	DPPD xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE4_1	dppd		Selectively multiply packed DP floating-point values from xmm1 with packed DP floating- point values from xmm2, add and selectively store the packed DP floating-point values to xmm1.
VEX.NDS.128.66.0F3A.WIG 41 /r ib	VDPPD xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vdppd		Selectively multiply packed DP floating-point values from xmm2 with packed DP floating- point values from xmm3, add and selectively store the packed DP floating-point values to xmm1.
														
66 0F 3A 40 /r ib	DPPS xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE4_1	dpps		Selectively multiply packed SP floating-point values from xmm1 with packed SP floating- point values from xmm2, add and selectively store the packed SP floating-point values or zero values to xmm1.
VEX.NDS.128.66.0F3A.WIG 40 /r ib	VDPPS xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vdpps		Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1.
VEX.NDS.256.66.0F3A.WIG 40 /r ib	VDPPS ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vdpps		Multiply packed single-precision floating-point values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1.
														
0F 77	EMMS	NP			TAG(*)				V	V		emms		Set the x87 FPU tag word to empty.
														
C8 ib iw 	ENTER 0, imm16	II	R, R	???	???	???			V	V		enterq		Create a stack frame for a procedure.
C8 ib iw	ENTER 1, imm16	II	R, R	???	???	???			V	V		enterq		Create a nested stack frame for a procedure.
C8 ib iw	ENTER imm8, imm16	II	R, R	???	???	???			V	V		enterq		Create a nested stack frame for a procedure.
														
66 0F 3A 17 /r ib	EXTRACTPS reg/m32, xmm2, imm8	MRI	W, R, R						V	V	SSE4_1	extractps		Extract a single-precision floating-point value from xmm2 at the source offset specified by imm8 and store the result to reg or m32. The upper 32 bits of r64 is zeroed if reg is r64.
VEX.128.66.0F3A.WIG 17 /r ib	VEXTRACTPS r/m32, xmm1, imm8	MRI	W, R, R						V	V	AVX	vextractps		Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
														
D9 F0	F2XM1			ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	f2xm1		Replace ST(0) with (2^(ST(0)) - 1).
														
D9 E1	FABS			ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fabs		Replace ST with its absolute value.
														
D8 /0	FADD m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fadds		Add m32fp to ST(0) and store result in ST(0).
DC /0	FADD m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	faddl		Add m64fp to ST(0) and store result in ST(0).
D8 C0 +i	FADD ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fadd		Add ST(0) to ST(i) and store result in ST(0).
DC C0 +i	FADD ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fadd		Add ST(i) to ST(0) and store result in ST(i).
DE C0 +i	FADDP ST(i), ST(0)	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	faddp		Add ST(0) to ST(i), store result in ST(i), and pop the register stack.
DE C1	FADDP			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	faddp		Add ST(0) to ST(1), store result in ST(1), and pop the register stack.
DA /0	FIADD m32int	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fiaddl		Add m32int to ST(0) and store result in ST(0).
DE /0	FIADD m16int	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fiadd		Add m16int to ST(0) and store result in ST(0).
														
DF /4	FBLD m80bcd	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fbld		Convert BCD value to floating-point and push onto the FPU stack.
														
DF /6	FBSTP m80bcd	M	R	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fbstp		Store ST(0) in m80bcd and pop ST(0).
														
D9 E0	FCHS			ST(0)	ST(0)				V	V	FPU	fchs		Complements sign of ST(0).
														
9B DB E2	FCLEX				S.PE S.UE S.OE S.ZE S.DE S.IE S.ES S.SF S.B	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fclex		Clear floating-point exception flags after checking for pending unmasked floating-point exceptions.
DB E2	FNCLEX				S.PE S.UE S.OE S.ZE S.DE S.IE S.ES S.SF S.B	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnclex		Clear floating-point exception flags without checking for pending unmasked floating-point exceptions.
														
DA C0 +i 	FCMOVB ST(0), ST(i) 	TO	w, R	E.CF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovb		Move if below (CF=1).
DA C8 +i 	FCMOVE ST(0), ST(i) 	TO	w, R	E.ZF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmove		Move if equal (ZF=1).
DA D0 +i 	FCMOVBE ST(0), ST(i) 	TO	w, R	E.CF E.ZF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovbe		Move if below or equal (CF=1 or ZF=1).
DA D8 +i 	FCMOVU ST(0), ST(i) 	TO	w, R	E.PF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovu		Move if unordered (PF=1).
DB C0 +i 	FCMOVNB ST(0), ST(i) 	TO	w, R	E.CF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovnb		Move if not below (CF=0).
DB C8 +i 	FCMOVNE ST(0), ST(i) 	TO	w, R	E.ZF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovne		Move if not equal (ZF=0).
DB D0 +i 	FCMOVNBE ST(0), ST(i) 	TO	w, R	E.CF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovnbe		Move if not below or equal (CF=0 and ZF=0).
DB D8 +i	FCMOVNU ST(0), ST(i)	TO	w, R	E.PF	S.C1	S.C0 S.C2 S.C3			V	V	FPU CMOV	fcmovnu		Move if not unordered (PF=0).
														
D8 /2 	FCOM m32fp 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcoms		Compare ST(0) with m32fp.
DC /2 	FCOM m64fp 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcoml		Compare ST(0) with m64fp.
D8 D0 +i 	FCOM ST(i) 	O	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcom		Compare ST(0) with ST(i).
D8 D1 	FCOM			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcom		Compare ST(0) with ST(1).
D8 /3 	FCOMP m32fp 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcomps		Compare ST(0) with m32fp and pop register stack.
DC /3 	FCOMP m64fp 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcompl		Compare ST(0) with m64fp and pop register stack.
D8 D8 +i 	FCOMP ST(i) 	O	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcomp		Compare ST(0) with ST(i) and pop register stack.
D8 D9 	FCOMP 			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcomp		Compare ST(0) with ST(1) and pop register stack.
DE D9	FCOMPP			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fcompp		Compare ST(0) with ST(1) and pop register stack twice.
														
DB F0 +i 	FCOMI ST, ST(i) 	TO	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF S.C1				V	V	FPU	fcomi		Compare ST(0) with ST(i) and set status flags accordingly.
DF F0 +i	FCOMIP ST, ST(i) 	TO	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF S.C1				V	V	FPU	fcomip		Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
DB E8 +i 	FUCOMI ST, ST(i) 	TO	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF S.C1				V	V	FPU	fucomi		Compare ST(0) with ST(i), check for ordered values, and set status flags accordingly.
DF E8 +i	FUCOMIP ST, ST(i) 	TO	R, R		E.CF E.OF E.SF E.ZF E.AF E.PF S.C1				V	V	FPU	fucomip		Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack.
														
D9 FF 	FCOS 			ST(0)	ST(0) s.c1 S.C2	s.c1 S.C0 S.C3			V	V	FPU	fcos		Replace ST(0) with its cosine.
														
D9 F6 	FDECSTP 			S.TOP	S.TOP S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdecstp		Decrement TOP field in FPU status word.
														
D8 /6 	FDIV m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivs		Divide ST(0) by m32fp and store result in ST(0).
DC /6 	FDIV m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivl		Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
D8 F0 +i 	FDIV ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdiv		Divide ST(0) by ST(i) and store result in ST(0).
DC F0 +i 	FDIV ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdiv		Divide ST(i) by ST(0) and store result in ST(i).
DE F0 +i 	FDIVP ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivp		Divide ST(i) by ST(0), store result in ST(i), and pop the register stack.
DE F1 	FDIVP 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivp		Divide ST(1) by ST(0), store result in ST(1), and pop the register stack.
DA /6 	FIDIV m32int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fidivl		Divide ST(0) by m32int and store result in ST(0).
DE /6 	FIDIV m16int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fidiv		Divide ST(0) by m64int and store result in ST(0).
														
D8 /7 	FDIVR m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivrs		Divide m32fp by ST(0) and store result in ST(0).
DC /7 	FDIVR m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivrl		Divide m64fp by ST(0) and store result in ST(0).
D8 F8 +i 	FDIVR ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivr		Divide ST(i) by ST(0) and store result in ST(0).
DC F8 +i 	FDIVR ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivr		Divide ST(0) by ST(i) and store result in ST(i).
DE F8 +i 	FDIVRP ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivrp		Divide ST(0) by ST(i), store result in ST(i), and pop the register stack.
DE F9	FDIVRP 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fdivrp		Divide ST(0) by ST(1), store result in ST(1), and pop the register stack.
DA /7 	FIDIVR m32int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fidivrl		Divide m32int by ST(0) and store result in ST(0).
DE /7 	FIDIVR m16int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fidivr		Divide m16int by ST(0) and store result in ST(0).
														
DD C0 +i 	FFREE ST(i) 	O	I			S.C0 S.C1 s.c2 S.C3			V	V	FPU	ffreep		Sets tag for ST(i) to empty.
														
DE /2 	FICOM m16int 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	ficom		Compare ST(0) with m16int.
DA /2 	FICOM m32int 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	ficoml		Compare ST(0) with m32int.
DE /3 	FICOMP m16int 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	ficomp		Compare ST(0) with m16int and pop stack register.
DA /3 	FICOMP m32int 	M	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	ficompl		Compare ST(0) with m32int and pop stack register.
														
DF /0 	FILD m16int 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fild		Push m16int onto the FPU register stack.
DB /0 	FILD m32int 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fildl		Push m32int onto the FPU register stack.
DF /5 	FILD m64int 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fildll		Push m64int onto the FPU register stack.
														
D9 F7 	FINCSTP 			S.TOP	S.TOP S.C1	S.C0 S.C2 S.C3			V	V	FPU	fincstp		Increment the TOP field in the FPU status register.
														
9B DB E3 	FINIT 				S.* TAG(*) FPUDATA FPUINSTR FPUOPCODE				V	V	FPU	finit		Initialize FPU after checking for pending unmasked floating-point exceptions.
DB E3 	FNINIT				S.* TAG(*) FPUDATA FPUINSTR FPUOPCODE				V	V	FPU	fninit		Initialize FPU without checking for pending unmasked floating-point exceptions.
														
DF /2 	FIST m16int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fist		Store ST(0) in m16int.
DB /2 	FIST m32int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fistl		Store ST(0) in m32int.
DF /3 	FISTP m16int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fistp		Store ST(0) in m16int and pop register stack.
DB /3 	FISTP m32int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fistpl		Store ST(0) in m32int and pop register stack.
DF /7 	FISTP m64int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fistpll		Store ST(0) in m64int and pop register stack.
														
DF /1 	FISTTP m16int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisttp		Store ST(0) in m16int with truncation.
DB /1 	FISTTP m32int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisttpl		Store ST(0) in m32int with truncation.
DD /1 	FISTTP m64int 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisttpll		Store ST(0) in m64int with truncation.
														
D9 /0 	FLD m32fp 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	flds		Push m32fp onto the FPU register stack.
DD /0 	FLD m64fp 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldl		Push m64fp onto the FPU register stack.
DB /5 	FLD m80fp 	M	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldt		Push m80fp onto the FPU register stack.
D9 C0 +i 	FLD ST(i) 	O	R		ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fld		Push ST(i) onto the FPU register stack.
														
D9 E8 	FLD1 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fld1		Push +1.0 onto the FPU register stack.
D9 E9 	FLDL2T 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldl2t		Push log210 onto the FPU register stack.
D9 EA 	FLDL2E 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldl2e		Push log2e onto the FPU register stack.
D9 EB 	FLDPI 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldpi		Push pi onto the FPU register stack.
D9 EC 	FLDLG2 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldlg2		Push log102 onto the FPU register stack.
D9 ED 	FLDLN2 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldln2		Push loge2 onto the FPU register stack.
D9 EE 	FLDZ 				ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fldz		Push +0.0 onto the FPU register stack.
														
D9 /5 	FLDCW m2byte 	M	R			S.C0 S.C1 S.C2 S.C3			V	V	FPU	fldcw		Load FPU control word from m2byte.
														
D9 /4 	FLDENV m28byte 	M	R		S.* TAG(*) FPUDATA FPUINSTR FPUOPCODE				V	V	FPU	fldenvl		Load FPU environment from m14byte or m28byte.
														
D8 /1 	FMUL m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmuls		Multiply ST(0) by m32fp and store result in ST(0).
DC /1 	FMUL m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmull		Multiply ST(0) by m64fp and store result in ST(0).
D8 C8 +i 	FMUL ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmul		Multiply ST(0) by ST(i) and store result in ST(0).
DC C8 +i 	FMUL ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmul		Multiply ST(i) by ST(0) and store result in ST(i).
DE C8 +i 	FMULP ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmulp		Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack.
DE C9 	FMULP 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fmulp		Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack.
DA /1 	FIMUL m32int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fimull		Multiply ST(0) by m32int and store result in ST(0).
DE /1 	FIMUL m16int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fimul		Multiply ST(0) by m16int and store result in ST(0).
														
D9 D0 	FNOP 					S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnop		No operation is performed.
														
D9 F3 	FPATAN 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fpatan		Replace ST(1) with arctan(ST(1)/ST(0)) and pop the register stack.
														
D9 F8 	FPREM 			ST(0) ST(1)	ST(0) S.C0 S.C1 S.C2 S.C3				V	V	FPU	fprem		Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1).
														
D9 F5 	FPREM1 			ST(0) ST(1)	ST(0) S.C0 S.C1 S.C2 S.C3				V	V	FPU	fprem1		Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1).
														
D9 F2 	FPTAN 			ST(0)	ST(0) S.C1 S.C2	S.C0 S.C3			V	V	FPU	fptan		Replace ST(0) with its tangent and push 1 onto the FPU stack.
														
D9 FC 	FRNDINT 			ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	frndint		Round ST(0) to an integer.
														
DD /4 	FRSTOR m108byte 	M	R		ST(*) TAG(*) FPUDATA FPUINSTR S.* M.*				V	V	FPU	frstor		Load FPU state from m94byte or m108byte.
														
9B DD /6 	FSAVE m108byte 	M	W	ST(*) S.* FPUDATA FPUINSTR FPUOPCODE	ST(*) TAG(*) S.* FPUDATA FPUINSTR FPUOPCODE				V	V	FPU	fsave		Store FPU state to m94byte or m108byte after checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
DD /6 	FNSAVE m108byte 	M	W	ST(*) S.* FPUDATA FPUINSTR FPUOPCODE	ST(*) TAG(*) S.* FPUDATA FPUINSTR FPUOPCODE				V	V	FPU	fnsave		Store FPU environment to m94byte or m108byte without checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
														
D9 FD 	FSCALE 			ST(0) ST(1)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fscale		Scale ST(0) by ST(1).
														
D9 FE 	FSIN 			ST(0)	ST(0) S.C1 S.C2	S.C0 S.C3			V	V	FPU	fsin		Replace ST(0) with its sine.
														
D9 FB 	FSINCOS 			ST(0)	ST(0) S.C1 S.C2	S.C0 S.C3			V	V	FPU	fsincos		Compute the sine and cosine of ST(0); replace ST(0) with the sine, and push the cosine onto the register stack.
														
D9 FA 	FSQRT 			ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsqrt		Computes square root of ST(0) and stores the result in ST(0).
														
D9 /2 	FST m32fp 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsts		Copy ST(0) to m32fp.
DD /2 	FST m64fp 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fstl		Copy ST(0) to m64fp.
DD D0 +i 	FST ST(i) 	O	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fst		Copy ST(0) to ST(i).
D9 /3 	FSTP m32fp 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fstps		Copy ST(0) to m32fp and pop register stack.
DD /3 	FSTP m64fp 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fstpl		Copy ST(0) to m64fp and pop register stack.
DB /7 	FSTP m80fp 	M	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fstpt		Copy ST(0) to m80fp and pop register stack.
DD D8 +i 	FSTP ST(i) 	O	W	ST(0)	S.C1	S.C0 S.C2 S.C3			V	V	FPU	fstp		Copy ST(0) to ST(i) and pop register stack.
														
9B D9 /7 	FSTCW m2byte 	M	W			S.C0 S.C1 S.C2 S.C3			V	V	FPU	fstcw		Store FPU control word to m2byte after checking for pending unmasked floating-point exceptions.
D9 /7 	FNSTCW m2byte 	M	W			S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnstcw		Store FPU control word to m2byte without checking for pending unmasked floating-point exceptions.
														
9B D9 /6 	FSTENV m28byte 	M	W		S.* TAG(*) FPUDATA FPUINSTR FPUOPCODE	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fstenvl		Store FPU environment to m14byte or m28byte after checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
D9 /6 	FNSTENV m28byte 	M	W		S.* TAG(*) FPUDATA FPUINSTR FPUOPCODE	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnstenvl		Store FPU environment to m14byte or m28byte without checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
														
9B DD /7 	FSTSW m2byte 	M	W		S.*	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fstsw		Store FPU status word at m2byte after checking for pending unmasked floating-point exceptions.
9B DF E0 	FSTSW AX 		W		S.*	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fstsw		Store FPU status word in AX register after checking for pending unmasked floating-point exceptions.
DD /7 	FNSTSW m2byte 	M	W		S.*	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnstsw		Store FPU status word at m2byte without checking for pending unmasked floating-point exceptions.
DF E0 	FNSTSW AX 		W		S.*	S.C0 S.C1 S.C2 S.C3			V	V	FPU	fnstsw		Store FPU status word in AX register without checking for pending unmasked floating-point exceptions.
														
D8 /4 	FSUB m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubs		Subtract m32fp from ST(0) and store result in ST(0).
DC /4 	FSUB m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubl		Subtract m64fp from ST(0) and store result in ST(0).
D8 E0 +i 	FSUB ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsub		Subtract ST(i) from ST(0) and store result in ST(0).
DC E0 +i 	FSUB ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsub		Subtract ST(0) from ST(i) and store result in ST(i).
DE E0 +i 	FSUBP ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubp		Subtract ST(0) from ST(i), store result in ST(i), and pop register stack.
DE E1 	FSUBP 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubp		Subtract ST(0) from ST(1), store result in ST(1), and pop register stack.
DA /4 	FISUB m32int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisubl		Subtract m32int from ST(0) and store result in ST(0).
DE /4 	FISUB m16int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisub		Subtract m16int from ST(0) and store result in ST(0).
														
D8 /5 	FSUBR m32fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubrs		Subtract ST(0) from m32fp and store result in ST(0).
DC /5 	FSUBR m64fp 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubrl		Subtract ST(0) from m64fp and store result in ST(0).
D8 E8 +i 	FSUBR ST(0), ST(i) 	TO	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubr		Subtract ST(0) from ST(i) and store result in ST(0).
DC E8 +i 	FSUBR ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubr		Subtract ST(i) from ST(0) and store result in ST(i).
DE E8 +i 	FSUBRP ST(i), ST(0) 	OT	RW, R		S.C1	S.C0 S.C2 S.C3			V	V		fsubrp		Subtract ST(i) from ST(0), store result in ST(i), and pop register stack.
DE E9	FSUBRP 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fsubrp		Subtract ST(1) from ST(0), store result in ST(1), and pop register stack.
DA /5 	FISUBR m32int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisubrl		Subtract ST(0) from m32int and store result in ST(0).
DE /5 	FISUBR m16int 	M	R	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fisubr		Subtract ST(0) from m16int and store result in ST(0).
														
D9 E4 	FTST 			ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	ftst		Compare ST(0) with 0.0.
														
DD E0 +i 	FUCOM ST(i) 	O	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fucom		Compare ST(0) with ST(i).
DD E1 	FUCOM 			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fucom		Compare ST(0) with ST(1).
DD E8 +i 	FUCOMP ST(i) 	O	R	ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fucomp		Compare ST(0) with ST(i) and pop register stack.
DD E9 	FUCOMP 			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fucomp		Compare ST(0) with ST(1) and pop register stack.
DA E9 	FUCOMPP 			ST(0) ST(1)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fucompp		Compare ST(0) with ST(1) and pop register stack twice.
														
D9 E5 	FXAM 			ST(0)	S.C0 S.C1 S.C2 S.C3				V	V	FPU	fxam		Classify value or number in ST(0).
														
D9 C8 +i 	FXCH ST(i) 	O	RW	ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fxch		Exchange the contents of ST(0) and ST(i).
D9 C9 	FXCH 			ST(0) ST(1)	ST(0) ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fxch		Exchange the contents of ST(0) and ST(1).
														
0F AE /1 	FXRSTOR m512byte	M	R		ST(*) MM* XMM* CS DS				V	V	FPU FXSR	fxrstor		Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
REX.W+ 0F AE /1	FXRSTOR64 m512byte	M	R		ST(*) MM* XMM*				V	NE	FPU FXSR	fxrstor64		Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
														
0F AE /0 	FXSAVE m512byte	M	W	ST(*) MM* XMM* M.* CS DS					V	V	FPU FXSR	fxsave		Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
REX.W+ 0F AE /0	FXSAVE64 m512byte	M	W	ST(*) MM* XMM* M.*					V	NE	FPU FXSR	fxsave64		Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
														
D9 F4 	FXTRACT			ST(0)	ST(0) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fxtract		Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack.
														
D9 F1 	FYL2X 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fyl2x		Replace ST(1) with (ST(1) * log2ST(0)) and pop the register stack.
														
D9 F9 	FYL2XP1 			ST(0) ST(1)	ST(1) S.C1	S.C0 S.C2 S.C3			V	V	FPU	fyl2xp1		Replace ST(1) with ST(1) * log2(ST(0) + 1.0) and pop the register stack.
														
66 0F 7C /r 	HADDPD xmm1, xmm2/m128	RM	RW, R						V	V	PNI	haddpd		Horizontal add packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 7C /r	VHADDPD xmm1, xmm2, xmm3/m128	RVM	W, R, R						V	V	AVX	vhaddpd		Horizontal add packed double-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 7C /r	VHADDPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vhaddpd		Horizontal add packed double-precision floating-point values from ymm2 and ymm3/mem.
														
F2 0F 7C /r 	HADDPS xmm1, xmm2/m128	RM	RW, R						V	V	PNI	haddps		Horizontal add packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG 7C /r	VHADDPS xmm1, xmm2, xmm3/m128	RVM	W, R, R						V	V	AVX	vhaddps		Horizontal add packed single-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.F2.0F.WIG 7C /r	VHADDPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vhaddps		Horizontal add packed single-precision floating-point values from ymm2 and ymm3/mem.
														
F4 	HLT 						NO	YES	V	V				Halt
														
66 0F 7D /r 	HSUBPD xmm1, xmm2/m128	RM	RW, R						V	V	PNI	hsubpd		Horizontal subtract packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG 7D /r 	VHSUBPD xmm1, xmm2, xmm3/m128 	RVM	W, R, R						V	V	AVX	vhsubpd		Horizontal subtract packed double-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 7D /r 	VHSUBPD ymm1, ymm2, ymm3/m256 	RVM	W, R, R						V	V	AVX	vhsubpd		Horizontal subtract packed double-precision floating-point values from ymm2 and ymm3/mem.
														
F2 0F 7D /r 	HSUBPS xmm1, xmm2/m128	RM	RW, R						V	V	PNI	hsubps		Horizontal subtract packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.NDS.128.F2.0F.WIG 7D /r	VHSUBPS xmm1, xmm2, xmm3/m128	RVM	W, R, R						V	V	AVX	vhsubps		Horizontal subtract packed single-precision floating-point values from xmm2 and xmm3/mem.
VEX.NDS.256.F2.0F.WIG 7D /r	VHSUBPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vhsubps		Horizontal subtract packed single-precision floating-point values from ymm2 and ymm3/mem.
														
F6 /7 	IDIV r/m8 	M	R	AX	AX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		idivb		Signed divide AX by r/m8, with result stored in: AL = Quotient, AH = Remainder.
REX+ F6 /7 	IDIV r/m8	M	R	AX	AX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	NE		idivb		Signed divide AX by r/m8, with result stored in AL = Quotient, AH = Remainder.
F7 /7 	IDIV r/m16 	M	R	AX DX	AX DX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		idivw		Signed divide DX:AX by r/m16, with result stored in AX = Quotient, DX = Remainder.
F7 /7 	IDIV r/m32 	M	R	EAX EDX	EAX EDX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	V		idivl		Signed divide EDX:EAX by r/m32, with result stored in EAX = Quotient, EDX = Remainder.
REX.W+ F7 /7 	IDIV r/m64 	M	R	RAX RDX	RAX RDX	E.CF E.OF E.SF E.ZF E.AF E.PF			V	NE		idivq		Signed divide RDX:RAX by r/m64, with result stored in RAX = Quotient, RDX = Remainder.
														
F6 /5	IMUL r/m8	M	R	AL	AX E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulb		AX= AL * r/m byte.
REX+ F6 /5	IMUL r/m8	M	R	AL	AX E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulb		AX= AL * r/m byte.
F7 /5 	IMUL r/m16 	M	R	AX	AX DX E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulw		DX:AX = AX * r/m word.
F7 /5 	IMUL r/m32 	M	R	EAX	EAX EDX E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imull		EDX:EAX = EAX * r/m32.
REX.W+ F7 /5 	IMUL r/m64 	M	R	RAX	RAX RDX E.CF E.OF	E.SF E.ZF E.AF E.PF			V	NE		imulq		RDX:RAX = RAX * r/m64.
0F AF /r 	IMUL r16, r/m16 	RM	RW, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulw		word register = word register * r/m16.
0F AF /r 	IMUL r32, r/m32 	RM	RW, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imull		doubleword register = doubleword register *  r/m32.
REX.W+ 0F AF /r 	IMUL r64, r/m64 	RM	RW, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	NE		imulq		Quadword register = Quadword register *  r/m64.
6B /r ib 	IMUL r16, r/m16, imm8 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulw		word register = r/m16 * sign-extended immediate byte.
6B /r ib 	IMUL r32, r/m32, imm8 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imull		doubleword register = r/m32 * sign- extended immediate byte.
REX.W+ 6B /r ib 	IMUL r64, r/m64, imm8 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	NE		imulq		Quadword register = r/m64 * sign-extended  immediate byte.
69 /r iw 	IMUL r16, r/m16, imm16 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imulw		word register = r/m16 * immediate word.
69 /r id 	IMUL r32, r/m32, imm32 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	V		imull		doubleword register = r/m32 * immediate doubleword.
REX.W+ 69 /r id 	IMUL r64, r/m64, imm32 	RMI	W, R, R		E.CF E.OF	E.SF E.ZF E.AF E.PF			V	NE		imulq		Quadword register = r/m64 * immediate doubleword.
														
E4 ib 	IN AL, imm8 	I	W, R	E.IOPL E.VM					V	V		inb		Input byte from imm8 I/O port address into AL.
E5 ib 	IN AX, imm8 	I	W, R	E.IOPL E.VM					V	V		inw		Input word from imm8 I/O port address into AX.
E5 ib 	IN EAX, imm8 	I	W, R	E.IOPL E.VM					V	V		inl		Input dword from imm8 I/O port address into EAX.
EC 	IN AL,DX 	NP	W, R	E.IOPL E.VM					V	V		inb		Input byte from I/O port in DX into AL.
ED 	IN AX,DX 	NP	W, R	E.IOPL E.VM					V	V		inw		Input word from I/O port in DX into AX.
ED 	IN EAX,DX 	NP	W, R	E.IOPL E.VM					V	V		inl		Input doubleword from I/O port in DX into EAX.
														
FE /0 	INC r/m8 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		incb		Increment r/m byte by 1.
REX+ FE /0 	INC r/m8	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	NE		incb		Increment r/m byte by 1.
FF /0 	INC r/m16 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		incw		Increment r/m word by 1.
FF /0 	INC r/m32 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	V		incl		Increment r/m doubleword by 1.
REX.W+ FF /0	INC r/m64 	M	RW		E.OF E.SF E.ZF E.AF E.PF				V	NE		incq		Increment r/m quadword by 1.
40 +rw 	INC r16 								NE	V				Increment word register by 1.
40 +rd 	INC r32 								NE	V				Increment doubleword register by 1.
														
6C 	INS m8, DX 	NP	I, R	E.IOPL E.VM E.DF RDI					V	V		ins		Input byte from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
6D 	INS m16, DX 	NP	I, R	E.IOPL E.VM E.DF RDI					V	V		ins		Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
6D 	INS m32, DX 	NP	I, R	E.IOPL E.VM E.DF RDI					V	V		ins		Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
6C 	INSB 	NP		E.IOPL E.VM E.DF DX RDI					V	V		insb		Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI.
PREF.66+ 6D 	INSW 	NP		E.IOPL E.VM E.DF DX RDI					V	V		insw		Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
6D 	INSD 	NP		E.IOPL E.VM E.DF DX RDI					V	V		insl		Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
														
66 0F 3A 21 /r ib 	INSERTPS xmm1, xmm2/m32, imm8	RMI	RW, R, R						V	V	SSE4_1	insertps		Insert a single precision floating-point value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
VEX.NDS.128.66.0F3A.WIG 21 /r ib	VINSERTPS xmm1, xmm2, xmm3/m32, imm8	RVMI	Z, W, R, R						V	V	AVX	vinsertps		Insert a single precision floating point value selected by imm8 from xmm3/m32 and merge into xmm2 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
														
CC 	INT 3 	NP	R	???	???	???			V	V		int		Interrupt 3-trap to debugger.
CD ib 	INT imm8 	I	R	???	???	???			V	V		int		Interrupt vector number specified by immediate byte.
CE 	INTO 								I	V				Interrupt 4-if overflow flag is 1.
														
0F 08 	INVD 						NO	YES	V	V				Flush internal caches; initiate flushing of external caches.
														
0F 01 /7 	INVLPG m 						NO	YES	V	V				Invalidate TLB Entry for page that contains m.
														
66 0F 38 82 /r 	INVPCID r32, m128								NE	V	INVPCID			Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descriptor in m128.
66 0F 38 82 /r	INVPCID r64, m128	RM	R, R						V	NE	INVPCID	invpcid		Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descriptor in m128.
														
PREF.66+ CF 	IRET 	NP		???	???	???			V	V		iretw		Interrupt return (16-bit operand size).
CF 	IRETD 	NP		???	???	???			V	V		iretl		Interrupt return (32-bit operand size).
REX.W+ CF 	IRETQ 	NP		???	???	???			V	NE		iretq		Interrupt return (64-bit operand size).
														
77 cb 	JA rel8 	D	R	E.CF E.ZF					V	V		ja		Jump short if above (CF=0 and ZF=0).
73 cb 	JAE rel8 	D	R	E.CF					V	V		jae		Jump short if above or equal (CF=0).
72 cb 	JB rel8 	D	R	E.CF					V	V		jb		Jump short if below (CF=1).
76 cb 	JBE rel8 	D	R	E.CF E.ZF					V	V		jbe		Jump short if below or equal (CF=1 or ZF=1).
72 cb 	JC rel8 	D	R	E.CF					V	V		jc		Jump short if carry (CF=1).
E3 cb 	JCXZ rel8 								NE	V				Jump short if CX register is 0.
E3 cb 	JECXZ rel8 	D	R	ECX					V	V		jecxz		Jump short if ECX register is 0.
E3 cb 	JRCXZ rel8 	D	R	RCX					V	NE		jrcxz		Jump short if RCX register is 0.
74 cb 	JE rel8 	D	R	E.ZF					V	V		je		Jump short if equal (ZF=1).
7F cb 	JG rel8 	D	R	E.ZF E.SF E.OF					V	V		jg		Jump short if greater (ZF=0 and SF=OF).
7D cb 	JGE rel8 	D	R	E.SF E.OF					V	V		jge		Jump short if greater or equal (SF=OF).
7C cb 	JL rel8 	D	R	E.SF E.OF					V	V		jl		Jump short if less (SF!= OF).
7E cb 	JLE rel8 	D	R	E.ZF E.SF E.OF					V	V		jle		Jump short if less or equal (ZF=1 or SF!= OF).
76 cb 	JNA rel8 	D	R	E.CF E.ZF					V	V		jna		Jump short if not above (CF=1 or ZF=1).
72 cb 	JNAE rel8 	D	R	E.CF					V	V		jnae		Jump short if not above or equal (CF=1).
73 cb 	JNB rel8 	D	R	E.CF					V	V		jnb		Jump short if not below (CF=0).
77 cb 	JNBE rel8 	D	R	E.CF E.ZF					V	V		jnbe		Jump short if not below or equal (CF=0 and ZF=0).
73 cb 	JNC rel8 	D	R	E.CF					V	V		jnc		Jump short if not carry (CF=0).
75 cb 	JNE rel8 	D	R	E.ZF					V	V		jne		Jump short if not equal (ZF=0).
7E cb 	JNG rel8 	D	R	E.ZF E.SF E.OF					V	V		jng		Jump short if not greater (ZF=1 or SF!= OF).
7C cb 	JNGE rel8 	D	R	E.SF E.OF					V	V		jnge		Jump short if not greater or equal (SF!= OF).
7D cb 	JNL rel8 	D	R	E.SF E.OF					V	V		jnl		Jump short if not less (SF=OF).
7F cb 	JNLE rel8 	D	R	E.ZF E.SF E.OF					V	V		jnle		Jump short if not less or equal (ZF=0 and SF=OF).
71 cb 	JNO rel8 	D	R	E.OF					V	V		jno		Jump short if not overflow (OF=0).
7B cb 	JNP rel8 	D	R	E.PF					V	V		jnp		Jump short if not parity (PF=0).
79 cb 	JNS rel8 	D	R	E.SF					V	V		jns		Jump short if not sign (SF=0).
75 cb 	JNZ rel8 	D	R	E.ZF					V	V		jnz		Jump short if not zero (ZF=0).
70 cb 	JO rel8 	D	R	E.OF					V	V		jo		Jump short if overflow (OF=1).
7A cb 	JP rel8 	D	R	E.PF					V	V		jp		Jump short if parity (PF=1).
7A cb 	JPE rel8 	D	R	E.PF					V	V		jpe		Jump short if parity even (PF=1).
7B cb 	JPO rel8 	D	R	E.PF					V	V		jpo		Jump short if parity odd (PF=0).
78 cb 	JS rel8 	D	R	E.SF					V	V		js		Jump short if sign (SF=1).
74 cb 	JZ rel8 	D	R	E.ZF					V	V		jz		Jump short if zero (ZF = 1).
0F 87 cw 	JA rel16 								NS	V				Jump near if above (CF=0 and ZF=0). Not supported in 64-bit mode.
0F 87 cd 	JA rel32 	D	R	E.CF E.ZF					V	V		ja		Jump near if above (CF=0 and ZF=0).
0F 83 cw 	JAE rel16 								NS	V				Jump near if above or equal (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JAE rel32 	D	R	E.CF					V	V		jae		Jump near if above or equal (CF=0).
0F 82 cw 	JB rel16 								NS	V				Jump near if below (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JB rel32 	D	R	E.CF					V	V		jb		Jump near if below (CF=1).
0F 86 cw 	JBE rel16 								NS	V				Jump near if below or equal (CF=1 or ZF=1). Not supported in 64-bit mode.
0F 86 cd 	JBE rel32 	D	R	E.CF E.ZF					V	V		jbe		Jump near if below or equal (CF=1 or ZF=1).
0F 82 cw 	JC rel16 								NS	V				Jump near if carry (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JC rel32 	D	R	E.CF					V	V		jc		Jump near if carry (CF=1).
0F 84 cw 	JE rel16 								NS	V				Jump near if equal (ZF=1). Not supported in 64-bit mode.
0F 84 cd 	JE rel32 	D	R	E.ZF					V	V		je		Jump near if 0 (ZF=1).
0F 8F cw 	JG rel16 								NS	V				Jump near if greater (ZF=0 and SF=OF). Not supported in 64-bit mode.
0F 8F cd 	JG rel32 	D	R	E.ZF E.SF E.OF					V	V		jg		Jump near if greater (ZF=0 and SF=OF).
0F 8D cw 	JGE rel16 								NS	V				Jump near if greater or equal (SF=OF). Not supported in 64-bit mode.
0F 8D cd 	JGE rel32 	D	R	E.SF E.OF					V	V		jge		Jump near if greater or equal (SF=OF).
0F 8C cw 	JL rel16 								NS	V				Jump near if less (SF!= OF). Not supported in  64-bit mode.
0F 8C cd 	JL rel32 	D	R	E.SF E.OF					V	V		jl		Jump near if less (SF!= OF).
0F 8E cw 	JLE rel16 								NS	V				Jump near if less or equal (ZF=1 or SF!= OF).  Not supported in 64-bit mode.
0F 8E cd 	JLE rel32 	D	R	E.ZF E.SF E.OF					V	V		jle		Jump near if less or equal (ZF=1 or SF!= OF).
0F 86 cw 	JNA rel16 								NS	V				Jump near if not above (CF=1 or ZF=1). Not supported in 64-bit mode.
0F 86 cd 	JNA rel32 	D	R	E.CF E.ZF					V	V		jna		Jump near if not above (CF=1 or ZF=1).
0F 82 cw 	JNAE rel16 								NS	V				Jump near if not above or equal (CF=1). Not supported in 64-bit mode.
0F 82 cd 	JNAE rel32 	D	R	E.CF					V	V		jnae		Jump near if not above or equal (CF=1).
0F 83 cw 	JNB rel16 								NS	V				Jump near if not below (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JNB rel32 	D	R	E.CF					V	V		jnb		Jump near if not below (CF=0).
0F 87 cw 	JNBE rel16 								NS	V				Jump near if not below or equal (CF=0 and ZF=0). Not supported in 64-bit mode.
0F 87 cd 	JNBE rel32 	D	R	E.CF E.ZF					V	V		jnbe		Jump near if not below or equal (CF=0 and ZF=0). 
0F 83 cw 	JNC rel16 								NS	V				Jump near if not carry (CF=0). Not supported in 64-bit mode.
0F 83 cd 	JNC rel32 	D	R	E.CF					V	V		jnc		Jump near if not carry (CF=0).
0F 85 cw 	JNE rel16 								NS	V				Jump near if not equal (ZF=0). Not supported in 64-bit mode.
0F 85 cd 	JNE rel32 	D	R	E.ZF					V	V		jne		Jump near if not equal (ZF=0).
0F 8E cw 	JNG rel16 								NS	V				Jump near if not greater (ZF=1 or SF != OF). Not supported in 64-bit mode.
0F 8E cd 	JNG rel32 	D	R	E.ZF E.SF E.OF					V	V		jng		Jump near if not greater (ZF=1 or SF != OF).
0F 8C cw 	JNGE rel16 								NS	V				Jump near if not greater or equal (SF != OF). Not supported in 64-bit mode.
0F 8C cd 	JNGE rel32 	D	R	E.SF E.OF					V	V		jnge		Jump near if not greater or equal (SF != OF).
0F 8D cw 	JNL rel16 								NS	V				Jump near if not less (SF=OF). Not supported in 64-bit mode.
0F 8D cd 	JNL rel32 	D	R	E.SF E.OF					V	V		jnl		Jump near if not less (SF=OF).
0F 8F cw 	JNLE rel16 								NS	V				Jump near if not less or equal (ZF=0 and SF=OF). Not supported in 64-bit mode.
0F 8F cd 	JNLE rel32 	D	R	E.ZF E.SF E.OF					V	V		jnle		Jump near if not less or equal (ZF=0 and SF=OF).
0F 81 cw 	JNO rel16 								NS	V				Jump near if not overflow (OF=0). Not supported in 64-bit mode.
0F 81 cd 	JNO rel32 	D	R	E.OF					V	V		jno		Jump near if not overflow (OF=0).
0F 8B cw 	JNP rel16 								NS	V				Jump near if not parity (PF=0). Not supported in 64-bit mode.
0F 8B cd 	JNP rel32 	D	R	E.PF					V	V		jnp		Jump near if not parity (PF=0).
0F 89 cw 	JNS rel16 								NS	V				Jump near if not sign (SF=0). Not supported in 64-bit mode.
0F 89 cd 	JNS rel32 	D	R	E.SF					V	V		jns		Jump near if not sign (SF=0).
0F 85 cw 	JNZ rel16 								NS	V				Jump near if not zero (ZF=0). Not supported in 64-bit mode.
0F 85 cd 	JNZ rel32 	D	R	E.ZF					V	V		jnz		Jump near if not zero (ZF=0).
0F 80 cw 	JO rel16 								NS	V				Jump near if overflow (OF=1). Not supported in 64-bit mode.
0F 80 cd 	JO rel32 	D	R	E.OF					V	V		jo		Jump near if overflow (OF=1).
0F 8A cw 	JP rel16 								NS	V				Jump near if parity (PF=1). Not supported in 64-bit mode.
0F 8A cd 	JP rel32 	D	R	E.PF					V	V		jp		Jump near if parity (PF=1).
0F 8A cw 	JPE rel16								NS	V				Jump near if parity even (PF=1). Not supported in 64-bit mode.
0F 8A cd 	JPE rel32 	D	R	E.PF					V	V		jpe		Jump near if parity even (PF=1).
0F 8B cw 	JPO rel16 								NS	V				Jump near if parity odd (PF=0). Not supported in 64-bit mode.
0F 8B cd 	JPO rel32 	D	R	E.PF					V	V		jpo		Jump near if parity odd (PF=0).
0F 88 cw 	JS rel16 								NS	V				Jump near if sign (SF=1). Not supported in 64-bit mode.
0F 88 cd 	JS rel32 	D	R	E.SF					V	V		js		Jump near if sign (SF=1).
0F 84 cw 	JZ rel16 								NS	V				Jump near if 0 (ZF=1). Not supported in 64-bit mode.
0F 84 cd 	JZ rel32 	D	R	E.ZF					V	V		jz		Jump near if 0 (ZF=1).
														
EB cb 	JMP rel8 	D	R		RIP				V	V		jmpq		Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits
E9 cw 	JMP rel16 								NS	V				Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode.
E9 cd 	JMP rel32 	D	R		RIP				V	V		jmpq		Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits
FF /4 	JMP r/m16 								NS	V				Jump near, absolute indirect, address = zero-extended r/m16. Not supported in 64-bit mode.
FF /4 	JMP r/m32 								NS	V				Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode.
FF /4 	JMP r/m64 	M	R		RIP				V	NE		jmpq		Jump near, absolute indirect, RIP = 64-Bit offset from register or memory
EA cd 	JMP ptr16:16 								I	V				Jump far, absolute, address given in operand
EA cp 	JMP ptr16:32 								I	V				Jump far, absolute, address given in operand
FF /5 	JMP m16:16 	D	R	???	???	???			V	V		jmpq		Jump far, absolute indirect, address given in m16:16
FF /5 	JMP m16:32 	D	R	???	???	???			V	V		jmpq		Jump far, absolute indirect, address given in m16:32.
REX.W+ FF /5 	JMP m16:64 	D	R	???	???	???			V	NE		jmpq		Jump far, absolute indirect, address given in m16:64.
														
9F 	LAHF 	NP		E.SF E.ZF E.PF E.AF E.CF	AH				V	V		lahf		Load: AH = EFLAGS(SF:ZF:0:AF:0:PF:1:CF).
														
0F 02 /r 	LAR r16, r16/m16 	RM	W, R		E.ZF		YES	NO	V	V		larw		r16 = access rights referenced by r16/m16
0F 02 /r 	LAR r32, r32/m16	RM	W, R		E.ZF		YES	NO	V	V		lar		reg = access rights referenced by r32/m16
REX.W+ 0F 02 /r 	LAR r64, r32/m16	RM	W, R		E.ZF		YES	NO	V	V		lar		reg = access rights referenced by r32/m16
														
F2 0F F0 /r 	LDDQU xmm1, m128	RM	W, R						V	V	PNI	lddqu		Load unaligned data from mem and return double quadword in xmm1.
VEX.128.F2.0F.WIG F0 /r	VLDDQU xmm1, m128	RM	Z, R						V	V	AVX	vlddqu		Load unaligned packed integer values from mem to xmm1.
VEX.256.F2.0F.WIG F0 /r	VLDDQU ymm1, m256	RM	W, R						V	V	AVX	vlddqu		Load unaligned packed integer values from mem to ymm1.
														
0F AE /2 	LDMXCSR m32	M	R		M.*				V	V	SSE	ldmxcsr		Load MXCSR register from m32.
VEX.LZ.0F.WIG AE /2	VLDMXCSR m32	M	R		M.*				V	V	AVX	vldmxcsr		Load MXCSR register from m32.
														
C5 /r 	LDS r16, m16:16 								I	V				Load DS:r16 with far pointer from memory.
C5 /r 	LDS r32, m16:32 								I	V				Load DS:r32 with far pointer from memory.
0F B2 /r 	LSS r16, m16:16 	RM	W, R		SS				V	V		lssw		Load SS:r16 with far pointer from memory.
0F B2 /r 	LSS r32, m16:32 	RM	W, R		SS				V	V		lssl		Load SS:r32 with far pointer from memory.
REX.W+ 0F B2 /r 	LSS r64, m16:64 	RM	W, R		SS				V	NE		lss		Load SS:r64 with far pointer from memory.
C4 /r 	LES r16, m16:16 								I	V				Load ES:r16 with far pointer from memory.
C4 /r 	LES r32, m16:32 								I	V				Load ES:r32 with far pointer from memory.
0F B4 /r 	LFS r16, m16:16 	RM	W, R		FS				V	V		lfsw		Load FS:r16 with far pointer from memory.
0F B4 /r 	LFS r32, m16:32 	RM	W, R		FS				V	V		lfsl		Load FS:r32 with far pointer from memory.
REX.W+ 0F B4 /r 	LFS r64, m16:64 	RM	W, R		FS				V	NE		lfs		Load FS:r64 with far pointer from memory.
0F B5 /r 	LGS r16, m16:16 	RM	W, R		GS				V	V		lgsw		Load GS:r16 with far pointer from memory.
0F B5 /r 	LGS r32, m16:32 	RM	W, R		GS				V	V		lgsl		Load GS:r32 with far pointer from memory.
REX.W+ 0F B5 /r 	LGS r64, m16:64 	RM	W, R		GS				V	NE		lgs		Load GS:r64 with far pointer from memory.
														
8D /r 	LEA r16, m 	RM	W, I						V	V		leaw		Store effective address for m in register r16.
8D /r 	LEA r32, m 	RM	W, I						V	V		leal		Store effective address for m in register r32.
REX.W+ 8D /r 	LEA r64, m 	RM	W, I						V	NE		leaq		Store effective address for m in register r64.
														
PREF.66+ C9 	LEAVE p66	NP	I						V	V		leavew		Set SP to BP, then pop BP.
C9 	LEAVE 								NE	V				Set ESP to EBP, then pop EBP.
C9 	LEAVE 	NP							V	NE		leaveq		Set RSP to RBP, then pop RBP.
														
0F AE E8	LFENCE 	NP							V	V		lfence		Serializes load operations.
														
0F 01 /2 	LGDT m16&32 						NO	YES	NE	V				Load m into GDTR.
0F 01 /3 	LIDT m16&32 						NO	YES	NE	V				Load m into IDTR.
0F 01 /2 	LGDT m16&64 						NO	YES	V	NE				Load m into GDTR.
0F 01 /3 	LIDT m16&64 						NO	YES	V	NE				Load m into IDTR.
														
0F 00 /2 	LLDT r/m16 						NO	YES	V	V				Load segment selector r/m16 into LDTR.
														
0F 01 /6 	LMSW r/m16 						NO	YES	V	V				Loads r/m16 in machine status word of CR0.
														
F0 	LOCK 	NP					YES	NO	V	V		lock		Asserts LOCK# signal for duration of the accompanying instruction.
														
AC 	LODS m8 	NP	I	E.DF SI rsi	AL				V	V		lods		For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
AD 	LODS m16 	NP	I	E.DF SI rsi	AX				V	V		lods		For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
AD 	LODS m32 	NP	I	E.DF SI rsi	EAX				V	V		lods		For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
REX.W+ AD 	LODS m64 	NP	I	E.DF SI rsi	RAX				V	NE		lods		Load qword at address (R)SI into RAX.
AC 	LODSB 	NP		E.DF SI rsi	AL				V	V		lodsb		For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
PREF.66+ AD 	LODSW 	NP		E.DF SI rsi	AX				V	V		lodsw		For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
AD 	LODSD 	NP		E.DF SI rsi	EAX				V	V		lodsl		For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
REX.W+ AD 	LODSQ 	NP		E.DF SI rsi	RAX				V	NE		lodsq		Load qword at address (R)SI into RAX.
														
E2 cb 	LOOP rel8 	D	R	RCX E.ZF					V	V		loop		Decrement count; jump short if count != 0.
E0 cb 	LOOPE rel8 	D	R	RCX E.ZF					V	V		loope		Decrement count; jump short if count != 0 and  ZF = 1.
E0 cb 	LOOPNE rel8 	D	R	RCX E.ZF					V	V		loopne		Decrement count; jump short if count != 0 and  ZF = 0.
														
0F 03 /r 	LSL r16, r16/m16 	RM	W, R		E.ZF		YES	NO	V	V		lslw		Load: r16 = segment limit, selector r16/m16.
0F 03 /r 	LSL r32, r32/m16	RM	W, R		E.ZF		YES	NO	V	V		lsl		Load: r32 = segment limit, selector r32/m16.
REX.W+ 0F 03 /r	LSL r64, r32/m16	RM	W, R		E.ZF		YES	NO	V	V		lsl		Load: r64 = segment limit, selector r32/m16
														
0F 00 /3 	LTR r/m16 						NO	YES	V	V				Load r/m16 into task register.
														
F3 0F BD /r	LZCNT r16, r/m16	RM	W, R		E.ZF E.CF	E.AF E.PF E.SF E.OF			V	V	BMI1	lzcntw		Count the number of leading zero bits in r/m16, return result in r16
F3 0F BD /r	LZCNT r32, r/m32	RM	W, R		E.ZF E.CF	E.AF E.PF E.SF E.OF			V	V	BMI1	lzcntl		Count the number of leading zero bits in r/m32, return result in r32
REX.W+ F3 0F BD /r	LZCNT r64, r/m64	MR	W, R		E.ZF E.CF	E.AF E.PF E.SF E.OF			V	NE	BMI1	lzcntq		Count the number of leading zero bits in r/m64, return result in r64
														
66 0F F7 /r 	MASKMOVDQU xmm1, xmm2	RM	R, R						V	V	SSE2	maskmovdqu		Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
VEX.128.66.0F.WIG F7 /r	VMASKMOVDQU xmm1, xmm2	RM	R, R						V	V	AVX	vmaskmovdqu		Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
														
0F F7 /r 	MASKMOVQ mm1, mm2	RM	R, R						V	V		maskmovq		Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI.
														
66 0F 5F /r 	MAXPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	maxpd		Return the maximum double-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 5F /r	VMAXPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vmaxpd		Return the maximum double-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 5F /r	VMAXPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vmaxpd		Return the maximum packed double-precision floating-point values between ymm2 and ymm3/mem.
														
0F 5F /r 	MAXPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	maxps		Return the maximum single-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 5F /r	VMAXPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vmaxps		Return the maximum single-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 5F /r	VMAXPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vmaxps		Return the maximum single double-precision floating-point values between ymm2 and ymm3/mem.
														
F2 0F 5F /r 	MAXSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	maxsd		Return the maximum scalar double-precision floating-point value between xmm2/mem64 and xmm1.
VEX.NDS.LIG.F2.0F.WIG 5F /r	VMAXSD xmm1, xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vmaxsd		Return the maximum scalar double-precision floating-point value between xmm3/mem64 and xmm2.
														
F3 0F 5F /r 	MAXSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	maxss		Return the maximum scalar single-precision floating-point value between xmm2/mem32 and xmm1.
VEX.NDS.LIG.F3.0F.WIG 5F /r	VMAXSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vmaxss		Return the maximum scalar single-precision floating-point value between xmm3/mem32 and xmm2.
														
0F AE F0	MFENCE 	NP							V	V		mfence		Serializes load and store operations.
														
66 0F 5D /r 	MINPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	minpd		Return the minimum double-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 5D /r	VMINPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vminpd		Return the minimum double-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 5D /r	VMINPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vminpd		Return the minimum packed double-precision floating-point values between ymm2 and ymm3/mem.
														
0F 5D /r 	MINPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	minps		Return the minimum single-precision floating-point values between xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 5D /r	VMINPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vminps		Return the minimum single-precision floating-point values between xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 5D /r	VMINPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vminps		Return the minimum single double-precision floating-point values between ymm2 and ymm3/mem.
														
F2 0F 5D /r 	MINSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	minsd		Return the minimum scalar double-precision floating-point value between xmm2/mem64 and xmm1.
VEX.NDS.LIG.F2.0F.WIG 5D /r	VMINSD xmm1, xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vminsd		Return the minimum scalar double precision floating-point value between xmm3/mem64 and xmm2.
														
F3 0F 5D /r 	MINSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	minss		Return the minimum scalar single-precision floating-point value between xmm2/mem32 and xmm1.
VEX.NDS.LIG.F3.0F.WIG 5D /r	VMINSS xmm1,xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vminss		Return the minimum scalar single precision floating-point value between xmm3/mem32 and xmm2.
														
0F 01 C8 	MONITOR 	NP							V	V	MONITOR	monitor		Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write-back memory caching type. The address is DS:EAX (DS:RAX in 64-bit mode).
														
88 /r 	MOV r/m8, r8 	MR	W, R						V	V		movb	YES	Move r8 to r/m8.
REX+ 88 /r 	MOV r/m8, r8	MR	W, R						V	NE		movb	YES	Move r8 to r/m8.
89 /r 	MOV r/m16, r16 	MR	W, R						V	V		movw	YES	Move r16 to r/m16.
89 /r 	MOV r/m32, r32 	MR	W, R						V	V		movl	YES	Move r32 to r/m32.
REX.W+ 89 /r 	MOV r/m64, r64 	MR	W, R						V	NE		movq	YES	Move r64 to r/m64.
8A /r 	MOV r8, r/m8 	RM	W, R						V	V		movb		Move r/m8 to r8.
REX+ 8A /r 	MOV r8, r/m8	RM	W, R						V	NE		movb		Move r/m8 to r8.
8B /r 	MOV r16, r/m16 	RM	W, R						V	V		movw		Move r/m16 to r16.
8B /r 	MOV r32, r/m32 	RM	W, R						V	V		movl		Move r/m32 to r32.
REX.W+ 8B /r 	MOV r64, r/m64 	RM	W, R						V	NE		movq		Move r/m64 to r64.
8C /r 	MOV r/m16, Sreg	MR	W, R						V	V		movw		Move segment register to r/m16.
REX.W+ 8C /r 	MOV r/m64, Sreg	MR	W, R						V	V		movq		Move zero extended 16-bit segment register to r/m64.
8E /r 	MOV Sreg, r/m16	RM	W, R						V	V		mov		Move r/m16 to segment register.
REX.W+ 8E /r 	MOV Sreg, r/m64	RM	W, R						V	V		mov		Move lower 16 bits of r/m64 to segment register.
A0 	MOV AL, moffs8	FD	W, R						V	V		movabsb		Move byte at (seg:offset) to AL.
REX.W+ A0 	MOV AL, moffs8, pw	FD	W, R, I						V	NE		movabsb		Move byte at (offset) to AL.
A1 	MOV AX, moffs16	FD	W, R						V	V		movabsw		Move word at (seg:offset) to AX.
A1 	MOV EAX, moffs32	FD	W, R						V	V		movabsl		Move doubleword at (seg:offset) to EAX.
REX.W+ A1 	MOV RAX, moffs64	FD	W, R						V	NE		movabsq		Move quadword at (offset) to RAX.
A2 	MOV moffs8, AL 	TD	W, R						V	V		movabsb		Move AL to (seg:offset).
REX.W+ A2 	MOV moffs8, AL, pw 	TD	W, R, I						V	NE		movabsb		Move AL to (offset).
A3 	MOV moffs16, AX 	TD	W, R						V	V		movabsw		Move AX to (seg:offset).
A3 	MOV moffs32, EAX 	TD	W, R						V	V		movabsl		Move EAX to (seg:offset).
REX.W+ A3 	MOV moffs64, RAX 	TD	W, R						V	NE		movabsq		Move RAX to (offset).
B0 +rb 	MOV r8, imm8 	OI	W, R						V	V		movb	YES	Move imm8 to r8.
REX+ B0 +rb	MOV r8, imm8 	OI	W, R						V	NE		movb	YES	Move imm8 to r8.
B8 +rw 	MOV r16, imm16 	OI	W, R						V	V		movw	YES	Move imm16 to r16.
B8 +rd 	MOV r32, imm32 	OI	W, R						V	V		movl	YES	Move imm32 to r32.
REX.W+ B8 +rd 	MOV r64, imm64 	OI	W, R						V	NE		movq		Move imm64 to r64.
C6 /0 	MOV r/m8, imm8 	MI	W, R						V	V		movb		Move imm8 to r/m8.
REX+ C6 /0 	MOV r/m8, imm8 	MI	W, R						V	NE		movb		Move imm8 to r/m8.
C7 /0 	MOV r/m16, imm16 	MI	W, R						V	V		movw		Move imm16 to r/m16.
C7 /0 	MOV r/m32, imm32 	MI	W, R						V	V		movl		Move imm32 to r/m32.
REX.W+ C7 /0 	MOV r/m64, imm32 	MI	W, R						V	NE		movq	YES	Move imm32 sign extended to 64-bits to r/m64.
														
0F 20 /r 	MOV r32, CR0-CR7						NO	YES	NE	V				Move control register to r32.
0F 20 /r	MOV r64, CR0-CR7						NO	YES	V	NE				Move extended control register to r64.
REX.R+ 0F 20	MOV r64, CR8						NO	YES	V	NE				Move extended CR8 to r64.1
0F 22 /r	MOV CR0-CR7, r32						NO	YES	NE	V				Move r32 to control register.
0F 22 /r	MOV CR0-CR7, r64						NO	YES	V	NE				Move r64 to extended control register.
REX.R+ 0F 22	MOV CR8, r64						NO	YES	V	NE				Move r64 to extended CR8.1
														
0F 21 /r 	MOV r32, DR0-DR7						NO	YES	NE	V				Move debug register to r32.
0F 21 /r	MOV r64, DR0-DR7						NO	YES	V	NE				Move extended debug register to r64.
0F 23 /r	MOV DR0-DR7, r32						NO	YES	NE	V				Move r32 to debug register.
0F 23 /r	MOV DR0-DR7, r64						NO	YES	V	NE				Move r64 to extended debug register.
														
66 0F 28 /r 	MOVAPD xmm1, xmm2/m128	RM	W, R						V	V	SSE2	movapd		Move packed double-precision floating-point values from xmm2/m128 to xmm1.
66 0F 29 /r	MOVAPD xmm2/m128, xmm1	MR	W, R						V	V	SSE2	movapd	YES	Move packed double-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 28 /r	VMOVAPD xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovapd	YES	Move aligned packed double-precision floating-point values from xmm2/mem to xmm1.
VEX.128.66.0F.WIG 29 /r	VMOVAPD xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovapd		Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 28 /r	VMOVAPD ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovapd	YES	Move aligned packed double-precision floating-point values from ymm2/mem to ymm1.
VEX.256.66.0F.WIG 29 /r	VMOVAPD ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovapd		Move aligned packed double-precision floating-point values from ymm1 to ymm2/mem.
														
0F 28 /r 	MOVAPS xmm1, xmm2/m128	RM	W, R						V	V	SSE	movaps		Move packed single-precision floating-point values from xmm2/m128 to xmm1.
0F 29 /r	MOVAPS xmm2/m128, xmm1	MR	W, R						V	V	SSE	movaps	YES	Move packed single-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.0F.WIG 28 /r	VMOVAPS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovaps	YES	Move aligned packed single-precision floating-point values from xmm2/mem to xmm1.
VEX.128.0F.WIG 29 /r	VMOVAPS xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovaps		Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem.
VEX.256.0F.WIG 28 /r	VMOVAPS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovaps	YES	Move aligned packed single-precision floating-point values from ymm2/mem to ymm1.
VEX.256.0F.WIG 29 /r	VMOVAPS ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovaps		Move aligned packed single-precision floating-point values from ymm1 to ymm2/mem.
														
0F 38 F0 /r 	MOVBE r16, m16 	RM	W, R						V	V	MOVBE	movbew		Reverse byte order in m16 and move to r16
0F 38 F0 /r 	MOVBE r32, m32 	RM	W, R						V	V	MOVBE	movbel		Reverse byte order in m32 and move to r32
REX.W+ 0F 38 F0 /r 	MOVBE r64, m64 	RM	W, R						V	NE	MOVBE	movbeq		Reverse byte order in m64 and move to r64.
0F 38 F1 /r 	MOVBE m16, r16 	MR	W, R						V	V	MOVBE	movbew		Reverse byte order in r16 and move to m16
0F 38 F1 /r 	MOVBE m32, r32 	MR	W, R						V	V	MOVBE	movbel		Reverse byte order in r32 and move to m32
REX.W+ 0F 38 F1 /r 	MOVBE m64, r64 	MR	W, R						V	NE	MOVBE	movbeq		Reverse byte order in r64 and move to m64.
														
0F 6E /r 	MOVD mm, r/m32	RM	W, R						V	V	MMX	movd		Move doubleword from r/m32 to mm.
REX.W+ 0F 6E /r	MOVQ mm, r/m64	RM	W, R						V	NE	MMX	movq	YES	Move quadword from r/m64 to mm.
0F 7E /r	MOVD r/m32, mm	MR	W, R						V	V	MMX	movd		Move doubleword from mm to r/m32.
REX.W+ 0F 7E /r	MOVQ r/m64, mm	MR	W, R						V	NE	MMX	movq	YES	Move quadword from mm to r/m64.
VEX.128.66.0F.W0 6E /r	VMOVD xmm1, r32/m32	RM	Z, R						V	V	AVX	vmovd		Move doubleword from r/m32 to xmm1.
VEX.128.66.0F.W1 6E /r	VMOVQ xmm1, r64/m64	RM	Z, R						V	NE	AVX	vmovq	YES	Move quadword from r/m64 to xmm1.
66 0F 6E /r	MOVD xmm, r/m32	RM	W, R						V	V	SSE2	movd		Move doubleword from r/m32 to xmm.
66 REX.W+ 0F 6E /r	MOVQ xmm, r/m64	RM	W, R						V	NE	SSE2	movq	YES	Move quadword from r/m64 to xmm.
66 0F 7E /r	MOVD r/m32, xmm	MR	W, R						V	V	SSE2	movd		Move doubleword from xmm register to r/m32.
66 REX.W+ 0F 7E /r	MOVQ r/m64, xmm	MR	W, R						V	NE	SSE2	movq	YES	Move quadword from xmm register to r/m64.
VEX.128.66.0F.W0 7E /r	VMOVD r32/m32, xmm1	MR	W, R						V	V	AVX	vmovd		Move doubleword from xmm1 register to r/m32.
VEX.128.66.0F.W1 7E /r	VMOVQ r64/m64, xmm1	MR	W, R						V	NE	AVX	vmovq	YES	Move quadword from xmm1 register to r/m64.
														
F2 0F 12 /r 	MOVDDUP xmm1, xmm2/m64	RM	W, R						V	V	PNI	movddup		Move one double-precision floating-point value from the lower 64-bit operand in xmm2/m64 to xmm1 and duplicate.
VEX.128.F2.0F.WIG 12 /r	VMOVDDUP xmm1, xmm2/m64	RM	Z, R						V	V	AVX	vmovddup		Move double-precision floating-point values from xmm2/mem and duplicate into xmm1.
VEX.256.F2.0F.WIG 12 /r	VMOVDDUP ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovddup		Move even index double-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
														
66 0F 6F /r 	MOVDQA xmm1, xmm2/m128	RM	W, R						V	V	SSE2	movdqa	YES	Move aligned double quadword from xmm2/m128 to xmm1.
66 0F 7F /r	MOVDQA xmm2/m128, xmm1	MR	W, R						V	V	SSE2	movdqa		Move aligned double quadword from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 6F /r	VMOVDQA xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovdqa	YES	Move aligned packed integer values from xmm2/mem to xmm1.
VEX.128.66.0F.WIG 7F /r	VMOVDQA xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovdqa		Move aligned packed integer values from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 6F /r	VMOVDQA ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovdqa	YES	Move aligned packed integer values from ymm2/mem to ymm1.
VEX.256.66.0F.WIG 7F /r	VMOVDQA ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovdqa		Move aligned packed integer values from ymm1 to ymm2/mem.
														
F3 0F 6F /r 	MOVDQU xmm1, xmm2/m128	RM	W, R						V	V	SSE2	movdqu	YES	Move unaligned double quadword from xmm2/m128 to xmm1.
F3 0F 7F /r	MOVDQU xmm2/m128, xmm1	MR	W, R						V	V	SSE2	movdqu		Move unaligned double quadword from xmm1 to xmm2/m128.
VEX.128.F3.0F.WIG 6F /r	VMOVDQU xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovdqu	YES	Move unaligned packed integer values from xmm2/mem to xmm1.
VEX.128.F3.0F.WIG 7F /r	VMOVDQU xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovdqu		Move unaligned packed integer values from xmm1 to xmm2/mem.
VEX.256.F3.0F.WIG 6F /r	VMOVDQU ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovdqu	YES	Move unaligned packed integer values from ymm2/mem to ymm1.
VEX.256.F3.0F.WIG 7F /r	VMOVDQU ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovdqu		Move unaligned packed integer values from ymm1 to ymm2/mem.
														
F2 0F D6 	MOVDQ2Q mm, xmm 	RM	W, R						V	V	MMX	movdq2q		Move low quadword from xmm to mmx register.
														
0F 12 /r 	MOVHLPS xmm1, xmm2	RM	W, R						V	V	SSE	movhlps		Move two packed single-precision floating-point values from high quadword of xmm2 to low quadword of xmm1.
VEX.NDS.128.0F.WIG 12 /r	VMOVHLPS xmm1, xmm2, xmm3	RVM	Z, R, R						V	V	AVX	vmovhlps		Merge two packed single-precision floating-point values from high quadword of xmm3 and low quadword of xmm2.
														
66 0F 16 /r 	MOVHPD xmm, m64	RM	W, R						V	V	SSE2	movhpd		Move double-precision floating-point value from m64 to high quadword of xmm.
66 0F 17 /r	MOVHPD m64, xmm	MR	W, R						V	V	SSE2	movhpd		Move double-precision floating-point value from high quadword of xmm to m64.
VEX.NDS.128.66.0F.WIG 16 /r	VMOVHPD xmm2, xmm1, m64	RVM	Z, R, R						V	V	AVX	vmovhpd		Merge double-precision floating-point value from m64 and the low quadword of xmm1.
VEX128.66.0F.WIG 17 /r	VMOVHPD m64, xmm1	MR	W, R						V	V	AVX	vmovhpd		Move double-precision floating-point values from high quadword of xmm1 to m64.
														
0F 16 /r 	MOVHPS xmm, m64	RM	W, R						V	V	SSE	movhps		Move two packed single-precision floating-point values from m64 to high quadword of xmm.
0F 17 /r	MOVHPS m64, xmm	MR	W, R						V	V	SSE	movhps		Move two packed single-precision floating-point values from high quadword of xmm to m64.
VEX.NDS.128.0F.WIG 16 /r	VMOVHPS xmm2, xmm1, m64	RVM	Z, R, R						V	V	AVX	vmovhps		Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1.
VEX.128.0F.WIG 17 /r	VMOVHPS m64, xmm1	MR	W, R						V	V	AVX	vmovhps		Move two packed single-precision floating-point values from high quadword of xmm1to m64.
														
0F 16 /r 	MOVLHPS xmm1, xmm2	RM	W, R						V	V	SSE	movlhps		Move two packed single-precision floating-point values from low quadword of xmm2 to high quadword of xmm1.
VEX.NDS.128.0F.WIG 16 /r	VMOVLHPS xmm1, xmm2, xmm3	RVM	Z, R, R						V	V	AVX	vmovlhps		Merge two packed single-precision floating-point values from low quadword of xmm3 and low quadword of xmm2.
														
66 0F 12 /r 	MOVLPD xmm, m64	RM	W, R						V	V	SSE2	movlpd		Move double-precision floating-point value from m64 to low quadword of xmm register.
66 0F 13 /r	MOVLPD m64, xmm	MR	W, R						V	V	SSE2	movlpd		Move double-precision floating-point nvalue from low quadword of xmm register to m64.
VEX.NDS.128.66.0F.WIG 12 /r	VMOVLPD xmm2, xmm1, m64	RVM	Z, R, R						V	V	AVX	vmovlpd		Merge double-precision floating-point value from m64 and the high quadword of xmm1.
VEX.128.66.0F.WIG 13 /r	VMOVLPD m64, xmm1	MR	W, R						V	V	AVX	vmovlpd		Move double-precision floating-point values from low quadword of xmm1 to m64.
														
0F 12 /r 	MOVLPS xmm, m64	RM	W, R						V	V	SSE	movlps		Move two packed single-precision floating-point values from m64 to low quadword of xmm.
0F 13 /r	MOVLPS m64, xmm	MR	W, R						V	V	SSE	movlps		Move two packed single-precision floating-point values from low quadword of xmm to m64.
VEX.NDS.128.0F.WIG 12 /r	VMOVLPS xmm2, xmm1, m64	RVM	Z, R, R						V	V	AVX	vmovlps		Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1.
VEX.128.0F.WIG 13 /r	VMOVLPS m64, xmm1	MR	W, R						V	V	AVX	vmovlps		Move two packed single-precision floating-point values from low quadword of xmm1 to m64.
														
66 0F 50 /r 	MOVMSKPD reg, xmm	RM	W, R						V	V	SSE2	movmskpd		Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
VEX.128.66.0F.WIG 50 /r	VMOVMSKPD reg, xmm2	RM	W, R						V	V	AVX	vmovmskpd		Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VEX.256.66.0F.WIG 50 /r	VMOVMSKPD reg, ymm2	RM	W, R						V	V	AVX	vmovmskpd		Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
														
0F 50 /r 	MOVMSKPS reg, xmm	RM	W, R						V	V	SSE	movmskps		Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
VEX.128.0F.WIG 50 /r	VMOVMSKPS reg, xmm2	RM	W, R						V	V	AVX	vmovmskps		Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VEX.256.0F.WIG 50 /r	VMOVMSKPS reg, ymm2	RM	W, R						V	V	AVX	vmovmskps		Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
														
66 0F 38 2A /r 	MOVNTDQA xmm1, m128	RM	W, R						V	V	SSE4_1	movntdqa		Move double quadword from m128 to xmm using non-temporal hint if WC memory type.
VEX.128.66.0F38.WIG 2A /r	VMOVNTDQA xmm1, m128	RM	Z, R						V	V	AVX	vmovntdqa		Move double quadword from m128 to xmm using non-temporal hint if WC memory type.
VEX.256.66.0F38.WIG 2A /r	VMOVNTDQA ymm1, m256	RM	W, R						V	V	AVX2	vmovntdqa		Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type.
														
66 0F E7 /r 	MOVNTDQ m128, xmm	MR	W, R						V	V	SSE2	movntdq		Move double quadword from xmm to m128 using non-temporal hint.
VEX.128.66.0F.WIG E7 /r	VMOVNTDQ m128, xmm1	MR	W, R						V	V	AVX	vmovntdqa		Move packed integer values in xmm1 to m128 using non-temporal hint.
VEX.256.66.0F.WIG E7 /r	MOVNTDQ m256, ymm1	MR	W, R						V	V	AVX	movntdq		Move packed integer values in ymm1 to m256 using non-temporal hint.
														
0F C3 /r 	MOVNTI m32, r32 	MR	W, R						V	V		movnti		Move doubleword from r32 to m32 using non-temporal hint.
REX.W+ 0F C3 /r 	MOVNTI m64, r64 	MR	W, R						V	NE		movnti		Move quadword from r64 to m64 using non-temporal hint.
														
66 0F 2B /r 	MOVNTPD m128, xmm	MR	W, R						V	V	SSE2	movntpd		Move packed double-precision floating-point values from xmm to m128 using non-temporal hint.
VEX.128.66.0F.WIG 2B /r	VMOVNTPD m128, xmm1	MR	W, R						V	V	AVX	vmovntpd		Move packed double-precision values in xmm1 to m128 using non-temporal hint.
VEX.256.66.0F.WIG 2B /r	VMOVNTPD m256, ymm1	MR	W, R						V	V	AVX	vmovntpd		Move packed double-precision values in ymm1 to m256 using non-temporal hint.
														
0F 2B /r 	MOVNTPS m128, xmm	MR	W, R						V	V	SSE	movntps		Move packed single-precision floating-point values from xmm to m128 using non-temporal hint.
VEX.128.0F.WIG 2B /r	VMOVNTPS m128, xmm1	MR	W, R						V	V	AVX	vmovntps		Move packed single-precision values xmm1 to mem using non-temporal hint.
VEX.256.0F.WIG 2B /r	VMOVNTPS m256, ymm1	MR	W, R						V	V	AVX	vmovntps		Move packed single-precision values ymm1 to mem using non-temporal hint.
														
0F E7 /r 	MOVNTQ m64, mm 	MR	W, R						V	V		movntq		Move quadword from mm to m64 using non-temporal hint.
														
0F 6F /r 	MOVQ mm, mm/m64	RM	W, R						V	V	MMX	movq	YES	Move quadword from mm/m64 to mm.
0F 7F /r	MOVQ mm/m64, mm	MR	W, R						V	V	MMX	movq		Move quadword from mm to mm/m64.
F3 0F 7E	MOVQ xmm1, xmm2/m64	RM	W, R						V	V	SSE2	movq	YES	Move quadword from xmm2/mem64 to xmm1.
VEX.128.F3.0F.WIG 7E /r	VMOVQ xmm1, xmm2	RM	Z, R						V	V	AVX	vmovq		Move quadword from xmm2 to xmm1.
VEX.128.F3.0F.WIG 7E /r	VMOVQ xmm1, m64	RM	Z, R						V	V	AVX	vmovq		Load quadword from m64 to xmm1.
66 0F D6	MOVQ xmm2/m64, xmm1	MR	W, R						V	V	SSE2	movq		Move quadword from xmm1 to xmm2/mem64.
VEX.128.66.0F.WIG D6 /r	VMOVQ xmm1/m64, xmm2	MR	Z, R						V	V	AVX	vmovq	YES	Move quadword from xmm2 register to xmm1/m64.
														
F3 0F D6 	MOVQ2DQ xmm, mm 	RM	W, R						V	V	MMX	movq2dq		Move quadword from mmx to low quadword of xmm.
														
A4 	MOVS m8, m8 	NP	I, I	E.DF ESI rsi EDI rdi					V	V		movs		For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
A5 	MOVS m16, m16 	NP	I, I	E.DF ESI rsi EDI rdi					V	V		movs		For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
A5 	MOVS m32, m32 	NP	I, I	E.DF ESI rsi EDI rdi					V	V		movs		For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
REX.W+ A5 	MOVS m64, m64 	NP	I, I	E.DF ESI rsi EDI rdi					V	NE		movs		Move qword from address (R|E)SI to (R|E)DI.
A4 	MOVSB 	NP		E.DF ESI rsi EDI rdi					V	V		movsb		For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
PREF.66+ A5 	MOVSW 	NP		E.DF ESI rsi EDI rdi					V	V		movsw		For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
A5 	MOVSD 	NP		E.DF ESI rsi EDI rdi					V	V		movsl		For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
REX.W+ A5 	MOVSQ 	NP		E.DF ESI rsi EDI rdi					V	NE		movsq		Move qword from address (R|E)SI to (R|E)DI.
														
F2 0F 10 /r 	MOVSD xmm1, xmm2/m64	RM	W, R						V	V	SSE2	movsd	YES	Move scalar double-precision floating-point value from xmm2/m64 to xmm1 register.
VEX.NDS.LIG.F2.0F.WIG 10 /r	VMOVSD xmm1, xmm2, xmm3	RVM	W, R, R						V	V	AVX	vmovsd		Merge scalar double-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F2.0F.WIG 10 /r	VMOVSD xmm1, m64	XM	Z, R						V	V	AVX	vmovsd		Load scalar double-precision floating-point value from m64 to xmm1 register.
F2 0F 11 /r	MOVSD xmm2/m64, xmm1	MR	W, R						V	V	SSE2	movsd		Move scalar double-precision floating-point value from xmm1 register to xmm2/m64.
VEX.NDS.LIG.F2.0F.WIG 11 /r	VMOVSD xmm1, xmm2, xmm3	MVR	Z, R, R						V	V	AVX	vmovsd	YES	Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1.
VEX.LIG.F2.0F.WIG 11 /r	VMOVSD m64, xmm1	MR	W, R						V	V	AVX	vmovsd		Move scalar double-precision floating-point value from xmm1 register to m64.
														
F3 0F 16 /r 	MOVSHDUP xmm1, xmm2/m128	RM	W, R						V	V	PNI	movshdup		Move two single-precision floating-point values from the higher 32-bit operand of each qword in xmm2/m128 to xmm1 and duplicate each 32-bit operand to the lower 32-bits of each qword.
VEX.128.F3.0F.WIG 16 /r	VMOVSHDUP xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovshdup		Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VEX.256.F3.0F.WIG 16 /r	VMOVSHDUP ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovshdup		Move odd index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
														
F3 0F 12 /r 	MOVSLDUP xmm1, xmm2/m128	RM	W, R						V	V	PNI	movsldup		Move two single-precision floating-point values from the lower 32-bit operand of each qword in xmm2/m128 to xmm1 and duplicate each 32-bit operand to the higher 32-bits of each qword.
VEX.128.F3.0F.WIG 12 /r	VMOVSLDUP xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovsldup		Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VEX.256.F3.0F.WIG 12 /r	VMOVSLDUP ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovsldup		Move even index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
														
F3 0F 10 /r 	MOVSS xmm1, xmm2/m32	RM	W, R						V	V	SSE	movss	YES	Move scalar single-precision floating-point value from xmm2/m32 to xmm1 register.
VEX.NDS.LIG.F3.0F.WIG 10 /r	VMOVSS xmm1, xmm2, xmm3	RVM	Z, R, R						V	V	AVX	vmovss		Merge scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F3.0F.WIG 10 /r	VMOVSS xmm1, m32	XM	Z, R						V	V	AVX	vmovss		Load scalar single-precision floating-point value from m32 to xmm1 register.
F3 0F 11 /r	MOVSS xmm2/m32, xmm	MR	W, R						V	V	SSE	movss		Move scalar single-precision floating-point value from xmm1 register to xmm2/m32.
VEX.NDS.LIG.F3.0F.WIG 11 /r	VMOVSS xmm1, xmm2, xmm3	MVR	Z, R, R						V	V	AVX	vmovss	YES	Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VEX.LIG.F3.0F.WIG 11 /r	VMOVSS m32, xmm1	MR	W, R						V	V	AVX	vmovss		Move scalar single-precision floating-point value from xmm1 register to m32.
														
0F BE /r 	MOVSX r16, r/m8 	RM	W, R						V	V		movsbw		Move byte to word with sign-extension.
REX+ 0F BE /r 	MOVSX r16, r/m8 	RM	W, R						V	V		movsbw		Move byte to word with sign-extension.
0F BE /r 	MOVSX r32, r/m8 	RM	W, R						V	V		movsbl		Move byte to doubleword with sign-extension.
REX+ 0F BE /r 	MOVSX r32, r/m8 	RM	W, R						V	V		movsbl		Move byte to doubleword with sign-extension.
REX.W+ 0F BE /r 	MOVSX r64, r/m8	RM	W, R						V	NE		movsbq		Move byte to quadword with sign-extension.
0F BF /r 	MOVSX r32, r/m16 	RM	W, R						V	V		movswl		Move word to doubleword, with sign-extension.
REX.W+ 0F BF /r 	MOVSX r64, r/m16 	RM	W, R						V	NE		movswq		Move word to quadword with sign-extension.
REX.W+ 63 /r 	MOVSXD r64, r/m32 	RM	W, R						V	NE		movslq		Move doubleword to quadword with sign-extension.
														
66 0F 10 /r 	MOVUPD xmm1, xmm2/m128	RM	W, R						V	V	SSE2	movupd	YES	Move packed double-precision floating-point values from xmm2/m128 to xmm1.
VEX.128.66.0F.WIG 10 /r	VMOVUPD xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovupd		Move unaligned packed double-precision floating-point from xmm2/mem to xmm1.
VEX.256.66.0F.WIG 10 /r	VMOVUPD ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovupd		Move unaligned packed double-precision floating-point from ymm2/mem to ymm1.
66 0F 11 /r	MOVUPD xmm2/m128, xmm	MR	W, R						V	V	SSE2	movupd		Move packed double-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.66.0F.WIG 11 /r	VMOVUPD xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovupd	YES	Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem.
VEX.256.66.0F.WIG 11 /r	VMOVUPD ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovupd	YES	Move unaligned packed double-precision floating-point from ymm1 to ymm2/mem.
														
0F 10 /r 	MOVUPS xmm1, xmm2/m128	RM	W, R						V	V	SSE	movups	YES	Move packed single-precision floating-point values from xmm2/m128 to xmm1.
VEX.128.0F.WIG 10 /r	VMOVUPS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vmovups		Move unaligned packed single-precision floating-point from xmm2/mem to xmm1.
VEX.256.0F.WIG 10 /r	VMOVUPS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vmovups		Move unaligned packed single-precision floating-point from ymm2/mem to ymm1.
0F 11 /r	MOVUPS xmm2/m128, xmm1	MR	W, R						V	V	SSE	movups		Move packed single-precision floating-point values from xmm1 to xmm2/m128.
VEX.128.0F.WIG 11 /r	VMOVUPS xmm2/m128, xmm1	MR	Z, R						V	V	AVX	vmovups	YES	Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem.
VEX.256.0F.WIG 11 /r	VMOVUPS ymm2/m256, ymm1	MR	W, R						V	V	AVX	vmovups	YES	Move unaligned packed single-precision floating-point from ymm1 to ymm2/mem.
														
0F B6 /r 	MOVZX r16, r/m8 	RM	W, R						V	V		movzbw		Move byte to word with zero-extension.
REX+ 0F B6 /r 	MOVZX r16, r/m8 	RM	W, R						V	V		movzbw		Move byte to word with zero-extension.
0F B6 /r 	MOVZX r32, r/m8 	RM	W, R						V	V		movzbl		Move byte to doubleword, zero-extension.
REX+ 0F B6 /r 	MOVZX r32, r/m8 	RM	W, R						V	V		movzbl		Move byte to doubleword, zero-extension.
REX.W+ 0F B6 /r 	MOVZX r64, r/m8	RM	W, R						V	NE		movzbq		Move byte to quadword, zero-extension.
0F B7 /r 	MOVZX r32, r/m16 	RM	W, R						V	V		movzwl		Move word to doubleword, zero-extension.
REX.W+ 0F B7 /r 	MOVZX r64, r/m16 	RM	W, R						V	NE		movzwq		Move word to quadword, zero-extension.
														
66 0F 3A 42 /r ib 	MPSADBW xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE4_1	mpsadbw		Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8.
VEX.NDS.128.66.0F3A.WIG 42 /r ib	VMPSADBW xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vmpsadbw		Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8.
VEX.NDS.256.66.0F3A.WIG 42 /r ib	VMPSADBW ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX2	vmpsadbw		Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by imm8.
														
F6 /4 	MUL r/m8 	M	R	AL	AX E.OF E.CF	E.SF E.ZF E.AF E.PF			V	V		mulb		Unsigned multiply (AX = AL * r/m8).
REX+ F6 /4 	MUL r/m8	M	R	AL	AX E.OF E.CF	E.SF E.ZF E.AF E.PF			V	NE		mulb		Unsigned multiply (AX = AL * r/m8).
F7 /4 	MUL r/m16 	M	R	AX	AX DX E.OF E.CF	E.SF E.ZF E.AF E.PF			V	V		mulw		Unsigned multiply (DX:AX = AX * r/m16).
F7 /4 	MUL r/m32 	M	R	EAX	EAX EDX E.OF E.CF	E.SF E.ZF E.AF E.PF			V	V		mull		Unsigned multiply (EDX:EAX = EAX * r/m32).
REX.W+ F7 /4 	MUL r/m64 	M	R	RAX	RAX RDX E.OF E.CF	E.SF E.ZF E.AF E.PF			V	NE		mulq		Unsigned multiply (RDX:RAX = RAX * r/m64.
														
66 0F 59 /r 	MULPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	mulpd		Multiply packed double-precision floating-point values in xmm2/m128 by xmm1.
VEX.NDS.128.66.0F.WIG 59 /r	VMULPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vmulpd		Multiply packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 59 /r	VMULPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vmulpd		Multiply packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
0F 59 /r 	MULPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	mulps		Multiply packed single-precision floating-point values in xmm2/mem by xmm1.
VEX.NDS.128.0F.WIG 59 /r	VMULPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vmulps		Multiply packed single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 59 /r	VMULPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vmulps		Multiply packed single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
														
F2 0F 59 /r 	MULSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	mulsd		Multiply the low double-precision floating-point value in xmm2/mem64 by low double-precision floating-point value in xmm1.
VEX.NDS.LIG.F2.0F.WIG 59 /r	VMULSD xmm1,xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vmulsd		Multiply the low double-precision floating-point value in xmm3/mem64 by low double precision floating-point value in xmm2.
														
F3 0F 59 /r 	MULSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	mulss		Multiply the low single-precision floating-point value in xmm2/mem by the low single-precision floating-point value in xmm1.
VEX.NDS.LIG.F3.0F.WIG 59 /r	VMULSS xmm1,xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vmulss		Multiply the low single-precision floating-point value in xmm3/mem by the low single-precision floating-point value in xmm2.
														
VEX.NDD.LZ.F2.0F38.W0 F6 /r	MULX r32a, r32b, r/m32	RVM	W, W, R	EDX					V	V	BMI2	mulxl		Unsigned multiply of r/m32 with EDX without affecting arithmetic flags
VEX.NDD.LZ.F2.0F38.W1 F6 /r	MULX r64a, r64b, r/m64	RVM	W, W, R	RDX					V	NE	BMI2	mulxq		Unsigned multiply of r/m64 with RDX without affecting arithmetic flags
														
0F 01 C9 	MWAIT 	NP		EAX ECX					V	V	MONITOR	mwait		A hint that allow the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.
														
F6 /3 	NEG r/m8 	M	RW		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		negb		Two's complement negate r/m8.
REX+ F6 /3 	NEG r/m8	M	RW		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		negb		Two's complement negate r/m8.
F7 /3 	NEG r/m16 	M	RW		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		negw		Two's complement negate r/m16.
F7 /3 	NEG r/m32 	M	RW		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V		negl		Two's complement negate r/m32.
REX.W+ F7 /3 	NEG r/m64 	M	RW		E.CF E.OF E.SF E.ZF E.AF E.PF				V	NE		negq		Two's complement negate r/m64.
														
90	NOP 	NP							V	V		nop		One byte no-operation instruction.
0F 1F /0 	NOP r/m16 	M	I						V	V	NOPL	nopw		Multi-byte no-operation instruction.
0F 1F /0 	NOP r/m32 	M	I						V	V	NOPL	nopl		Multi-byte no-operation instruction.
														
F6 /2 	NOT r/m8 	M	RW						V	V		notb		Reverse each bit of r/m8.
REX+ F6 /2 	NOT r/m8	M	RW						V	NE		notb		Reverse each bit of r/m8.
F7 /2 	NOT r/m16 	M	RW						V	V		notw		Reverse each bit of r/m16.
F7 /2 	NOT r/m32 	M	RW						V	V		notl		Reverse each bit of r/m32.
REX.W+ F7 /2 	NOT r/m64 	M	RW						V	NE		notq		Reverse each bit of r/m64.
														
0C ib 	OR AL, imm8 	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orb		AL OR imm8.
0D iw 	OR AX, imm16 	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orw		AX OR imm16.
0D id 	OR EAX, imm32 	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orl		EAX OR imm32.
REX.W+ 0D id 	OR RAX, imm32 	I	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orq		RAX OR imm32 (sign-extended).
80 /1 ib 	OR r/m8, imm8 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orb		r/m8 OR imm8.
REX+ 80 /1 ib 	OR r/m8, imm8 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orb		r/m8 OR imm8.
81 /1 iw 	OR r/m16, imm16 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orw		r/m16 OR imm16.
81 /1 id 	OR r/m32, imm32 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orl		r/m32 OR imm32.
REX.W+ 81 /1 id 	OR r/m64, imm32 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orq		r/m64 OR imm32 (sign-extended).
83 /1 ib 	OR r/m16, imm8 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orw	YES	r/m16 OR imm8 (sign-extended).
83 /1 ib 	OR r/m32, imm8 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orl	YES	r/m32 OR imm8 (sign-extended).
REX.W+ 83 /1 ib 	OR r/m64, imm8 	MI	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orq	YES	r/m64 OR imm8 (sign-extended).
08 /r 	OR r/m8, r8 	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orb	YES	r/m8 OR r8.
REX+ 08 /r 	OR r/m8, r8	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orb	YES	r/m8 OR r8.
09 /r 	OR r/m16, r16 	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orw	YES	r/m16 OR r16.
09 /r 	OR r/m32, r32 	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orl	YES	r/m32 OR r32.
REX.W+ 09 /r 	OR r/m64, r64 	MR	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orq	YES	r/m64 OR r64.
0A /r 	OR r8, r/m8 	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orb		r8 OR r/m8.
REX+ 0A /r 	OR r8, r/m8	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orb		r8 OR r/m8.
0B /r 	OR r16, r/m16 	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orw		r16 OR r/m16.
0B /r 	OR r32, r/m32 	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	V		orl		r32 OR r/m32.
REX.W+ 0B /r 	OR r64, r/m64 	RM	RW, R		E.OF E.CF E.SF E.ZF E.PF	E.AF			V	NE		orq		r64 OR r/m64.
														
66 0F 56 /r 	ORPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	orpd		Bitwise OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 56 /r 	VORPD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vorpd		Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 56 /r 	VORPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vorpd		Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/mem.
														
0F 56 /r 	ORPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	orps		Bitwise OR of xmm1 and xmm2/m128.
VEX.NDS.128.0F.WIG 56 /r	VORPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vorps		Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 56 /r	VORPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vorps		Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/mem.
														
E6 ib 	OUT imm8, AL 	I	R, R	E.IOPL E.VM					V	V		outb		Output byte in AL to I/O port address imm8.
E7 ib 	OUT imm8, AX 	I	R, R	E.IOPL E.VM					V	V		outw		Output word in AX to I/O port address imm8.
E7 ib 	OUT imm8, EAX 	I	R, R	E.IOPL E.VM					V	V		outl		Output doubleword in EAX to I/O port address imm8.
EE 	OUT DX, AL 	NP	R, R	E.IOPL E.VM					V	V		outb		Output byte in AL to I/O port address in DX.
EF 	OUT DX, AX 	NP	R, R	E.IOPL E.VM					V	V		outw		Output word in AX to I/O port address in DX.
EF 	OUT DX, EAX 	NP	R, R	E.IOPL E.VM					V	V		outl		Output doubleword in EAX to I/O port address in DX.
														
6E 	OUTS DX, m8 	NP	R, I	E.IOPL E.VM E.DF RSI					V	V		outs		Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTS DX, m16 	NP	R, I	E.IOPL E.VM E.DF RSI					V	V		outs		Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTS DX, m32 	NP	R, I	E.IOPL E.VM E.DF RSI					V	V		outs		Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6E 	OUTSB 	NP		E.IOPL E.VM E.DF DX RSI					V	V		outsb		Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
PREF.66+ 6F 	OUTSW 	NP		E.IOPL E.VM E.DF DX RSI					V	V		outsw		Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
6F 	OUTSD 	NP		E.IOPL E.VM E.DF DX RSI					V	V		outsl		Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
														
0F 38 1C /r 	PABSB mm1, mm2/m64	RM	W, R						V	V	SSSE3	pabsb		Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1C /r	PABSB xmm1, xmm2/m128	RM	W, R						V	V	SSSE3	pabsb		Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
0F 38 1D /r	PABSW mm1, mm2/m64	RM	W, R						V	V	SSSE3	pabsw		Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1D /r	PABSW xmm1, xmm2/m128	RM	W, R						V	V	SSSE3	pabsw		Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
0F 38 1E /r	PABSD mm1, mm2/m64	RM	W, R						V	V	SSSE3	pabsd		Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1.
66 0F 38 1E /r	PABSD xmm1, xmm2/m128	RM	W, R						V	V	SSSE3	pabsd		Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1C /r	VPABSB xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vpabsb		Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1D /r	VPABSW xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vpabsw		Compute the absolute value of 16- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.128.66.0F38.WIG 1E /r	VPABSD xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vpabsd		Compute the absolute value of 32- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VEX.256.66.0F38.WIG 1C /r	VPABSB ymm1, ymm2/m256	RM	W, R						V	V	AVX2	vpabsb		Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1.
VEX.256.66.0F38.WIG 1D /r	VPABSW ymm1, ymm2/m256	RM	W, R						V	V	AVX2	vpabsw		Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1
VEX.256.66.0F38.WIG 1E /r	VPABSD ymm1, ymm2/m256	RM	W, R						V	V	AVX2	vpabsd		Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1.
														
0F 63 /r 	PACKSSWB mm1, mm2/m64	RM	RW, R						V	V	MMX	packsswb		Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation.
66 0F 63 /r	PACKSSWB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	packsswb		Converts 8 packed signed word integers from xmm1 and from xxm2/m128 into 16 packed signed byte integers in xxm1 using signed saturation.
0F 6B /r	PACKSSDW mm1, mm2/m64	RM	RW, R						V	V	MMX	packssdw		Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation.
66 0F 6B /r	PACKSSDW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	packssdw		Converts 4 packed signed doubleword integers from xmm1 and from xxm2/m128 into 8 packed signed word integers in xxm1 using signed saturation.
VEX.NDS.128.66.0F.WIG 63 /r	VPACKSSWB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpacksswb		Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation.
VEX.NDS.128.66.0F.WIG 6B /r	VPACKSSDW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpackssdw		Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation.
VEX.NDS.256.66.0F.WIG 63 /r	VPACKSSWB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpacksswb		Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation.
VEX.NDS.256.66.0F.WIG 6B /r	VPACKSSDW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpackssdw		Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation.
														
66 0F 38 2B /r 	PACKUSDW xmm1, xmm2/m128 	RM	W, R						V	V	SSE4_1	packusdw		Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
VEX.NDS.128.66.0F38.WIG 2B /r 	VPACKUSDW xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpackusdw		Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
VEX.NDS.256.66.0F38.WIG 2B /r	VPACKUSDW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpackusdw		Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m128 into 16 packed unsigned word integers in ymm1 using unsigned saturation.
														
0F 67 /r 	PACKUSWB mm, mm/m64	RM	RW, R						V	V	MMX	packuswb		Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation.
66 0F 67 /r	PACKUSWB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	packuswb		Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
VEX.NDS.128.66.0F.WIG 67 /r	VPACKUSWB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpackuswb		Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
VEX.NDS.256.66.0F.WIG 67 /r	VPACKUSWB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpackuswb		Converts 16 signed word integers from ymm2 And 16 signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation.
														
0F FC /r 	PADDB mm, mm/m64	RM	RW, R						V	V	MMX	paddb		Add packed byte integers from mm/m64 and mm.
66 0F FC /r	PADDB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddb		Add packed byte integers from xmm2/m128 and xmm1.
0F FD /r	PADDW mm, mm/m64	RM	RW, R						V	V	MMX	paddw		Add packed word integers from mm/m64 and mm.
66 0F FD /r	PADDW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddw		Add packed word integers from xmm2/m128 and xmm1.
0F FE /r	PADDD mm, mm/m64	RM	RW, R						V	V	MMX	paddd		Add packed doubleword integers from mm/m64 and mm.
66 0F FE /r	PADDD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddd		Add packed doubleword integers from xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG FC /r	VPADDB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddb		Add packed byte integers from xmm3/m128 and xmm2.
VEX.NDS.128.66.0F.WIG FD /r	VPADDW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddw		Add packed word integers from xmm3/m128 and xmm2.
VEX.NDS.128.66.0F.WIG FE /r	VPADDD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddd		Add packed doubleword integers from xmm3/m128 and xmm2.
VEX.NDS.256.66.0F.WIG FC /r	VPADDB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddb		Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1.
VEX.NDS.256.66.0F.WIG FD /r	VPADDW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddw		Add packed word integers from ymm2, ymm3/m256 and store in ymm1.
VEX.NDS.256.66.0F.WIG FE /r	VPADDD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddd		Add packed doubleword integers from ymm2, ymm3/m256 and store in ymm1.
														
0F D4 /r 	PADDQ mm1, mm2/m64	RM	RW, R						V	V	SSE2	paddq		Add quadword integer mm2/m64 to mm1.
66 0F D4 /r	PADDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddq		Add packed quadword integers xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG D4 /r	VPADDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddq		Add packed quadword integers xmm3/m128 and xmm2.
VEX.NDS.256.66.0F.WIG D4 /r	VPADDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddq		Add packed quadword integers from ymm2, ymm3/m256 and store in ymm1.
														
0F EC /r 	PADDSB mm, mm/m64	RM	RW, R						V	V	MMX	paddsb		Add packed signed byte integers from mm/m64 and mm and saturate the results.
66 0F EC /r	PADDSB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddsb		Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results.
0F ED /r	PADDSW mm, mm/m64	RM	RW, R						V	V	MMX	paddsw		Add packed signed word integers from mm/m64 and mm and saturate the results.
66 0F ED /r	PADDSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddsw		Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results.
VEX.NDS.128.66.0F.WIG EC /r 	VPADDSB xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpaddsb		Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results.
VEX.NDS.128.66.0F.WIG ED /r 	VPADDSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddsw		Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results.
VEX.NDS.256.66.0F.WIG EC /r	VPADDSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddsb		Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VEX.NDS.256.66.0F.WIG ED /r	VPADDSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddsw		Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
														
0F DC /r 	PADDUSB mm, mm/m64	RM	RW, R						V	V	MMX	paddusb		Add packed unsigned byte integers from mm/m64 and mm and saturate the results.
66 0F DC /r	PADDUSB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddusb		Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results.
0F DD /r	PADDUSW mm, mm/m64	RM	RW, R						V	V	MMX	paddusw		Add packed unsigned word integers from mm/m64 and mm and saturate the results.
66 0F DD /r	PADDUSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	paddusw		Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results.
VEX.NDS.128.66.0F.WIG DC /r	VPADDUSB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddusb		Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results.
VEX.NDS.128.66.0F.WIG DD /r	VPADDUSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpaddusw		Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results.
VEX.NDS.256.66.0F.WIG DC /r	VPADDUSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddusb		Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VEX.NDS.256.66.0F.WIG DD /r	VPADDUSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpaddusw		Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
														
0F 3A 0F 	PALIGNR mm1, mm2/m64, imm8	RMI	RW, R, R						V	V	SSSE3	palignr		Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1.
66 0F 3A 0F	PALIGNR xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSSE3	palignr		Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1
VEX.NDS.128.66.0F3A.WIG 0F /r ib	VPALIGNR xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vpalignr		Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
VEX.NDS.256.66.0F3A.WIG 0F /r ib	VPALIGNR ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX2	vpalignr		Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1.
														
0F DB /r 	PAND mm, mm/m64	RM	RW, R						V	V	MMX	pand		Bitwise AND mm/m64 and mm.
66 0F DB /r	PAND xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pand		Bitwise AND of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG DB /r	VPAND xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpand		Bitwise AND of xmm3/m128 and xmm.
VEX.NDS.256.66.0F.WIG DB /r	VPAND ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpand		Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1.
														
0F DF /r 	PANDN mm, mm/m64	RM	RW, R						V	V	MMX	pandn		Bitwise AND NOT of mm/m64 and mm.
66 0F DF /r	PANDN xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pandn		Bitwise AND NOT of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG DF /r	VPANDN xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpandn		Bitwise AND NOT of xmm3/m128 and xmm2.
VEX.NDS.256.66.0F.WIG DF /r	VPANDN ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpandn		Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1.
														
F3 90 	PAUSE 	NP							V	V		pause		Gives hint to processor that improves performance of spin-wait loops.
														
0F E0 /r 	PAVGB mm1, mm2/m64	RM	RW, R						V	V	SSE	pavgb		Average packed unsigned byte integers from mm2/m64 and mm1 with rounding.
66 0F E0 /r	PAVGB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pavgb		Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding.
0F E3 /r	PAVGW mm1, mm2/m64	RM	RW, R						V	V	SSE	pavgw		Average packed unsigned word integers from mm2/m64 and mm1 with rounding.
66 0F E3 /r	PAVGW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pavgw		Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding.
VEX.NDS.128.66.0F.WIG E0 /r	VPAVGB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpavgb		Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding.
VEX.NDS.128.66.0F.WIG E3 /r	VPAVGW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpavgw		Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding.
VEX.NDS.256.66.0F.WIG E0 /r	VPAVGB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpavgb		Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1.
VEX.NDS.256.66.0F.WIG E3 /r	VPAVGW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpavgw		Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1.
														
66 0F 38 10 /r 	PBLENDVB xmm1, xmm2/m128, <XMM0> 	RM	RW, R, R						V	V	SSE4_1	pblendvb		Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1.
VEX.NDS.128.66.0F3A.W0 4C /r /is4 	VPBLENDVB xmm1, xmm2, xmm3/m128, xmm4 	RVMR	Z, R, R, R						V	V	AVX	vpblendvb		Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1.
VEX.NDS.256.66.0F3A.W0 4C /r /is4	VPBLENDVB ymm1, ymm2, ymm3/m256, ymm4	RVMR	W, R, R, R						V	V	AVX2	vpblendvb		Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1.
														
66 0F 3A 0E /r ib 	PBLENDW xmm1, xmm2/m128, imm8 	RMI	RW, R, R						V	V	SSE4_1	pblendw		Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.128.66.0F3A.WIG 0E /r ib 	VPBLENDW xmm1, xmm2, xmm3/m128, imm8 	RVMI	Z, R, R, R						V	V	AVX	vpblendw		Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.256.66.0F3A.WIG 0E /r ib	VPBLENDW ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX2	vpblendw		Select words from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
														
66 0F 3A 44 /r ib 	PCLMULQDQ xmm1, xmm2/m128, imm8 	RMI	RW, R, R						V	V	PCLMULQDQ	pclmulqdq		Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm1 and xmm2/m128 should be used.
VEX.NDS.128.66.0F3A.WIG 44 /r ib 	VPCLMULQDQ xmm1, xmm2, xmm3/m128, imm8 	RVMI	Z, R, R, R						V	V	PCLMULQDQ AVX	vpclmulqdq		Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm2 and xmm3/m128 should be used.
														
0F 74 /r 	PCMPEQB mm, mm/m64	RM	RW, R						V	V	MMX	pcmpeqb		Compare packed bytes in mm/m64 and mm for equality.
66 0F 74 /r	PCMPEQB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpeqb		Compare packed bytes in xmm2/m128 and xmm1 for equality.
0F 75 /r	PCMPEQW mm, mm/m64	RM	RW, R						V	V	MMX	pcmpeqw		Compare packed words in mm/m64 and mm for equality.
66 0F 75 /r	PCMPEQW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpeqw		Compare packed words in xmm2/m128 and xmm1 for equality.
0F 76 /r	PCMPEQD mm, mm/m64	RM	RW, R						V	V	MMX	pcmpeqd		Compare packed doublewords in mm/m64 and mm for equality.
66 0F 76 /r	PCMPEQD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpeqd		Compare packed doublewords in xmm2/m128 and xmm1 for equality.
VEX.NDS.128.66.0F.WIG 74 /r	VPCMPEQB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpeqb		Compare packed bytes in xmm3/m128 and xmm2 for equality.
VEX.NDS.128.66.0F.WIG 75 /r	VPCMPEQW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpeqw		Compare packed words in xmm3/m128 and xmm2 for equality.
VEX.NDS.128.66.0F.WIG 76 /r	VPCMPEQD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpeqd		Compare packed doublewords in xmm3/m128 and xmm2 for equality.
VEX.NDS.256.66.0F.WIG 75 /r	VPCMPEQW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpcmpeqb		Compare packed words in ymm3/m256 and ymm2 for equality.
VEX.NDS.256.66.0F.WIG 76 /r	VPCMPEQD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpcmpeqw		Compare packed doublewords in ymm3/m256 and ymm2 for equality.
														
66 0F 38 29 /r 	PCMPEQQ xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pcmpeqq		Compare packed qwords in xmm2/m128 and xmm1 for equality.
VEX.NDS.128.66.0F38.WIG 29 /r 	VPCMPEQQ xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpcmpeqq		Compare packed quadwords in xmm3/m128 and xmm2 for equality.
VEX.NDS.256.66.0F38.WIG 29 /r	VPCMPEQQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpcmpeqq		Compare packed quadwords in ymm3/m256 and ymm2 for equality.
														
66 0F 3A 61 /r	PCMPESTRI xmm1, xmm2/m128, imm8 	RMI	R, R, R	EAX EDX	ECX E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	SSE4_2	pcmpestri		Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
VEX.128.66.0F3A.WIG 61 /r ib 	VPCMPESTRI xmm1, xmm2/m128, imm8 	RMI	R, R, R	EAX EDX	ECX E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	AVX	vpcmpestri		Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
														
66 0F 3A 60 /r	PCMPESTRM xmm1, xmm2/m128, imm8 	RMI	R, R, R	EAX EDX	XMM0 E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	SSE4_2	pcmpestrm		Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0
VEX.128.66.0F3A.WIG 60 /r ib 	VPCMPESTRM xmm1, xmm2/m128, imm8 	RMI	R, R, R	EAX EDX	XMM0 E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	AVX	vpcmpestrm		Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0.
														
0F 64 /r 	PCMPGTB mm, mm/m64	RM	RW, R						V	V	MMX	pcmpgtb		Compare packed signed byte integers in mm and mm/m64 for greater than.
66 0F 64 /r	PCMPGTB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpgtb		Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than.
0F 65 /r	PCMPGTW mm, mm/m64	RM	RW, R						V	V	MMX	pcmpgtw		Compare packed signed word integers in mm and mm/m64 for greater than.
66 0F 65 /r	PCMPGTW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpgtw		Compare packed signed word integers in xmm1 and xmm2/m128 for greater than.
0F 66 /r	PCMPGTD mm, mm/m64	RM	RW, R						V	V	MMX	pcmpgtd		Compare packed signed doubleword integers in mm and mm/m64 for greater than.
66 0F 66 /r	PCMPGTD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pcmpgtd		Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than.
VEX.NDS.128.66.0F.WIG 64 /r	VPCMPGTB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpgtb		Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than.
VEX.NDS.128.66.0F.WIG 65 /r	VPCMPGTW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpgtw		Compare packed signed word integers in xmm2 and xmm3/m128 for greater than.
VEX.NDS.128.66.0F.WIG 66 /r	VPCMPGTD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpcmpgtd		Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than.
VEX.NDS.256.66.0F.WIG 64 /r	VPCMPGTB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vpcmpgtb		Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than.
VEX.NDS.256.66.0F.WIG 65 /r	VPCMPGTW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpcmpgtw		Compare packed signed word integers in ymm2 and ymm3/m256 for greater than.
VEX.NDS.256.66.0F.WIG 66 /r	VPCMPGTD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpcmpgtd		Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than.
														
66 0F 38 37 /r 	PCMPGTQ xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_2	pcmpgtq		Compare packed signed qwords in xmm2/m128 and xmm1 for greater than.
VEX.NDS.128.66.0F38.WIG 37 /r 	VPCMPGTQ xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpcmpgtq		Compare packed signed qwords in xmm2 and xmm3/m128 for greater than.
VEX.NDS.256.66.0F38.WIG 37 /r	VPCMPGTQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpvmpgtq		Compare packed signed qwords in ymm2 and ymm3/m256 for greater than.
														
66 0F 3A 63 /r	PCMPISTRI xmm1, xmm2/m128, imm8 	RM	R, R, R		ECX E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	SSE4_2	pcmpistri		Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
VEX.128.66.0F3A.WIG 63 /r ib 	VPCMPISTRI xmm1, xmm2/m128, imm8 	RM	R, R, R		ECX E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	AVX	vpcmpistri		Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
														
66 0F 3A 62 /r	PCMPISTRM xmm1, xmm2/m128, imm8 	RM	R, R, R		XMM0 E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	SSE4_2	pcmpistrm		Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0.
VEX.128.66.0F3A.WIG 62 /r ib 	VPCMPISTRM xmm1, xmm2/m128, imm8 	RM	R, R, R		XMM0 E.CF E.ZF E.SF E.OF E.AF E.PF				V	V	AVX	vpcmpistrm		Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0.
														
VEX.NDS.LZ.F2.0F38.W0 F5 /r	PDEP r32a, r32b, r/m32	RVM	W, R, R						V	V	BMI2	pdepl		Parallel deposit of bits from r32b using mask in r/m32, result is written to r32a.
VEX.NDS.LZ.F2.0F38.W1 F5 /r	PDEP r64a, r64b, r/m64	RVM	W, R, R						V	NE	BMI2	pdelq		Parallel deposit of bits from r64b using mask in r/m64, result is written to r64a.
														
VEX.NDS.LZ.F3.0F38.W0 F5 /r	PEXT r32a, r32b, r/m32	RVM	W, R, R						V	V	BMI2	pextl		Parallel extract of bits from r32b using mask in r/m32, result is written to r32a.
VEX.NDS.LZ.F3.0F38.W1 F5 /r	PEXT r64a, r64b, r/m64	RVM	W, R, R						V	NE	BMI2	pextq		Parallel extract of bits from r64b using mask in r/m64, result is written to r64a.
														
66 0F 3A 14 /r ib 	PEXTRB reg/m8, xmm2, imm8 	MRI	W, R, R						V	V	SSE4_1	pextrb		Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into rreg or m8. The upper bits of r32 or r64 are zeroed.
66 0F 3A 16 /r ib 	PEXTRD r/m32, xmm2, imm8 	MRI	W, R, R						V	V	SSE4_1	pextrd		Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32.
66 REX.W+ 0F 3A 16 /r ib 	PEXTRQ r/m64, xmm2, imm8 	MRI	W, R, R						V	NE	SSE4_1	pextrq		Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64.
VEX.128.66.0F3A.W0 14 /r ib 	VPEXTRB reg/m8, xmm2, imm8 	MRI	W, R, R						V	V	AVX	vpextrb		Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with
VEX.128.66.0F3A.W0 16 /r ib 	VPEXTRD r32/m32, xmm2, imm8 	MRI	W, R, R						V	V	AVX	vpextrd		Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
VEX.128.66.0F3A.W1 16 /r ib 	VPEXTRQ r64/m64, xmm2, imm8 	MRI	W, R, R						V	I	AVX	vpextrq		Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
														
0F C5 /r ib 	PEXTRW reg, mm, imm8	RMI	W, R, R						V	V	SSE	pextrw		Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
66 0F C5 /r ib	PEXTRW reg, xmm, imm8	RMI	W, R, R						V	V	SSE2	pextrw	YES	Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
66 0F 3A 15 /r ib 	PEXTRW reg/m16, xmm, imm8 	MRI	W, R, R						V	V	SSE4_1	pextrw		Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero-extend the result in the destination, r32 or r64.
VEX.128.66.0F.W0 C5 /r ib 	VPEXTRW reg, xmm1, imm8 	RMI	W, R, R						V	V	AVX	vpextrw	YES	Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros.
VEX.128.66.0F3A.W0 15 /r ib 	VPEXTRW reg/m16, xmm2, imm8 	MRI	W, R, R						V	V	AVX	vpextrw		Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros.
														
0F 38 01 /r 	PHADDW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phaddw		Add 16-bit integers horizontally, pack to MM1.
66 0F 38 01 /r	PHADDW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phaddw		Add 16-bit integers horizontally, pack to XMM1.
0F 38 02 /r	PHADDD mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phaddd		Add 32-bit integers horizontally, pack to MM1.
66 0F 38 02 /r	PHADDD xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phaddd		Add 32-bit integers horizontally, pack to XMM1.
VEX.NDS.128.66.0F38.WIG 01 /r	VPHADDW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphaddw		Add 16-bit integers horizontally, pack to xmm1.
VEX.NDS.128.66.0F38.WIG 02 /r	VPHADDD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphaddd		Add 32-bit integers horizontally, pack to xmm1.
VEX.NDS.256.66.0F38.WIG 01 /r	VPHADDW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphaddw		Add 16-bit signed integers horizontally, pack to ymm1.
VEX.NDS.256.66.0F38.WIG 02 /r	VPHADDD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphaddd		Add 32-bit signed integers horizontally, pack to ymm1.
														
0F 38 03 /r 	PHADDSW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phaddsw		Add 16-bit signed integers horizontally, pack saturated integers to MM1.
66 0F 38 03 /r	PHADDSW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phaddsw		Add 16-bit signed integers horizontally, pack saturated integers to XMM1.
VEX.NDS.128.66.0F38.WIG 03 /r	VPHADDSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphaddsw		Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
VEX.NDS.256.66.0F38.WIG 03 /r	VPHADDSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphaddsw		Add 16-bit signed integers horizontally, pack saturated integers to ymm1.
														
66 0F 38 41 /r 	PHMINPOSUW xmm1, xmm2/m128 	RM	W, R						V	V	SSE4_1	phminposuw		Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
VEX.128.66.0F38.WIG 41 /r 	VPHMINPOSUW xmm1, xmm2/m128 	RM	Z, R						V	V	AVX	vphminposuw		Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
														
0F 38 05 /r 	PHSUBW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phsubw		Subtract 16-bit signed integers horizontally, pack to MM1.
66 0F 38 05 /r	PHSUBW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phsubw		Subtract 16-bit signed integers horizontally, pack to XMM1.
0F 38 06 /r	PHSUBD mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phsubd		Subtract 32-bit signed integers horizontally, pack to MM1.
66 0F 38 06 /r	PHSUBD xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phsubd		Subtract 32-bit signed integers horizontally, pack to XMM1
VEX.NDS.128.66.0F38.WIG 05 /r	VPHSUBW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphsubw		Subtract 16-bit signed integers horizontally, pack to xmm1.
VEX.NDS.128.66.0F38.WIG 06 /r	VPHSUBD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphsubd		Subtract 32-bit signed integers horizontally, pack to xmm1.
VEX.NDS.256.66.0F38.WIG 05 /r	VPHSUBW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphsubw		Subtract 16-bit signed integers horizontally, pack to ymm1.
VEX.NDS.256.66.0F38.WIG 06 /r	VPHSUBD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphsubd		Subtract 32-bit signed integers horizontally, pack to ymm1.
														
0F 38 07 /r 	PHSUBSW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	phsubsw		Subtract 16-bit signed integer horizontally, pack saturated integers to MM1.
66 0F 38 07 /r	PHSUBSW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	phsubsw		Subtract 16-bit signed integer horizontally, pack saturated integers to XMM1
VEX.NDS.128.66.0F38.WIG 07 /r	VPHSUBSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vphsubsw		Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
VEX.NDS.256.66.0F38.WIG 07 /r	VPHSUBSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vphsubsw		Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1.
														
66 0F 3A 20 /r ib 	PINSRB xmm1, r32/m8, imm8 	RMI	RW, R, R						V	V	SSE4_1	pinsrb		Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8.
66 0F 3A 22 /r ib 	PINSRD xmm1, r/m32, imm8 	RMI	RW, R, R						V	V	SSE4_1	pinsrd		Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8.
66 REX.W+ 0F 3A 22 /r ib 	PINSRQ xmm1, r/m64, imm8 								NE	V	SSE4_1	pinsrq		Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8.
VEX.NDS.128.66.0F3A.W0 20 /r ib 	VPINSRB xmm1, xmm2, r32/m8, imm8 	RVMI	Z, R, R, R						V	V	AVX	vpinsrb		Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
VEX.NDS.128.66.0F3A.W0 22 /r ib 	VPINSRD xmm1, xmm2, r32/m32, imm8 	RVMI	Z, R, R, R						V	V	AVX	vpinsrd		Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
VEX.NDS.128.66.0F3A.W1 22 /r ib 	VPINSRQ xmm1, xmm2, r64/m64, imm8 	RVMI	Z, R, R, R						V	I	AVX	vpinsrq		Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
														
0F C4 /r ib 	PINSRW mm, r32/m16, imm8	RMI	RW, R, R						V	V	SSE	pinsrw		Insert the low word from r32 or from m16 into mm at the word position specified by imm8
66 0F C4 /r ib	PINSRW xmm, r32/m16, imm8	RMI	RW, R, R						V	V	SSE2	pinsrw		Move the low word of r32 or from m16 into xmm at the word position specified by imm8.
VEX.NDS.128.66.0F.W0 C4 /r ib	VPINSRW xmm1, xmm2, r32/m16, imm8	RVMI	Z, R, R, R						V	V	AVX	vpinsrw		Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8.
														
0F 38 04 /r 	PMADDUBSW mm1, mm2/m64	RM	RW, R						V	V	MMX SSSE3	pmaddubsw		Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to MM1.
66 0F 38 04 /r	PMADDUBSW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	pmaddubsw		Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to XMM1.
VEX.NDS.128.66.0F38.WIG 04 /r	VPMADDUBSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmaddubsw		Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
VEX.NDS.256.66.0F38.WIG 04 /r	VPMADDUBSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaddubsw		Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1.
														
0F F5 /r 	PMADDWD mm, mm/m64	RM	RW, R						V	V	MMX	pmaddwd		Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm.
66 0F F5 /r	PMADDWD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmaddwd		Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1.
VEX.NDS.128.66.0F.WIG F5 /r	VPMADDWD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmaddwd		Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1.
VEX.NDS.256.66.0F.WIG F5 /r	VPMADDWD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaddwd		Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1.
														
66 0F 38 3C /r 	PMAXSB xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmaxsb		Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3C /r 	VPMAXSB xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmaxsb		Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VEX.NDS.256.66.0F38.WIG 3C /r	VPMAXSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxsb		Compare packed signed byte integers in ymm2 and ymm3/m128 and store packed maximum values in ymm1.
														
66 0F 38 3D /r 	PMAXSD xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmaxsd		Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3D /r 	VPMAXSD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmaxsd		Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VEX.NDS.256.66.0F38.WIG 3D /r	VPMAXSD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxsd		Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed maximum values in ymm1.
														
0F EE /r 	PMAXSW mm1, mm2/m64	RM	RW, R						V	V	SSE	pmaxsw		Compare signed word integers in mm2/m64 and mm1 and return maximum values.
66 0F EE /r	PMAXSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmaxsw		Compare signed word integers in xmm2/m128 and xmm1 and return maximum values.
VEX.NDS.128.66.0F.WIG EE /r	VPMAXSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmaxsw		Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1.
VEX.NDS.256.66.0F.WIG EE /r	VPMAXSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxsw		Compare packed signed word integers in ymm3/m128 and ymm2 and store packed maximum values in ymm1.
														
0F DE /r 	PMAXUB mm1, mm2/m64	RM	RW, R						V	V	SSE	pmaxub		Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values.
66 0F DE /r	PMAXUB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmaxub		Compare unsigned byte integers in xmm2/m128 and xmm1 and returns maximum values.
VEX.NDS.128.66.0F.WIG DE /r	VPMAXUB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmaxub		Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VEX.NDS.256.66.0F.WIG DE /r	VPMAXUB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxub		Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
														
66 0F 38 3F /r 	PMAXUD xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmaxud		Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3F /r 	VPMAXUD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmaxud		Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VEX.NDS.256.66.0F38.WIG 3F /r	VPMAXUD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxud		Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
														
66 0F 38 3E /r 	PMAXUW xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmaxuw		Compare packed unsigned word integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3E /r 	VPMAXUW xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmaxuw		Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1.
VEX.NDS.256.66.0F38.WIG 3E /r	VPMAXUW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmaxuw		Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1.
														
66 0F 38 38 /r 	PMINSB xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pminsb		Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 38 /r 	VPMINSB xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpminsb		Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VEX.NDS.256.66.0F38.WIG 38 /r	VPMINSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpminsb		Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
														
66 0F 38 39 /r 	PMINSD xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pminsd		Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 39 /r 	VPMINSD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpminsd		Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VEX.NDS.256.66.0F38.WIG 39 /r	VPMINSD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpminsd		Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed minimum values in ymm1.
														
0F EA /r 	PMINSW mm1, mm2/m64	RM	RW, R						V	V	SSE	pminsw		Compare signed word integers in mm2/m64 and mm1 and return minimum values.
66 0F EA /r	PMINSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pminsw		Compare signed word integers in xmm2/m128 and xmm1 and return minimum values.
VEX.NDS.128.66.0F.WIG EA /r	VPMINSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpminsw		Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
														
0F DA /r 	PMINUB mm1, mm2/m64	RM	RW, R						V	V	SSE	pminub		Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values.
66 0F DA /r	PMINUB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pminub		Compare unsigned byte integers in xmm2/m128 and xmm1 and returns minimum values.
VEX.NDS.128.66.0F.WIG DA /r	VPMINUB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpminub		Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VEX.NDS.256.66.0F.WIG DA /r	VPMINUB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpminub		Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
														
66 0F 38 3B /r 	PMINUD xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pminud		Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3B /r 	VPMINUD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpminud		Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VEX.NDS.256.66.0F38.WIG 3B /r	VPMINUD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpminud		Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
														
66 0F 38 3A /r 	PMINUW xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pminuw		Compare packed unsigned word integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
VEX.NDS.128.66.0F38.WIG 3A /r 	VPMINUW xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpminuw		Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
VEX.NDS.256.66.0F38.WIG 3A /r	VPMINUW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpminuw		Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1.
														
0F D7 /r 	PMOVMSKB reg, mm	RM	W, R						V	V	SSE	pmovmskb		Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed
66 0F D7 /r	PMOVMSKB reg, xmm	RM	W, R						V	V	SSE2	pmovmskb		Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed
VEX.128.66.0F.WIG D7 /r	VPMOVMSKB reg, xmm1	RM	W, R						V	V	AVX	vpmovmskb		Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros.
VEX.256.66.0F.WIG D7 /r	VPMOVMSKB reg, ymm1	RM	W, R						V	V	AVX2	vpmovmskb		Move a 32-bit mask of ymm1 to reg. The upper bits of r64 are filled with zeros.
														
66 0f 38 20 /r 	PMOVSXBW xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovsxbw		Sign extend 8 packed signed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed signed 16-bit integers in xmm1.
66 0f 38 21 /r 	PMOVSXBD xmm1, xmm2/m32 	RM	W, R						V	V	SSE4_1	pmovsxbd		Sign extend 4 packed signed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed signed 32-bit integers in xmm1.
66 0f 38 22 /r 	PMOVSXBQ xmm1, xmm2/m16	RM	W, R						V	V	SSE4_1	pmovsxbq		Sign extend 2 packed signed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed signed 64-bit integers in xmm1.
66 0f 38 23 /r 	PMOVSXWD xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovsxwd		Sign extend 4 packed signed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed signed 32-bit integers in xmm1.
66 0f 38 24 /r 	PMOVSXWQ xmm1, xmm2/m32 	RM	W, R						V	V	SSE4_1	pmovsxwq		Sign extend 2 packed signed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed signed 64-bit integers in xmm1.
66 0f 38 25 /r 	PMOVSXDQ xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovsxdq		Sign extend 2 packed signed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed signed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 20 /r 	VPMOVSXBW xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovsxbw		Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VEX.128.66.0F38.WIG 21 /r 	VPMOVSXBD xmm1, xmm2/m32 	RM	Z, R						V	V	AVX	vpmovsxbd		Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 22 /r 	VPMOVSXBQ xmm1, xmm2/m16 	RM	Z, R						V	V	AVX	vpmovsxbq		Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 23 /r 	VPMOVSXWD xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovsxwd		Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 24 /r 	VPMOVSXWQ xmm1, xmm2/m32 	RM	Z, R						V	V	AVX	vpmovsxwq		Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 25 /r 	VPMOVSXDQ xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovsxdq		Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1
VEX.256.66.0F38.WIG 20 /r	VPMOVSXBW ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovsxbw		Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VEX.256.66.0F38.WIG 21 /r	VPMOVSXBD ymm1, xmm2/m64	RM	W, R						V	V	AVX2	vpmovsxbd		Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VEX.256.66.0F38.WIG 22 /r	VPMOVSXBQ ymm1, xmm2/m32	RM	W, R						V	V	AVX2	vpmovsxbq		Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VEX.256.66.0F38.WIG 23 /r	VPMOVSXWD ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovsxwd		Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1.
VEX.256.66.0F38.WIG 24 /r	VPMOVSXWQ ymm1, xmm2/m64	RM	W, R						V	V	AVX2	vpmovsxwq		Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1.
VEX.256.66.0F38.WIG 25 /r	VPMOVSXDQ ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovsxdq		Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1.
														
66 0f 38 30 /r 	PMOVZXBW xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovzxbw		Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
66 0f 38 31 /r 	PMOVZXBD xmm1, xmm2/m32 	RM	W, R						V	V	SSE4_1	pmovzxbd		Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
66 0f 38 32 /r 	PMOVZXBQ xmm1, xmm2/m16 	RM	W, R						V	V	SSE4_1	pmovzxbq		Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
66 0f 38 33 /r 	PMOVZXWD xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovzxwd		Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
66 0f 38 34 /r 	PMOVZXWQ xmm1, xmm2/m32 	RM	W, R						V	V	SSE4_1	pmovzxwq		Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
66 0f 38 35 /r 	PMOVZXDQ xmm1, xmm2/m64 	RM	W, R						V	V	SSE4_1	pmovzxdq		Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 30 /r 	VPMOVZXBW xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovzxbw		Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VEX.128.66.0F38.WIG 31 /r 	VPMOVZXBD xmm1, xmm2/m32 	RM	Z, R						V	V	AVX	vpmovzxbd		Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 32 /r 	VPMOVZXBQ xmm1, xmm2/m16 	RM	Z, R						V	V	AVX	vpmovzxbq		Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 33 /r 	VPMOVZXWD xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovzxwd		Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VEX.128.66.0F38.WIG 34 /r 	VPMOVZXWQ xmm1, xmm2/m32 	RM	Z, R						V	V	AVX	vpmovzxwq		Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VEX.128.66.0F38.WIG 35 /r 	VPMOVZXDQ xmm1, xmm2/m64 	RM	Z, R						V	V	AVX	vpmovzxdq		Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VEX.256.66.0F38.WIG 30 /r	VPMOVZXBW ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovzxbw		Zero extend 16 packed 8-bit integers in the low 16 bytes of xmm2/m128 to 16 packed 16-bit integers in ymm1.
VEX.256.66.0F38.WIG 31 /r	VPMOVZXBD ymm1, xmm2/m64	RM	W, R						V	V	AVX2	vpmovzxbd		Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VEX.256.66.0F38.WIG 32 /r	VPMOVZXBQ ymm1, xmm2/m32	RM	W, R						V	V	AVX2	vpmovzxbq		Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VEX.256.66.0F38.WIG 33 /r	VPMOVZXWD ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovzxwd		Zero extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1
VEX.256.66.0F38.WIG 34 /r	VPMOVZXWQ ymm1, xmm2/m64	RM	W, R						V	V	AVX2	vpmovzxwq		Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1.
VEX.256.66.0F38.WIG 35 /r	VPMOVZXDQ ymm1, xmm2/m128	RM	W, R						V	V	AVX2	vpmovzxdq		Zero extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1.
														
66 0F 38 28 /r 	PMULDQ xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmuldq		Multiply the packed signed dword integers in xmm1 and xmm2/m128 and store the quadword product in xmm1.
VEX.NDS.128.66.0F38.WIG 28 /r 	VPMULDQ xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmuldq		Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VEX.NDS.256.66.0F38.WIG 28 /r	VPMULDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmuldq		Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1.
														
0F 38 0B /r 	PMULHRSW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	pmulhrsw		Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to MM1.
66 0F 38 0B /r	PMULHRSW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	pmulhrsw		Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to XMM1.
VEX.NDS.128.66.0F38.WIG 0B /r	VPMULHRSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmulhrsw		Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
VEX.NDS.256.66.0F38.WIG 0B /r	VPMULHRSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmulhrsw		Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1.
														
0F E4 /r 	PMULHUW mm1, mm2/m64	RM	RW, R						V	V	SSE	pmulhuw		Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
66 0F E4 /r	PMULHUW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmulhuw		Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG E4 /r	VPMULHUW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmulhuw		Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.256.66.0F.WIG E4 /r	VPMULHUW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmulhuw		Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
														
0F E5 /r 	PMULHW mm, mm/m64	RM	RW, R						V	V	MMX	pmulhw		Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
66 0F E5 /r	PMULHW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmulhw		Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG E5 /r	VPMULHW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmulhw		Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VEX.NDS.256.66.0F.WIG E5 /r	VPMULHW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmulhw		Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
														
66 0F 38 40 /r 	PMULLD xmm1, xmm2/m128 	RM	RW, R						V	V	SSE4_1	pmulld		Multiply the packed dword signed integers in xmm1 and xmm2/m128 and store the low 32 bits of each product in xmm1.
VEX.NDS.128.66.0F38.WIG 40 /r 	VPMULLD xmm1, xmm2, xmm3/m128 	RVM	Z, R, R						V	V	AVX	vpmulld		Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VEX.NDS.256.66.0F38.WIG 40 /r	VPMULLD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmulld		Multiply the packed dword signed integers in ymm2 and ymm3/m256 and store the low 32 bits of each product in ymm1.
														
0F D5 /r 	PMULLW mm, mm/m64	RM	RW, R						V	V	MMX	pmullw		Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1.
66 0F D5 /r	PMULLW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmullw		Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1.
VEX.NDS.128.66.0F.WIG D5 /r	VPMULLW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmullw		Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VEX.NDS.256.66.0F.WIG D5 /r	VPMULLW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmullw		Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1.
														
0F F4 /r 	PMULUDQ mm1, mm2/m64	RM	RW, R						V	V	SSE2	pmuludq		Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1.
66 0F F4 /r	PMULUDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pmuludq		Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1.
VEX.NDS.128.66.0F.WIG F4 /r	VPMULUDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpmuludq		Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VEX.NDS.256.66.0F.WIG F4 /r	VPMULUDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpmuludq		Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1.
														
8F /0 	POP r/m16 	M	W						V	V		popw		Pop top of stack into m16; increment stack pointer.
8F /0 	POP r/m32 								NE	V				Pop top of stack into m32; increment stack pointer.
8F /0 	POP r/m64 	M	W						V	NE		popq		Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size.
58 +rw 	POP r16 	O	W						V	V		popw	YES	Pop top of stack into r16; increment stack pointer.
58 +rd 	POP r32 								NE	V				Pop top of stack into r32; increment stack pointer.
58 +rd 	POP r64 	O	W						V	NE		popq	YES	Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size.
1F 	POP DS 								I	V				Pop top of stack into DS; increment stack pointer.
07	POP ES 								I	V				Pop top of stack into ES; increment stack pointer.
17	POP SS 								I	V				Pop top of stack into SS; increment stack pointer.
PREF.66+ 0F A1 	POP FS, p66	NP	W, I						V	V		popq		Pop top of stack into FS; increment stack pointer by 16 bits.
0F A1 	POP FS 								NE	V				Pop top of stack into FS; increment stack pointer by 32 bits.
0F A1 	POP FS 	NP	W						V	NE		popq		Pop top of stack into FS; increment stack pointer by 64 bits.
PREF.66+ 0F A9 	POP GS, p66	NP	W, I						V	V		popq		Pop top of stack into GS; increment stack pointer by 16 bits.
0F A9 	POP GS 								NE	V				Pop top of stack into GS; increment stack pointer by 32 bits.
0F A9 	POP GS 	NP	W						V	NE		popq		Pop top of stack into GS; increment stack pointer by 64 bits.
														
61	POPA 								I	V				Pop DI, SI, BP, BX, DX, CX, and AX.
61	POPAD 								I	V				Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX.
														
F3 0F B8 /r 	POPCNT r16, r/m16 	RM	W, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	POPCNT	popcnt		POPCNT on r/m16
F3 0F B8 /r 	POPCNT r32, r/m32 	RM	W, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	POPCNT	popcnt		POPCNT on r/m32
F3 REX.W+ 0F B8 /r 	POPCNT r64, r/m64 	RM	W, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE	POPCNT	popcnt		POPCNT on r/m64
														
9D 	POPF 	NP			E.*				V	V		popf		Pop top of stack into lower 16 bits of EFLAGS.
9D 	POPFD 								NE	V				Pop top of stack into EFLAGS.
9D 	POPFQ 	NP			E.*				V	NE		popfq		Pop top of stack and zero-extend into RFLAGS.
														
0F EB /r 	POR mm, mm/m64	RM	RW, R						V	V	MMX	por		Bitwise OR of mm/m64 and mm.
66 0F EB /r	POR xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	por		Bitwise OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG EB /r	VPOR xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpor		Bitwise OR of xmm2/m128 and xmm3.
VEX.NDS.256.66.0F.WIG EB /r	VPOR ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpor		Bitwise OR of ymm2/m256 and ymm3
														
0F 18 /1 	PREFETCHT0 m8 	M	I						V	V		prefetcht0		Move data from m8 closer to the processor using T0 hint.
0F 18 /2 	PREFETCHT1 m8 	M	I						V	V		prefetcht1		Move data from m8 closer to the processor using T1 hint.
0F 18 /3 	PREFETCHT2 m8 	M	I						V	V		prefetcht2		Move data from m8 closer to the processor using T2 hint.
0F 18 /0 	PREFETCHNTA m8 	M	I						V	V		prefetchnta		Move data from m8 closer to the processor using NTA hint.
														
0F F6 /r 	PSADBW mm1, mm2/m64	RM	RW, R						V	V	SSE	psadbw		Computes the absolute differences of the packed unsigned byte integers from mm2 /m64 and mm1; differences are then summed to produce an unsigned word integer result.
66 0F F6 /r	PSADBW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psadbw		Computes the absolute differences of the packed unsigned byte integers from xmm2 /m128 and xmm1; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
VEX.NDS.128.66.0F.WIG F6 /r	VPSADBW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsadbw		Computes the absolute differences of the packed unsigned byte integers from xmm3 /m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
VEX.NDS.256.66.0F.WIG F6 /r	VPSADBW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsadbw		Computes the absolute differences of the packed unsigned byte integers from ymm3/m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
														
0F 38 00 /r 	PSHUFB mm1, mm2/m64	RM	RW, R						V	V	SSSE3	pshufb		Shuffle bytes in mm1 according to contents of mm2/m64.
66 0F 38 00 /r	PSHUFB xmm1, xmm2/m128	RM	RZ, R						V	V	SSSE3	pshufb		Shuffle bytes in xmm1 according to contents of xmm2/m128.
VEX.NDS.128.66.0F38.WIG 00 /r	VPSHUFB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpshufb		Shuffle bytes in xmm2 according to contents of xmm3/m128.
VEX.NDS.256.66.0F38.WIG 00 /r	VPSHUFB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpshufb		Shuffle bytes in ymm2 according to contents of ymm3/m256.
														
66 0F 70 /r ib 	PSHUFD xmm1, xmm2/m128, imm8	RMI	W, R, R						V	V	SSE2	pshufd		Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.66.0F.WIG 70 /r ib	VPSHUFD xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AVX	vpshufd		Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.256.66.0F.WIG 70 /r ib	VPSHUFD ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX2	vpshufd		Shuffle the doublewords in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
														
F3 0F 70 /r ib 	PSHUFHW xmm1, xmm2/m128, imm8	RMI	W, R, R						V	V	SSE2	pshufhw		Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.F3.0F.WIG 70 /r ib	VPSHUFHW xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AVX	vpshufhw		Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.256.F3.0F.WIG 70 /r ib	VPSHUFHW ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX2	vpshufhw		Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
														
F2 0F 70 /r ib 	PSHUFLW xmm1, xmm2/m128, imm8	RMI	W, R, R						V	V	SSE2	pshuflw		Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.128.F2.0F.WIG 70 /r ib	VPSHUFLW xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AVX	vpshuflw		Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VEX.256.F2.0F.WIG 70 /r ib	VPSHUFLW ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX2	vpshuflw		Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
														
0F 70 /r ib 	PSHUFW mm1, mm2/m64, imm8	RMI	W, R, R						V	V		pshufw		Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1.
														
0F 38 08 /r 	PSIGNB mm1, mm2/m64	RM	RW, R						V	V	SSSE3	psignb		Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64
66 0F 38 08 /r	PSIGNB xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	psignb		Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128.
0F 38 09 /r	PSIGNW mm1, mm2/m64	RM	RW, R						V	V	SSSE3	psignw		Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128.
66 0F 38 09 /r	PSIGNW xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	psignw		Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128.
0F 38 0A /r	PSIGND mm1, mm2/m64	RM	RW, R						V	V	SSSE3	psignd		Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128.
66 0F 38 0A /r	PSIGND xmm1, xmm2/m128	RM	RW, R						V	V	SSSE3	psignd		Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128.
VEX.NDS.128.66.0F38.WIG 08 /r	VPSIGNB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsignb		Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128.
VEX.NDS.128.66.0F38.WIG 09 /r	VPSIGNW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsignw		Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128.
VEX.NDS.128.66.0F38.WIG 0A /r	VPSIGND xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsignd		Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128.
														
66 0F 73 /7 ib 	PSLLDQ xmm1, imm8	MI	RW, R						V	V	SSE2	pslldq		Shift xmm1 left by imm8 bytes while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /7 ib	VPSLLDQ xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpslldq		Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1.
VEX.NDD.256.66.0F.WIG 73 /7 ib	VPSLLDQ ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpslldq		Shift ymm2 left by imm8 bytes while shifting in 0s and store result in ymm1.
														
0F F1 /r	PSLLW mm, mm/m64	RM	RW, R						V	V	MMX	psllw		Shift words in mm left mm/m64 while shifting in 0s.
66 0F F1 /r	PSLLW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psllw		Shift words in xmm1 left by xmm2/m128 while shifting in 0s.
0F 71 /6 ib	PSLLW xmm1, imm8	MI	RW, R						V	V	MMX	psllw		Shift words in mm left by imm8 while shifting in 0s.
66 0F 71 /6 ib	PSLLW xmm1, imm8	MI	RW, R						V	V	SSE2	psllw	YES	Shift words in xmm1 left by imm8 while shifting in 0s.
0F F2 /r	PSLLD mm, mm/m64	RM	RW, R						V	V	MMX	pslld		Shift doublewords in mm left by mm/m64 while shifting in 0s.
66 0F F2 /r	PSLLD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pslld		Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s.
0F 72 /6 ib	PSLLD mm, imm8	MI	RW, R						V	V	MMX	pslld		Shift doublewords in mm left by imm8 while shifting in 0s.
66 0F 72 /6 ib	PSLLD xmm1, imm8	MI	RW, R						V	V	SSE2	pslld		Shift doublewords in xmm1 left by imm8 while shifting in 0s.
0F F3 /r	PSLLQ mm, mm/m64	RM	RW, R						V	V	MMX	psllq		Shift quadword in mm left by mm/m64 while shifting in 0s.
66 0F F3 /r	PSLLQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psllq		Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s.
0F 73 /6 ib	PSLLQ mm, imm8	MI	RW, R						V	V	MMX	psllq		Shift quadword in mm left by imm8 while shifting in 0s.
66 0F 73 /6 ib	PSLLQ xmm1, imm8	MI	RW, R						V	V	SSE2	psllq		Shift quadwords in xmm1 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F1 /r	VPSLLW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsllw		Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 71 /6 ib	VPSLLW xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsllw		Shift words in xmm2 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F2 /r	VPSLLD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpslld		Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 72 /6 ib	VPSLLD xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpslld		Shift doublewords in xmm2 left by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG F3 /r	VPSLLQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsllq		Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /6 ib	VPSLLQ xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsllq		Shift quadwords in xmm2 left by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG F1 /r	VPSLLW ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsllw		Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.256.66.0F.WIG 71 /6 ib	VPSLLW ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsllw		Shift words in ymm2 left by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG F2 /r	VPSLLD ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpslld		Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s
VEX.NDD.256.66.0F.WIG 72 /6 ib	VPSLLD ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpslld		Shift doublewords in ymm2 left by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG F3 /r	VPSLLQ ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsllq		Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.256.66.0F.WIG 73 /6 ib	VPSLLQ ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsllq		Shift quadwords in ymm2 left by imm8 while shifting in 0s.
														
0F E1 /r	PSRAW mm, mm/m64	RM	RW, R						V	V	MMX	psraw		Shift words in mm right by mm/m64 while shifting in sign bits.
66 0F E1 /r	PSRAW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psraw		Shift words in xmm1 right by xmm2/m128 while shifting in sign bits.
0F 71 /4 ib	PSRAW mm, imm8	MI	RW, R						V	V	MMX	psraw		Shift words in mm right by imm8 while shifting in sign bits
66 0F 71 /4 ib	PSRAW xmm1, imm8	MI	RW, R						V	V	SSE2	psraw		Shift words in xmm1 right by imm8 while shifting in sign bits
0F E2 /r	PSRAD mm, mm/m64	RM	RW, R						V	V	MMX	psrad		Shift doublewords in mm right by mm/m64 while shifting in sign bits.
66 0F E2 /r	PSRAD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psrad		Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits.
0F 72 /4 ib	PSRAD mm, imm8	MI	RW, R						V	V	MMX	psrad		Shift doublewords in mm right by imm8 while shifting in sign bits.
66 0F 72 /4 ib	PSRAD xmm1, imm8	MI	RW, R						V	V	SSE2	psrad		Shift doublewords in xmm1 right by imm8 while shifting in sign bits.
VEX.NDS.128.66.0F.WIG E1 /r	VPSRAW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsraw		Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.128.66.0F.WIG 71 /4 ib	VPSRAW xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsraw		Shift words in xmm2 right by imm8 while shifting in sign bits.
VEX.NDS.128.66.0F.WIG E2 /r	VPSRAD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsrad		Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.128.66.0F.WIG 72 /4 ib	VPSRAD xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsrad		Shift doublewords in xmm2 right by imm8 while shifting in sign bits.
VEX.NDS.256.66.0F.WIG E1 /r	VPSRAW ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsraw		Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.256.66.0F.WIG 71 /4 ib	VPSRAW ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsraw		Shift words in ymm2 right by imm8 while shifting in sign bits.
VEX.NDS.256.66.0F.WIG E2 /r	VPSRAD ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsrad		Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VEX.NDD.256.66.0F.WIG 72 /4 ib	VPSRAD ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsrad		Shift doublewords in ymm2 right by imm8 while shifting in sign bits.
														
66 0F 73 /3 ib	PSRLDQ xmm1, imm8	MI	RW, R						V	V	SSE2	psrldq		Shift xmm1 right by imm8 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /3 ib	VPSRLDQ xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsrldq		Shift xmm2 right by imm8 bytes while shifting in 0s.
VEX.NDD.256.66.0F.WIG 73 /3 ib	VPSRLDQ ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsrldq		Shift ymm1 right by imm8 bytes while shifting in 0s.
														
0F D1 /r	PSRLW mm, mm/m64	RM	RW, R						V	V	MMX	psrlw		Shift words in mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D1 /r	PSRLW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psrlw		Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
0F 71 /2 ib	PSRLW mm, imm8	MI	RW, R						V	V	MMX	psrlw		Shift words in mm right by imm8 while shifting in 0s.
66 0F 71 /2 ib	PSRLW xmm1, imm8	MI	RW, R						V	V	SSE2	psrlw		Shift words in xmm1 right by imm8 while shifting in 0s.
0F D2 /r	PSRLD mm, mm/m64	RM	RW, R						V	V	MMX	psrld		Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D2 /r	PSRLD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psrld		Shift doublewords in xmm1 right by amount specified in xmm2 /m128 while shifting in 0s.
0F 72 /2 ib	PSRLD mm, imm8	MI	RW, R						V	V	MMX	psrld		Shift doublewords in mm right by imm8 while shifting in 0s.
66 0F 72 /2 ib	PSRLD xmm1, imm8	MI	RW, R						V	V	SSE2	psrld		Shift doublewords in xmm1 right by imm8 while shifting in 0s.
0F D3 /r	PSRLQ mm, mm/m64	RM	RW, R						V	V	MMX	psrlq		Shift mm right by amount specified in mm/m64 while shifting in 0s.
66 0F D3 /r	PSRLQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psrlq		Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
0F 73 /2 ib	PSRLQ mm, imm8	MI	RW, R						V	V	MMX	psrlq		Shift mm right by imm8 while shifting in 0s. 
66 0F 73 /2 ib	PSRLQ xmm1, imm8	MI	RW, R						V	V	SSE2	psrlq		Shift quadwords in xmm1 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D1 /r	VPSRLW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsrlw		Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 71 /2 ib	VPSRLW xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsrlw		Shift words in xmm2 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D2 /r	VPSRLD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsrld		Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 72 /2 ib	VPSRLD xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsrld		Shift doublewords in xmm2 right by imm8 while shifting in 0s.
VEX.NDS.128.66.0F.WIG D3 /r	VPSRLQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsrlq		Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.128.66.0F.WIG 73 /2 ib	VPSRLQ xmm1, xmm2, imm8	VMI	Z, R, R						V	V	AVX	vpsrlq		Shift quadwords in xmm2 right by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG D1 /r	VPSRLW ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsrlw		Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.256.66.0F.WIG 71 /2 ib	VPSRLW ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsrlw		Shift words in ymm2 right by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG D2 /r	VPSRLD ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsrld		Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.256.66.0F.WIG 72 /2 ib	VPSRLD ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsrld		Shift doublewords in ymm2 right by imm8 while shifting in 0s.
VEX.NDS.256.66.0F.WIG D3 /r	VPSRLQ ymm1, ymm2, xmm3/m128	RVM	W, R, R						V	V	AVX2	vpsrlq		Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VEX.NDD.256.66.0F.WIG 73 /2 ib	VPSRLQ ymm1, ymm2, imm8	VMI	W, R, R						V	V	AVX2	vpsrlq		Shift quadwords in ymm2 right by imm8 while shifting in 0s.
														
0F F8 /r	PSUBB mm, mm/m64	RM	RW, R						V	V	MMX	psubb		Subtract packed byte integers in mm/m64 from packed byte integers in mm.
66 0F F8 /r	PSUBB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubb		Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1.
0F F9 /r	PSUBW mm, mm/m64	RM	RW, R						V	V	MMX	psubw		Subtract packed word integers in mm/m64 from packed word integers in mm.
66 0F F9 /r	PSUBW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubw		Subtract packed word integers in xmm2/m128 from packed word integers in xmm1.
0F FA /r	PSUBD mm, mm/m64	RM	RW, R						V	V	MMX	psubd		Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm.
66 0F FA /r	PSUBD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubd		Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1.
VEX.NDS.128.66.0F.WIG F8 /r	VPSUBB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubb		Subtract packed byte integers in xmm3/m128 from xmm2.
VEX.NDS.128.66.0F.WIG F9 /r	VPSUBW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubw		Subtract packed word integers in xmm3/m128 from xmm2.
VEX.NDS.128.66.0F.WIG FA /r	VPSUBD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubd		Subtract packed doubleword integers in xmm3/m128 from xmm2.
VEX.NDS.256.66.0F.WIG F8 /r	VPSUBB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubb		Subtract packed byte integers in ymm3/m256 from ymm2.
VEX.NDS.256.66.0F.WIG F9 /r	VPSUBW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubw		Subtract packed word integers in ymm3/m256 from ymm2.
VEX.NDS.256.66.0F.WIG FA /r	VPSUBD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubd		Subtract packed doubleword integers in ymm3/m256 from ymm2.
														
0F FB /r	PSUBQ mm1, mm2/m64	RM	RW, R						V	V	SSE2	psubq		Subtract quadword integer in mm1 from mm2 /m64.
66 0F FB /r	PSUBQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubq		Subtract packed quadword integers in xmm1 from xmm2 /m128.
VEX.NDS.128.66.0F.WIG FB /r	VPSUBQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubq		Subtract packed quadword integers in xmm3/m128 from xmm2.
VEX.NDS.256.66.0F.WIG FB /r	VPSUBQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubq		Subtract packed quadword integers in ymm3/m256 from ymm2.
														
0F E8 /r	PSUBSB mm, mm/m64	RM	RW, R						V	V	MMX	psubsb		Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results.
66 0F E8 /r	PSUBSB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubsb		Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results.
0F E9 /r	PSUBSW mm, mm/m64	RM	RW, R						V	V	MMX	psubsw		Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results.
66 0F E9 /r	PSUBSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubsw		Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results.
VEX.NDS.128.66.0F.WIG E8 /r	VPSUBSB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubsb		Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results.
VEX.NDS.128.66.0F.WIG E9 /r	VPSUBSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubsw		Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results.
VEX.NDS.256.66.0F.WIG E8 /r	VPSUBSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubsb		Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results.
VEX.NDS.256.66.0F.WIG E9 /r	VPSUBSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubsw		Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results.
														
0F D8 /r	PSUBUSB mm, mm/m64	RM	RW, R						V	V	MMX	psubusb		Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result.
66 0F D8 /r	PSUBUSB xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubusb		Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result.
0F D9 /r	PSUBUSW mm, mm/m64	RM	RW, R						V	V	MMX	psubusw		Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result.
66 0F D9 /r	PSUBUSW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	psubusw		Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result.
VEX.NDS.128.66.0F.WIG D8 /r	VPSUBUSB xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubusb		Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result.
VEX.NDS.128.66.0F.WIG D9 /r	VPSUBUSW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpsubusw		Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result.
VEX.NDS.256.66.0F.WIG D8 /r	VPSUBUSB ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubusb		Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result.
VEX.NDS.256.66.0F.WIG D9 /r	VPSUBUSW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsubusw		Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result.
														
66 0F 38 17 /r	PTEST xmm1, xmm2/m128	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	SSE4_1	ptest		Set ZF if xmm2/m128 AND xmm1 result is all 0s. Set CF if xmm2/m128 AND NOT xmm1 result is all 0s.
VEX.128.66.0F38.WIG 17 /r	VPTEST xmm1, xmm2/m128	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vptest		Set ZF and CF depending on bitwise AND and ANDN of sources.
VEX.256.66.0F38.WIG 17 /r	VPTEST ymm1, ymm2/m256	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vptest		Set ZF and CF depending on bitwise AND and ANDN of sources.
														
0F 68 /r	PUNPCKHBW mm, mm/m64	RM	RW, R						V	V	MMX	punpckhbw		Unpack and interleave high-order bytes from mm and mm/m64 into mm.
66 0F 68 /r	PUNPCKHBW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpckhbw		Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1.
0F 69 /r	PUNPCKHWD mm, mm/m64	RM	RW, R						V	V	MMX	punpckhwd		Unpack and interleave high-order words from mm and mm/m64 into mm.
66 0F 69 /r	PUNPCKHWD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpckhwd		Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1.
0F 6A /r	PUNPCKHDQ mm, mm/m64	RM	RW, R						V	V	MMX	punpckhdq		Unpack and interleave high-order doublewords from mm and mm/m64 into mm.
66 0F 6A /r	PUNPCKHDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpckhdq		Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1.
66 0F 6D /r	PUNPCKHQDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpckhqdq		Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 68 /r	VPUNPCKHBW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpckhbw		Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 69 /r	VPUNPCKHWD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpckhwd		Interleave high-order words from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6A /r	VPUNPCKHDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpckhdq		Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6D /r 	VPUNPCKHQDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpckhqdq		Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VEX.NDS.256.66.0F.WIG 68 /r	VPUNPCKHBW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpckhbw		Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 69 /r	VPUNPCKHWD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpckhwd		Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 6A /r	VPUNPCKHDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpckhdq		Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 6D /r	VPUNPCKHQDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpckhqdq		Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register.
														
0F 60 /r	PUNPCKLBW mm, mm/m32	RM	RW, R						V	V	MMX	punpcklbw		Interleave low-order bytes from mm and mm/m32 into mm.
66 0F 60 /r	PUNPCKLBW xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpcklbw		Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1.
0F 61 /r	PUNPCKLWD mm, mm/m32	RM	RW, R						V	V	MMX	punpcklwd		Interleave low-order words from mm and mm/m32 into mm.
66 0F 61 /r	PUNPCKLWD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpcklwd		Interleave low-order words from xmm1 and xmm2/m128 into xmm1.
0F 62 /r	PUNPCKLDQ mm, mm/m32	RM	RW, R						V	V	MMX	punpckldq		Interleave low-order doublewords from mm and mm/m32 into mm.
66 0F 62 /r	PUNPCKLDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpckldq		Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1.
66 0F 6C /r	PUNPCKLQDQ xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	punpcklqdq		Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register.
VEX.NDS.128.66.0F.WIG 60 /r	VPUNPCKLBW xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpcklbw		Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 61 /r	VPUNPCKLWD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpcklwd		Interleave low-order words from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 62 /r	VPUNPCKLDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpckldq		Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1.
VEX.NDS.128.66.0F.WIG 6C /r	VPUNPCKLQDQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpunpcklqdq		Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VEX.NDS.256.66.0F.WIG 60 /r	VPUNPCKLBW ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpcklbw		Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 61 /r	VPUNPCKLWD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpcklwd		Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 62 /r	VPUNPCKLDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpckldq		Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VEX.NDS.256.66.0F.WIG 6C /r	VPUNPCKLQDQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpunpcklqdq		Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register.
														
FF /6	PUSH r/m16 	M	R						V	V		pushw		Push r/m16.
FF /6	PUSH r/m32								NE	V				Push r/m32.
FF /6	PUSH r/m64	M	R						V	NE		pushq		Push r/m64.
50 +rw	PUSH r16	O	R						V	V		pushw	YES	Push r16.
50 +rd	PUSH r32								NE	V				Push r32.
50 +rd	PUSH r64	O	R						V	NE		pushq	YES	Push r64.
6A	PUSH imm8	I	R						V	V		pushq		Push imm8.
68	PUSH imm16	I	R						V	V		pushq		Push imm16.
68	PUSH imm32	I	R						V	V		pushq		Push imm32.
0E	PUSH CS								I	V				Push CS.
16	PUSH SS								I	V				Push SS.
1E	PUSH DS								I	V				Push DS.
06	PUSH ES								I	V				Push ES.
0F A0	PUSH FS	NP	R						V	V		pushq		Push FS.
0F A8	PUSH GS	NP	R						V	V		pushq		Push GS.
														
60	PUSHA								I	V				Push AX, CX, DX, BX, original SP, BP, SI, and DI.
60	PUSHAD								I	V				Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI.
														
9C	PUSHF	NP		E.*					V	V		pushf		Push lower 16 bits of EFLAGS.
9C	PUSHFD								NE	V				Push EFLAGS.
9C	PUSHFQ	NP		E.*					V	NE		pushfq		Push RFLAGS.
														
0F EF /r	PXOR mm, mm/m64	RM	RW, R						V	V	MMX	pxor		Bitwise XOR of mm/m64 and mm.
66 0F EF /r	PXOR xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	pxor		Bitwise XOR of xmm2/m128 and xmm1. 
VEX.NDS.128.66.0F.WIG EF /r	VPXOR xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpxor		Bitwise XOR of xmm3/m128 and xmm2.
VEX.NDS.256.66.0F.WIG EF /r	VPXOR ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpxor		Bitwise XOR of ymm3/m256 and ymm2.
														
D0 /2	RCL r/m8, 1	M1	RW, R		E.CF E.OF				V	V		rclb		Rotate 9 bits (CF, r/m8) left once.
REX+ D0 /2	RCL r/m8, 1	M1	RW, R		E.CF E.OF				V	NE		rclb		Rotate 9 bits (CF, r/m8) left once.
D2 /2	RCL r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	V		rclb		Rotate 9 bits (CF, r/m8) left CL times.
REX+ D2 /2	RCL r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rclb		Rotate 9 bits (CF, r/m8) left CL times.
C0 /2 ib	RCL r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rclb		Rotate 9 bits (CF, r/m8) left imm8 times.
REX+ C0 /2 ib	RCL r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rclb		Rotate 9 bits (CF, r/m8) left imm8 times.
D1 /2	RCL r/m16, 1	M1	RW, R		E.CF E.OF				V	V		rclw		Rotate 17 bits (CF, r/m16) left once.
D3 /2	RCL r/m16, CL	MC	RW, R		e.cf e.of	e.of			V	V		rclw		Rotate 17 bits (CF, r/m16) left CL times.
C1 /2 ib	RCL r/m16, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rclw		Rotate 17 bits (CF, r/m16) left imm8 times.
D1 /2	RCL r/m32, 1	M1	RW, R		E.CF E.OF				V	V		rcll		Rotate 33 bits (CF, r/m32) left once.
REX.W+ D1 /2	RCL r/m64, 1	M1	RW, R		E.CF E.OF				V	NE		rclq		Rotate 65 bits (CF, r/m64) left once. Uses a 6 bit count.
D3 /2	RCL r/m32, CL	MC	RW, R		e.cf e.of	e.of			V	V		rcll		Rotate 33 bits (CF, r/m32) left CL times.
REX.W+ D3 /2	RCL r/m64, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rclq		Rotate 65 bits (CF, r/m64) left CL times. Uses a 6 bit count.
C1 /2 ib	RCL r/m32, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rcll		Rotate 33 bits (CF, r/m32) left imm8 times.
REX.W+ C1 /2 ib	RCL r/m64, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rclq		Rotate 65 bits (CF, r/m64) left imm8 times. Uses a 6 bit count.
D0 /3	RCR r/m8, 1	M1	RW, R		E.CF E.OF				V	V		rcrb		Rotate 9 bits (CF, r/m8) right once.
REX+ D0 /3	RCR r/m8, 1	M1	RW, R		E.CF E.OF				V	NE		rcrb		Rotate 9 bits (CF, r/m8) right once.
D2 /3	RCR r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	V		rcrb		Rotate 9 bits (CF, r/m8) right CL times.
REX+ D2 /3	RCR r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rcrb		Rotate 9 bits (CF, r/m8) right CL times.
C0 /3 ib	RCR r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rcrb		Rotate 9 bits (CF, r/m8) right imm8 times.
REX+ C0 /3 ib	RCR r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rcrb		Rotate 9 bits (CF, r/m8) right imm8 times.
D1 /3	RCR r/m16, 1	M1	RW, R		E.CF E.OF				V	V		rcrw		Rotate 17 bits (CF, r/m16) right once.
D3 /3	RCR r/m16, CL	MC	RW, R		e.cf e.of	e.of			V	V		rcrw		Rotate 17 bits (CF, r/m16) right CL times.
C1 /3 ib	RCR r/m16, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rcrw		Rotate 17 bits (CF, r/m16) right imm8 times.
D1 /3	RCR r/m32, 1	M1	RW, R		E.CF E.OF				V	V		rcrl		Rotate 33 bits (CF, r/m32) right once. Uses a 6 bit count.
REX.W+ D1 /3	RCR r/m64, 1	M1	RW, R		E.CF E.OF				V	NE		rcrq		Rotate 65 bits (CF, r/m64) right once. Uses a 6 bit count.
D3 /3	RCR r/m32, CL	MC	RW, R		e.cf e.of	e.of			V	V		rcrl		Rotate 33 bits (CF, r/m32) right CL times.
REX.W+ D3 /3	RCR r/m64, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rcrq		Rotate 65 bits (CF, r/m64) right CL times. Uses a 6 bit count.
C1 /3 ib	RCR r/m32, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rcrl		Rotate 33 bits (CF, r/m32) right imm8 times.
REX.W+ C1 /3 ib	RCR r/m64, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rcrq		Rotate 65 bits (CF, r/m64) right imm8 times. Uses a 6 bit count.
D0 /0	ROL r/m8, 1	M1	RW, R		E.CF E.OF				V	V		rolb		Rotate 8 bits r/m8 left once.
REX+ D0 /0	ROL r/m8, 1	M1	RW, R		E.CF E.OF				V	NE		rolb		Rotate 8 bits r/m8 left once.
D2 /0	ROL r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	V		rolb		Rotate 8 bits r/m8 left CL times.
REX+ D2 /0	ROL r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rolb		Rotate 8 bits r/m8 left CL times.
C0 /0 ib	ROL r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rolb		Rotate 8 bits r/m8 left imm8 times.
REX+ C0 /0 ib	ROL r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rolb		Rotate 8 bits r/m8 left imm8 times.
D1 /0	ROL r/m16, 1	M1	RW, R		E.CF E.OF				V	V		rolw		Rotate 16 bits r/m16 left once.
D3 /0	ROL r/m16, CL	MC	RW, R		e.cf e.of	e.of			V	V		rolw		Rotate 16 bits r/m16 left CL times.
C1 /0 ib	ROL r/m16, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rolw		Rotate 16 bits r/m16 left imm8 times.
D1 /0	ROL r/m32, 1	M1	RW, R		E.CF E.OF				V	V		roll		Rotate 32 bits r/m32 left once.
REX.W+ D1 /0	ROL r/m64, 1	M1	RW, R		E.CF E.OF				V	NE		rolq		Rotate 64 bits r/m64 left once. Uses a 6 bit count.
D3 /0	ROL r/m32, CL	MC	RW, R		e.cf e.of	e.of			V	V		roll		Rotate 32 bits r/m32 left CL times.
REX.W+ D3 /0	ROL r/m64, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rolq		Rotate 64 bits r/m64 left CL times. Uses a 6 bit count.
C1 /0 ib	ROL r/m32, imm8	MI	RW, R		e.cf e.of	e.of			V	V		roll		Rotate 32 bits r/m32 left imm8 times.
REX.W+ C1 /0 ib	ROL r/m64, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rolq		Rotate 64 bits r/m64 left imm8 times. Uses a 6 bit count.
D0 /1	ROR r/m8, 1	M1	RW, R		E.CF E.OF				V	V		rorb		Rotate 8 bits r/m8 right once.
REX+ D0 /1	ROR r/m8, 1	M1	RW, R		E.CF E.OF				V	NE		rorb		Rotate 8 bits r/m8 right once.
D2 /1	ROR r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	V		rorb		Rotate 8 bits r/m8 right CL times.
REX+ D2 /1	ROR r/m8, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rorb		Rotate 8 bits r/m8 right CL times.
C0 /1 ib	ROR r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rorb		Rotate 8 bits r/m16 right imm8 times.
REX+ C0 /1 ib	ROR r/m8, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rorb		Rotate 8 bits r/m16 right imm8 times.
D1 /1	ROR r/m16, 1	M1	RW, R		E.CF E.OF				V	V		rorw		Rotate 16 bits r/m16 right once.
D3 /1	ROR r/m16, CL	MC	RW, R		e.cf e.of	e.of			V	V		rorw		Rotate 16 bits r/m16 right CL times.
C1 /1 ib	ROR r/m16, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rorw		Rotate 16 bits r/m16 right imm8 times.
D1 /1	ROR r/m32, 1	M1	RW, R		E.CF E.OF				V	V		rorl		Rotate 32 bits r/m32 right once.
REX.W+ D1 /1	ROR r/m64, 1	M1	RW, R		E.CF E.OF				V	NE		rorq		Rotate 64 bits r/m64 right once. Uses a 6 bit count.
D3 /1	ROR r/m32, CL	MC	RW, R		e.cf e.of	e.of			V	V		rorl		Rotate 32 bits r/m32 right CL times.
REX.W+ D3 /1	ROR r/m64, CL	MC	RW, R		e.cf e.of	e.of			V	NE		rorq		Rotate 64 bits r/m64 right CL times. Uses a 6 bit count.
C1 /1 ib	ROR r/m32, imm8	MI	RW, R		e.cf e.of	e.of			V	V		rorl		Rotate 32 bits r/m32 right imm8 times.
REX.W+ C1 /1 ib	ROR r/m64, imm8	MI	RW, R		e.cf e.of	e.of			V	NE		rorq		Rotate 64 bits r/m64 right imm8 times. Uses a 6 bit count.
														
0F 53 /r	RCPPS xmm1, xmm2/m128	RM	W, R						V	V	SSE	rcpps		Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 53 /r	VRCPPS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vrcpps		Computes the approximate reciprocals of packed single-precision values in xmm2/mem and stores the results in xmm1.
VEX.256.0F.WIG 53 /r	VRCPPS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vrcpps		Computes the approximate reciprocals of packed single-precision values in ymm2/mem and stores the results in ymm1.
														
F3 0F 53 /r	RCPSS xmm1, xmm2/m32	RM	Z, R						V	V	SSE	rcpss		Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm2/m32 and stores the result in xmm1.
VEX.NDS.LIG.F3.0F.WIG 53 /r	VRCPSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vrcpss		Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
														
F3 0F AE /0	RDFSBASE r32	M	W						V	I	FSGSBASE	rdfsbase		Load the 32-bit destination register with the FS base address.
REX.W+ F3 0F AE /0	RDFSBASE r64	M	W						V	I	FSGSBASE	rdfsbase		Load the 64-bit destination register with the FS base address.
F3 0F AE /1	RDGSBASE r32	M	W						V	I	FSGSBASE	rdgsbase		Load the 32-bit destination register with the GS base address.
REX.W+ F3 0F AE /1	RDGSBASE r64	M	W						V	I	FSGSBASE	rdgsbase		Load the 64-bit destination register with the GS base address.
														
0F 32	RDMSR						NO	YES	V	V	MSR			Read MSR specified by ECX into EDX:EAX.
														
0F 33	RDPMC						YES	YES*	V	V	MSR			Read performance-monitoring counter specified by ECX into EDX:EAX.
														
0F C7 /6	RDRAND r16	M	W		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	RDRAND	rdrand		Read a 16-bit random number and store in the destination register.
0F C7 /6	RDRAND r32	M	W		E.CF E.OF E.SF E.ZF E.AF E.PF				V	V	RDRAND	rdrand		Read a 32-bit random number and store in the destination register.
REX.W+ 0F C7 /6	RDRAND r64	M	W		E.CF E.OF E.SF E.ZF E.AF E.PF				V	I	RDRAND	rdrand		Read a 64-bit random number and store in the destination register.
														
0F 31	RDTSC						YES	YES*	V	V	TSC			Read time-stamp counter into EDX:EAX.
														
0F 01 F9	RDTSCP						YES	YES*	V	V	RDTSCP			Read 64-bit time-stamp counter and 32-bit IA32_TSC_AUX value into EDX:EAX and ECX.
														
F3 6C	REP_INS m8, DX 	NP	I, R	ECX E.IOPL E.VM E.DF RDI					V	V	REP_GOOD	rep insb	YES	Input (E)CX bytes from port DX into ES:[(E)DI].
F3 REX.W+ 6C	REP_INS m8, DX	NP	I, R	RCX E.IOPL E.VM E.DF RDI					V	NE	REP_GOOD	rep insb		Input RCX bytes from port DX into [RDI].
F3 6D	REP_INS m16, DX	NP	I, R	ECX E.IOPL E.VM E.DF RDI					V	V	REP_GOOD	rep insw	YES	Input (E)CX words from port DX into ES:[(E)DI.]
F3 6D	REP_INS m32, DX	NP	I, R	ECX E.IOPL E.VM E.DF RDI					V	V	REP_GOOD	rep insl	YES	Input (E)CX doublewords from port DX into ES:[(E)DI].
F3 REX.W+ 6D	REP_INS m64, DX	NP	I, R	RCX E.IOPL E.VM E.DF RDI					V	NE	REP_GOOD	rep insq		Input RCX default size from port DX into [RDI].
F3 A4	REP_MOVS m8, m8	NP	I, I	ECX E.DF ESI rsi EDI rdi					V	V	REP_GOOD	rep movsb	YES	Move (E)CX bytes from DS:[(E)SI] to ES:[(E)DI].
F3 REX.W+ A4	REP_MOVS m8, m8	NP	I, I	RCX E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	rep movsb		Move RCX bytes from [RSI] to [RDI].
F3 A5	REP_MOVS m16, m16	NP	I, I	ECX E.DF ESI rsi EDI rdi					V	V	REP_GOOD	rep movsw	YES	Move (E)CX words from DS:[(E)SI] to ES:[(E)DI].
F3 A5	REP_MOVS m32, m32	NP	I, I	ECX E.DF ESI rsi EDI rdi					V	V	REP_GOOD	rep movsl	YES	Move (E)CX doublewords from DS:[(E)SI] to ES:[(E)DI].
F3 REX.W+ A5	REP_MOVS m64, m64	NP	I, I	RCX E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	rep movsq		Move RCX quadwords from [RSI] to [RDI].
F3 6E	REP_OUTS DX, m8	NP	R, I	ECX E.IOPL E.VM E.DF RSI					V	V	REP_GOOD	rep outsb	YES	Output (E)CX bytes from DS:[(E)SI] to port DX.
F3 REX.W+ 6E	REP_OUTS DX, m8	NP	R, I	RCX E.IOPL E.VM E.DF RSI					V	NE	REP_GOOD	rep outsb		Output RCX bytes from [RSI] to port DX.
F3 6F	REP_OUTS DX, m16	NP	R, I	ECX E.IOPL E.VM E.DF RSI					V	V	REP_GOOD	rep outsw	YES	Output (E)CX words from DS:[(E)SI] to port DX.
F3 6F	REP_OUTS DX, m32	NP	R, I	ECX E.IOPL E.VM E.DF RSI					V	V	REP_GOOD	rep outsl	YES	Output (E)CX doublewords from DS:[(E)SI] to port DX.
F3 REX.W+ 6F	REP_OUTS DX, m64	NP	R, I	RCX E.IOPL E.VM E.DF RSI					V	NE	REP_GOOD	rep outsq		Output RCX default size from [RSI] to port DX.
F3 AC	REP_LODS AL	NP	W	ECX E.DF SI rsi					V	V	REP_GOOD	rep lodsb	YES	Load (E)CX bytes from DS:[(E)SI] to AL.
F3 REX.W+ AC	REP_LODS AL	NP	W	RCX E.DF SI rsi					V	NE	REP_GOOD	rep lodsb		Load RCX bytes from [RSI] to AL.
F3 AD	REP_LODS AX	NP	W	ECX E.DF SI rsi					V	V	REP_GOOD	rep lodsw	YES	Load (E)CX words from DS:[(E)SI] to AX.
F3 AD	REP_LODS EAX	NP	W	ECX E.DF SI rsi					V	V	REP_GOOD	rep lodsl	YES	Load (E)CX doublewords from DS:[(E)SI] to EAX.
F3 REX.W+ AD	REP_LODS RAX	NP	W	RCX E.DF SI rsi					V	NE	REP_GOOD	rep lodsq		Load RCX quadwords from [RSI] to RAX.
F3 AA	REP_STOS m8	NP	I	ECX E.DF RDI AL					V	V	REP_GOOD	rep stosb	YES	Fill (E)CX bytes at ES:[(E)DI] with AL.
F3 REX.W+ AA	REP_STOS m8	NP	I	RCX E.DF RDI AL					V	NE	REP_GOOD	rep stosb		Fill RCX bytes at [RDI] with AL.
F3 AB	REP_STOS m16	NP	I	ECX E.DF RDI AL					V	V	REP_GOOD	rep stosw	YES	Fill (E)CX words at ES:[(E)DI] with AX.
F3 AB	REP_STOS m32	NP	I	ECX E.DF RDI AL					V	V	REP_GOOD	rep stosl	YES	Fill (E)CX doublewords at ES:[(E)DI] with EAX.
F3 REX.W+ AB	REP_STOS m64	NP	I	RCX E.DF RDI AL					V	NE	REP_GOOD	rep stosq		Fill RCX quadwords at [RDI] with RAX.
F3 A6	REPE_CMPS m8, m8	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repz cmpsb	YES	Find nonmatching bytes in ES:[(E)DI] and DS:[(E)SI].
F3 REX.W+ A6	REPE_CMPS m8, m8	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	repz cmpsb		Find non-matching bytes in [RDI] and [RSI].
F3 A7	REPE_CMPS m16, m16	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repz cmpsw	YES	Find nonmatching words in ES:[(E)DI] and DS:[(E)SI].
F3 A7	REPE_CMPS m32, m32	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repz cmpsl	YES	Find nonmatching doublewords in ES:[(E)DI] and DS:[(E)SI].
F3 REX.W+ A7	REPE_CMPS m64, m64	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	repz cmpsq		Find non-matching quadwords in [RDI] and [RSI].
F3 AE	REPE_SCAS m8	NP	I	E.ZF E.DF RDI AL					V	V	REP_GOOD	repz scasb	YES	Find non-AL byte starting at ES:[(E)DI].
F3 REX.W+ AE	REPE_SCAS m8	NP	I	E.ZF E.DF RDI AL					V	NE	REP_GOOD	repz scasb		Find non-AL byte starting at [RDI].
F3 AF	REPE_SCAS m16	NP	I	E.ZF E.DF RDI AX					V	V	REP_GOOD	repz scasw	YES	Find non-AX word starting at ES:[(E)DI].
F3 AF	REPE_SCAS m32	NP	I	E.ZF E.DF RDI EAX					V	V	REP_GOOD	repz scasl	YES	Find non-EAX doubleword starting at ES:[(E)DI].
F3 REX.W+ AF	REPE_SCAS m64	NP	I	E.ZF E.DF RDI RAX					V	NE	REP_GOOD	repz scasq		Find non-RAX quadword starting at [RDI].
F2 A6	REPNE_CMPS m8, m8	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repnz cmpsb	YES	Find matching bytes in ES:[(E)DI] and DS:[(E)SI].
F2 REX.W+ A6	REPNE_CMPS m8, m8	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	repnz cmpsb		Find matching bytes in [RDI] and [RSI].
F2 A7	REPNE_CMPS m16, m16	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repnz cmpsw	YES	Find matching words in ES:[(E)DI] and DS:[(E)SI].
F2 A7	REPNE_CMPS m32, m32	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	V	REP_GOOD	repnz cmpsl	YES	Find matching doublewords in ES:[(E)DI] and DS:[(E)SI].
F2 REX.W+ A7	REPNE_CMPS m64, m64	NP	I, I	E.ZF E.DF ESI rsi EDI rdi					V	NE	REP_GOOD	repnz cmpsq		Find matching doublewords in [RDI] and [RSI].
F2 AE	REPNE_SCAS m8	NP	I	E.ZF E.DF RDI AL					V	V	REP_GOOD	repnz scasb	YES	Find AL, starting at ES:[(E)DI].
F2 REX.W+ AE	REPNE_SCAS m8	NP	I	E.ZF E.DF RDI AL					V	NE	REP_GOOD	repnz scasb		Find AL, starting at [RDI].
F2 AF	REPNE_SCAS m16	NP	I	E.ZF E.DF RDI AX					V	V	REP_GOOD	repnz scasw	YES	Find AX, starting at ES:[(E)DI].
F2 AF	REPNE_SCAS m32	NP	I	E.ZF E.DF RDI EAX					V	V	REP_GOOD	repnz scasl	YES	Find EAX, starting at ES:[(E)DI].
F2 REX.W+ AF	REPNE_SCAS m64	NP	I	E.ZF E.DF RDI RAX					V	NE	REP_GOOD	repnz scasq		Find RAX, starting at [RDI].
														
C3	RET	NP			RIP				V	V		retq		Near return to calling procedure.
CB	RET far	NP	I	???	???	???			V	V		lretl		Far return to calling procedure.
C2 iw	RET imm16	I	R		RIP RSP				V	V		retq		Near return to calling procedure and pop imm16 bytes from stack.
CA iw	RET imm16, far	I	R, I	???	???	???			V	V		retq		Far return to calling procedure and pop imm16 bytes from stack.
														
VEX.LZ.F2.0F3A.W0 F0 /r ib	RORX r32, r/m32, imm8	RMI	W, R, R						V	V	BMI2	rorxl		Rotate 32-bit r/m32 right imm8 times without affecting arithmetic flags.
VEX.LZ.F2.0F3A.W1 F0 /r ib	RORX r64, r/m64, imm8	RMI	W, R, R						V	NE	BMI2	rorxq		Rotate 64-bit r/m64 right imm8 times without affecting arithmetic flags.
														
66 0F 3A 09 /r ib	ROUNDPD xmm1, xmm2/m128, imm8	RMI	W, R, R	M.RC					V	V	SSE4_1	roundpd		Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.128.66.0F3A.WIG 09 /r ib	VROUNDPD xmm1, xmm2/m128, imm8	RMI	Z, R, R	M.RC					V	V	AVX	vroundpd		Round packed double-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.256.66.0F3A.WIG 09 /r ib	VROUNDPD ymm1, ymm2/m256, imm8	RMI	W, R, R	M.RC					V	V	AVX	vroundpd		Round packed double-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
														
66 0F 3A 08 /r ib	ROUNDPS xmm1, xmm2/m128, imm8	RMI	W, R, R	M.RC					V	V	SSE4_1	roundps		Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.128.66.0F3A.WIG 08 /r ib	VROUNDPS xmm1, xmm2/m128, imm8	RMI	Z, R, R	M.RC					V	V	AVX	vroundps		Round packed single-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.256.66.0F3A.WIG 08 /r ib	VROUNDPS ymm1, ymm2/m256, imm8	RMI	W, R, R	M.RC					V	V	AVX	vroundps		Round packed single-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
														
66 0F 3A 0B /r ib	ROUNDSD xmm1, xmm2/m64, imm8	RMI	W, R, R	M.RC					V	V	SSE4_1	roundsd		Round the low packed double precision floating-point value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.NDS.LIG.66.0F3A.WIG 0B /r ib	VROUNDSD xmm1, xmm2, xmm3/m64, imm8	RVMI	Z, R, R, R	M.RC					V	V	AVX	vroundsd		Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
														
66 0F 3A 0A /r ib	ROUNDSS xmm1, xmm2/m32, imm8	RMI	W, R, R	M.RC					V	V	SSE4_1	roundss		Round the low packed single precision floating-point value in xmm2/m32 and place the result in xmm1. The rounding mode is determined by imm8.
VEX.NDS.LIG.66.0F3A.WIG 0A ib	VROUNDSS xmm1, xmm2, xmm3/m32, imm8	RVMI	Z, R, R, R	M.RC					V	V	AVX	vroundss		Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
														
0F AA	RSM						NO	YES	I	V				Resume operation of interrupted program.
														
0F 52 /r	RSQRTPS xmm1, xmm2/m128	RM	W, R						V	V	SSE	rsqrtps		Computes the approximate reciprocals of the square roots of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 52 /r	VRSQRTPS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vrsqrtps		Computes the approximate reciprocals of the square roots of packed single-precision values in xmm2/mem and stores the results in xmm1.
VEX.256.0F.WIG 52 /r	VRSQRTPS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vrsqrtps		Computes the approximate reciprocals of the square roots of packed single-precision values in ymm2/mem and stores the results in ymm1.
														
F3 0F 52 /r	RSQRTSS xmm1, xmm2/m32	RM	W, R						V	V	SSE	rsqrtss		Computes the approximate reciprocal of the square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1.
VEX.NDS.LIG.F3.0F.WIG 52 /r	VRSQRTSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vrsqrtss		Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
														
9E	SAHF	NP		AH	E.SF E.ZF E.AF E.PF E.CF				V	V		sahf		Loads SF, ZF, AF, PF, and CF from AH into EFLAGS register.
														
D0 /4	SAL r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		salb		Multiply r/m8 by 2, once.
REX+ D0 /4	SAL r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		salb		Multiply r/m8 by 2, once.
D2 /4	SAL r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		salb		Multiply r/m8 by 2, CL times.
REX+ D2 /4	SAL r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		salb		Multiply r/m8 by 2, CL times.
C0 /4 ib	SAL r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		salb		Multiply r/m8 by 2, imm8 times.
REX+ C0 /4 ib	SAL r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		salb		Multiply r/m8 by 2, imm8 times.
D1 /4	SAL r/m16, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		salw		Multiply r/m16 by 2, once.
D3 /4	SAL r/m16, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		salw		Multiply r/m16 by 2, CL times.
C1 /4 ib	SAL r/m16, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		salw		Multiply r/m16 by 2, imm8 times.
D1 /4	SAL r/m32, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		sall		Multiply r/m32 by 2, once.
REX.W+ D1 /4	SAL r/m64, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		salq		Multiply r/m64 by 2, once.
D3 /4	SAL r/m32, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sall		Multiply r/m32 by 2, CL times.
REX.W+ D3 /4	SAL r/m64, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		salq		Multiply r/m64 by 2, CL times.
C1 /4 ib	SAL r/m32, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sall		Multiply r/m32 by 2, imm8 times.
REX.W+ C1 /4 ib	SAL r/m64, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		salq		Multiply r/m64 by 2, imm8 times.
D0 /7	SAR r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		sarb		Signed divide r/m8 by 2, once.
REX+ D0 /7	SAR r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		sarb		Signed divide r/m8 by 2, once.
D2 /7	SAR r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarb		Signed divide r/m8 by 2, CL times.
REX+ D2 /7	SAR r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		sarb		Signed divide r/m8 by 2, CL times.
C0 /7 ib	SAR r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarb		Signed divide r/m8 by 2, imm8 time.
REX+ C0 /7 ib	SAR r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		sarb		Signed divide r/m8 by 2, imm8 time.
D1 /7	SAR r/m16,1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		sarw		Signed divide r/m16 by 2, once.
D3 /7	SAR r/m16, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarw		Signed divide r/m16 by 2, CL times.
C1 /7 ib	SAR r/m16, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarw		Signed divide r/m16 by 2, imm8 times.
D1 /7	SAR r/m32, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		sarl		Signed divide r/m32 by 2, once.
REX.W+ D1 /7	SAR r/m64, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		sarq		Signed divide r/m32 by 2, once.
D3 /7	SAR r/m32, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarl		Signed divide r/m32 by 2, CL times.
REX.W+ D3 /7	SAR r/m64, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		sarq		Signed divide r/m32 by 2, CL times.
C1 /7 ib	SAR r/m32, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	V		sarl		Signed divide r/m32 by 2, imm8 times.
REX.W+ C1 /7 ib	SAR r/m64, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.of e.af			V	NE		sarq		Signed divide r/m32 by 2, imm8 times.
D0 /4	SHL r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shlb		Multiply r/m8 by 2, once.
REX+ D0 /4	SHL r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		shlb		Multiply r/m8 by 2, once.
D2 /4	SHL r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shlb		Multiply r/m8 by 2, CL times.
REX+ D2 /4	SHL r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shlb		Multiply r/m8 by 2, CL times.
C0 /4 ib	SHL r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shlb		Multiply r/m8 by 2, imm8 times.
REX+ C0 /4 ib	SHL r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shlb		Multiply r/m8 by 2, imm8 times.
D1 /4	SHL r/m16,1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shlw		Multiply r/m16 by 2, once.
D3 /4	SHL r/m16, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shlw		Multiply r/m16 by 2, CL times.
C1 /4 ib	SHL r/m16, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shlw		Multiply r/m16 by 2, imm8 times.
D1 /4	SHL r/m32,1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shll		Multiply r/m32 by 2, once.
REX.W+ D1 /4	SHL r/m64,1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		shlq		Multiply r/m64 by 2, once.
D3 /4	SHL r/m32, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shll		Multiply r/m32 by 2, CL times.
REX.W+ D3 /4	SHL r/m64, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shlq		Multiply r/m32 by 2, CL times.
C1 /4 ib	SHL r/m32, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shll		Multiply r/m32 by 2, imm8 times.
REX.W+ C1 /4 ib	SHL r/m64, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shlq		Multiply r/m32 by 2, imm8 times.
D0 /5	SHR r/m8,1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shrb		Unsigned divide r/m8 by 2, once.
REX+ D0 /5	SHR r/m8, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		shrb		Unsigned divide r/m8 by 2, once.
D2 /5	SHR r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrb		Unsigned divide r/m8 by 2, CL times.
REX+ D2 /5	SHR r/m8, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shrb		Unsigned divide r/m8 by 2, CL times.
C0 /5 ib	SHR r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrb		Unsigned divide r/m8 by 2, imm8 times.
REX+ C0 /5 ib	SHR r/m8, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shrb		Unsigned divide r/m8 by 2, imm8 times.
D1 /5	SHR r/m16, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shrw		Unsigned divide r/m16 by 2, once.
D3 /5	SHR r/m16, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrw		Unsigned divide r/m16 by 2, CL times
C1 /5 ib	SHR r/m16, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrw		Unsigned divide r/m16 by 2, imm8 times.
D1 /5	SHR r/m32, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	V		shrl		Unsigned divide r/m32 by 2, once.
REX.W+ D1 /5	SHR r/m64, 1	M1	RW, R		E.CF E.OF E.PF E.SF E.ZF	E.AF			V	NE		shrq		Unsigned divide r/m32 by 2, once.
D3 /5	SHR r/m32, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrl		Unsigned divide r/m32 by 2, CL times.
REX.W+ D3 /5	SHR r/m64, CL	MC	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shrq		Unsigned divide r/m32 by 2, CL times.
C1 /5 ib	SHR r/m32, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	V		shrl		Unsigned divide r/m32 by 2, imm8 times.
REX.W+ C1 /5 ib	SHR r/m64, imm8	MI	RW, R		e.cf e.of e.pf e.sf e.zf	e.cf e.of e.af			V	NE		shrq		Unsigned divide r/m32 by 2, imm8 times.
														
VEX.NDS.LZ.F3.0F38.W0 F7 /r	SARX r32a, r/m32, r32b	RMV	W, R, R						V	V	BMI2	sarxl		Shift r/m32 arithmetically right with count specified in r32b
VEX.NDS.LZ.66.0F38.W0 F7 /r	SHLX r32a, r/m32, r32b	RMV	W, R, R						V	V	BMI2	shlxl		Shift r/m32 logically left with count specified in r32b
VEX.NDS.LZ.F2.0F38.W0 F7 /r	SHRX r32a, r/m32, r32b	RMV	W, R, R						V	V	BMI2	shrxl		Shift r/m32 logically right with count specified in r32b
VEX.NDS.LZ.F3.0F38.W1 F7 /r	SARX r64a, r/m64, r64b	RMV	W, R, R						V	NE	BMI2	sarxq		Shift r/m64 arithmetically right with count specified in r64b
VEX.NDS.LZ.66.0F38.W1 F7 /r	SHLX r64a, r/m64, r64b	RMV	W, R, R						V	NE	BMI2	shlxq		Shift r/m64 logically left with count specified in r64b.
VEX.NDS.LZ.F2.0F38.W1 F7 /r	SHRX r64a, r/m64, r64b	RMV	W, R, R						V	NE	BMI2	shrxq		Shift r/m64 logically right with count specified in r64b
														
1C ib	SBB AL, imm8	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbb		Subtract with borrow imm8 from AL.
1D iw	SBB AX, imm16	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbw		Subtract with borrow imm16 from AX.
1D id	SBB EAX, imm32	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbl		Subtract with borrow imm32 from EAX.
REX.W+ 1D id	SBB RAX, imm32	I	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbq		Subtract with borrow sign-extended imm.32 to 64-bits from RAX.
80 /3 ib	SBB r/m8, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbb		Subtract with borrow imm8 from r/m8.
REX+ 80 /3 ib	SBB r/m8, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbb		Subtract with borrow imm8 from r/m8.
81 /3 iw	SBB r/m16, imm16	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbw		Subtract with borrow imm16 from r/m16.
81 /3 id	SBB r/m32, imm32	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbl		Subtract with borrow imm32 from r/m32.
REX.W+ 81 /3 id	SBB r/m64, imm32	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbq		Subtract with borrow sign-extended imm32 to 64-bits from r/m64.
83 /3 ib	SBB r/m16, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbw	YES	Subtract with borrow sign-extended imm8 from r/m16.
83 /3 ib	SBB r/m32, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbl	YES	Subtract with borrow sign-extended imm8 from r/m32.
REX.W+ 83 /3 ib	SBB r/m64, imm8	MI	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbq	YES	Subtract with borrow sign-extended imm8 from r/m64.
18 /r	SBB r/m8, r8	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbb	YES	Subtract with borrow r8 from r/m8.
REX+ 18 /r	SBB r/m8, r8	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbb	YES	Subtract with borrow r8 from r/m8.
19 /r	SBB r/m16, r16	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbw	YES	Subtract with borrow r16 from r/m16.
19 /r	SBB r/m32, r32	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbl	YES	Subtract with borrow r32 from r/m32.
REX.W+ 19 /r	SBB r/m64, r64	MR	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbq	YES	Subtract with borrow r64 from r/m64.
1A /r	SBB r8, r/m8	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbb		Subtract with borrow r/m8 from r8.
REX+ 1A /r	SBB r8, r/m8	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbb		Subtract with borrow r/m8 from r8.
1B /r	SBB r16, r/m16	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbw		Subtract with borrow r/m16 from r16.
1B /r	SBB r32, r/m32	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	V		sbbl		Subtract with borrow r/m32 from r32.
REX.W+ 1B /r	SBB r64, r/m64	RM	RW, R	E.CF	E.OF E.SF E.ZF E.AF E.PF E.CF				V	NE		sbbq		Subtract with borrow r/m64 from r64.
														
AE	SCAS m8	NP	I	E.DF RDI AL					V	V		scas		Compare AL with byte at ES:(E)DI or RDI, then set status flags.
AF	SCAS m16	NP	I	E.DF RDI AX					V	V		scas		Compare AX with word at ES:(E)DI or RDI, then set status flags.
AF	SCAS m32	NP	I	E.DF RDI EAX					V	V		scas		Compare EAX with doubleword at ES(E)DI or RDI then set status flags.
REX.W+ AF	SCAS m64	NP	I	E.DF RDI RAX					V	NE		scas		Compare RAX with quadword at RDI or EDI then set status flags.
AE	SCASB	NP		E.DF RDI AL					V	V		scasb		Compare AL with byte at ES:(E)DI or RDI then set status flags.
PREF.66+ AF	SCASW	NP		E.DF RDI AX					V	V		scasw		Compare AX with word at ES:(E)DI or RDI then set status flags.
AF	SCASD	NP		E.DF RDI EAX					V	V		scasl		Compare EAX with doubleword at ES:(E)DI or RDI then set status flags.
REX.W+ AF	SCASQ	NP		E.DF RDI RAX					V	NE		scasq		Compare RAX with quadword at RDI or EDI then set status flags.
														
0F 97 /0	SETA r/m8	M	w	E.CF E.ZF					V	V		seta		Set byte if above (CF=0 and ZF=0).
REX+ 0F 97 /0 	SETA r/m8	M	w	E.CF E.ZF					V	NE		seta		Set byte if above (CF=0 and ZF=0).
0F 93 /0	SETAE r/m8	M	w	E.CF					V	V		setae		Set byte if above or equal (CF=0).
REX+ 0F 93 /0	SETAE r/m8	M	w	E.CF					V	NE		setae		Set byte if above or equal (CF=0).
0F 92 /0	SETB r/m8	M	w	E.CF					V	V		setb		Set byte if below (CF=1).
REX+ 0F 92 /0	SETB r/m8	M	w	E.CF					V	NE		setb		Set byte if below (CF=1).
0F 96 /0	SETBE r/m8	M	w	E.CF E.ZF					V	V		setbe		Set byte if below or equal (CF=1 or ZF=1).
REX+ 0F 96 /0	SETBE r/m8	M	w	E.CF E.ZF					V	NE		setbe		Set byte if below or equal (CF=1 or ZF=1).
0F 92 /0	SETC r/m8	M	w	E.CF					V	V		setc		Set byte if carry (CF=1).
REX+ 0F 92 /0	SETC r/m8	M	w	E.CF					V	NE		setc		Set byte if carry (CF=1).
0F 94 /0	SETE r/m8	M	w	E.ZF					V	V		sete		Set byte if equal (ZF=1).
REX+ 0F 94 /0	SETE r/m8	M	w	E.ZF					V	NE		sete		Set byte if equal (ZF=1).
0F 9F /0	SETG r/m8	M	w	E.ZF E.SF E.OF					V	V		setg		Set byte if greater (ZF=0 and SF=OF).
REX+ 0F 9F /0	SETG r/m8	M	w	E.ZF E.SF E.OF					V	NE		setg		Set byte if greater (ZF=0 and SF=OF).
0F 9D /0	SETGE r/m8	M	w	E.SF E.OF					V	V		setge		Set byte if greater or equal (SF=OF).
REX+ 0F 9D /0	SETGE r/m8	M	w	E.SF E.OF					V	NE		setge		Set byte if greater or equal (SF=OF).
0F 9C /0	SETL r/m8	M	w	E.SF E.OF					V	V		setl		Set byte if less (SF!= OF).
REX+ 0F 9C /0	SETL r/m8	M	w	E.SF E.OF					V	NE		setl		Set byte if less (SF!= OF).
0F 9E /0	SETLE r/m8	M	w	E.ZF E.SF E.OF					V	V		setle		Set byte if less or equal (ZF=1 or SF!= OF).
REX+ 0F 9E /0	SETLE r/m8	M	w	E.ZF E.SF E.OF					V	NE		setle		Set byte if less or equal (ZF=1 or SF!= OF).
0F 96 /0	SETNA r/m8	M	w	E.CF E.ZF					V	V		setna		Set byte if not above (CF=1 or ZF=1).
REX+ 0F 96 /0	SETNA r/m8	M	w	E.CF E.ZF					V	NE		setna		Set byte if not above (CF=1 or ZF=1).
0F 92 /0	SETNAE r/m8	M	w	E.CF					V	V		setnae		Set byte if not above or equal (CF=1).
REX+ 0F 92 /0	SETNAE r/m8	M	w	E.CF					V	NE		setnae		Set byte if not above or equal (CF=1).
0F 93 /0	SETNB r/m8	M	w	E.CF					V	V		setnb		Set byte if not below (CF=0).
REX+ 0F 93 /0	SETNB r/m8	M	w	E.CF					V	NE		setnb		Set byte if not below (CF=0).
0F 97 /0	SETNBE r/m8	M	w	E.CF E.ZF					V	V		setnbe		Set byte if not below or equal (CF=0 and ZF=0).
REX+ 0F 97 /0	SETNBE r/m8	M	w	E.CF E.ZF					V	NE		setnbe		Set byte if not below or equal (CF=0 and ZF=0).
0F 93 /0	SETNC r/m8	M	w	E.CF					V	V		setnc		Set byte if not carry (CF=0).
REX+ 0F 93 /0	SETNC r/m8	M	w	E.CF					V	NE		setnc		Set byte if not carry (CF=0).
0F 95 /0	SETNE r/m8	M	w	E.ZF					V	V		setne		Set byte if not equal (ZF=0).
REX+ 0F 95 /0	SETNE r/m8	M	w	E.ZF					V	NE		setne		Set byte if not equal (ZF=0).
0F 9E /0	SETNG r/m8	M	w	E.ZF E.SF E.OF					V	V		setng		Set byte if not greater (ZF=1 or SF!= OF)
REX+ 0F 9E /0	SETNG r/m8	M	w	E.ZF E.SF E.OF					V	NE		setng		Set byte if not greater (ZF=1 or SF!= OF)
0F 9C /0	SETNGE r/m8	M	w	E.SF E.OF					V	V		setnge		Set byte if not greater or equal (SF!= OF).
REX+ 0F 9C /0	SETNGE r/m8	M	w	E.SF E.OF					V	NE		setnge		Set byte if not greater or equal (SF!= OF).
0F 9D /0	SETNL r/m8	M	w	E.SF E.OF					V	V		setnl		Set byte if not less (SF=OF).
REX+ 0F 9D /0	SETNL r/m8	M	w	E.SF E.OF					V	NE		setnl		Set byte if not less (SF=OF).
0F 9F /0	SETNLE r/m8	M	w	E.ZF E.SF E.OF					V	V		setnle		Set byte if not less or equal (ZF=0 and SF=OF).
REX+ 0F 9F /0	SETNLE r/m8	M	w	E.ZF E.SF E.OF					V	NE		setnle		Set byte if not less or equal (ZF=0 and SF=OF).
0F 91 /0	SETNO r/m8	M	w	E.OF					V	V		setno		Set byte if not overflow (OF=0).
REX+ 0F 91 /0	SETNO r/m8	M	w	E.OF					V	NE		setno		Set byte if not overflow (OF=0).
0F 9B /0	SETNP r/m8	M	w	E.PF					V	V		setnp		Set byte if not parity (PF=0).
REX+ 0F 9B /0	SETNP r/m8	M	w	E.PF					V	NE		setnp		Set byte if not parity (PF=0).
0F 99 /0	SETNS r/m8	M	w	E.SF					V	V		setns		Set byte if not sign (SF=0).
REX+ 0F 99 /0	SETNS r/m8	M	w	E.SF					V	NE		setns		Set byte if not sign (SF=0).
0F 95 /0	SETNZ r/m8	M	w	E.ZF					V	V		setnz		Set byte if not zero (ZF=0).
REX+ 0F 95 /0	SETNZ r/m8	M	w	E.ZF					V	NE		setnz		Set byte if not zero (ZF=0).
0F 90 /0	SETO r/m8	M	w	E.OF					V	V		seto		Set byte if overflow (OF=1)
REX+ 0F 90 /0	SETO r/m8	M	w	E.OF					V	NE		seto		Set byte if overflow (OF=1)
0F 9A /0	SETP r/m8	M	w	E.PF					V	V		setp		Set byte if parity (PF=1).
REX+ 0F 9A /0	SETP r/m8	M	w	E.PF					V	NE		setp		Set byte if parity (PF=1).
0F 9A /0	SETPE r/m8	M	w	E.PF					V	V		setpe		Set byte if parity even (PF=1).
REX+ 0F 9A /0	SETPE r/m8	M	w	E.PF					V	NE		setpe		Set byte if parity even (PF=1).
0F 9B /0	SETPO r/m8	M	w	E.PF					V	V		setpo		Set byte if parity odd (PF=0).
REX+ 0F 9B /0	SETPO r/m8	M	w	E.PF					V	NE		setpo		Set byte if parity odd (PF=0).
0F 98 /0	SETS r/m8	M	w	E.SF					V	V		sets		Set byte if sign (SF=1).
REX+ 0F 98 /0	SETS r/m8	M	w	E.SF					V	NE		sets		Set byte if sign (SF=1).
0F 94 /0	SETZ r/m8	M	w	E.ZF					V	V		setz		Set byte if zero (ZF=1).
REX+ 0F 94 /0	SETZ r/m8	M	w	E.ZF					V	NE		setz		Set byte if zero (ZF=1).
														
0F AE F8	SFENCE	NP							V	V		sfence		Serializes store operations.
														
0F 01 /0	SGDT m						NO	NO	V	V				Store GDTR to m.
														
0F A4	SHLD r/m16, r16, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shldw		Shift r/m16 to left imm8 places while shifting bits from r16 in from the right.
0F A5	SHLD r/m16, r16, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shldw		Shift r/m16 to left CL places while shifting bits from r16 in from the right.
0F A4	SHLD r/m32, r32, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shldl		Shift r/m32 to left imm8 places while shifting bits from r32 in from the right.
REX.W+ 0F A4 	SHLD r/m64, r64, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	NE		shldq		Shift r/m64 to left imm8 places while shifting bits from r64 in from the right.
0F A5	SHLD r/m32, r32, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shldl		Shift r/m32 to left CL places while shifting bits from r32 in from the right.
REX.W+ 0F A5	SHLD r/m64, r64, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	NE		shldq		Shift r/m64 to left CL places while shifting bits from r64 in from the right.
														
0F AC	SHRD r/m16, r16, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shrdw		Shift r/m16 to right imm8 places while shifting bits from r16 in from the left.
0F AD	SHRD r/m16, r16, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shrdw		Shift r/m16 to right CL places while shifting bits from r16 in from the left.
0F AC	SHRD r/m32, r32, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shrdl		Shift r/m32 to right imm8 places while shifting bits from r32 in from the left.
REX.W+ 0F AC	SHRD r/m64, r64, imm8	MRI	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	NE		shrdq		Shift r/m64 to right imm8 places while shifting bits from r64 in from the left.
0F AD	SHRD r/m32, r32, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	V		shrdl		Shift r/m32 to right CL places while shifting bits from r32 in from the left.
REX.W+ 0F AD	SHRD r/m64, r64, CL	MRC	Rwu, R, R		e.cf e.sf e.zf e.pf	e.of e.sf e.zf e.af e.cf e.pf			V	NE		shrdq		Shift r/m64 to right CL places while shifting bits from r64 in from the left.
														
66 0F C6 /r ib	SHUFPD xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE2	shufpd		Shuffle packed double-precision floating- point values selected by imm8 from xmm1 and xmm2/m128 to xmm1.
VEX.NDS.128.66.0F.WIG C6 /r ib	VSHUFPD xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vshufpd		Shuffle Packed double-precision floating- point values selected by imm8 from xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG C6 /r ib	VSHUFPD ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vshufpd		Shuffle Packed double-precision floating- point values selected by imm8 from ymm2 and ymm3/mem.
														
0F C6 /r ib	SHUFPS xmm1, xmm2/m128, imm8	RMI	RW, R, R						V	V	SSE	shufps		Shuffle packed single-precision floating-point values selected by imm8 from xmm1 and xmm1/m128 to xmm1.
VEX.NDS.128.0F.WIG C6 /r ib	VSHUFPS xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX	vshufps		Shuffle Packed single-precision floating-point values selected by imm8 from xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG C6 /r ib	VSHUFPS ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vshufps		Shuffle Packed single-precision floating-point values selected by imm8 from ymm2 and ymm3/mem.
														
0F 01 /1	SIDT m						NO	NO	V	V				Store IDTR to m.
														
0F 00 /0	SLDT r/m16						NO	NO	V	V				Stores segment selector from LDTR in r/m16.
REX.W+ 0F 00 /0	SLDT r64/m16						NO	NO	V	V				Stores segment selector from LDTR in r64/m16.
														
0F 01 /4	SMSW r/m16						NO*	NO	V	V				Store machine status word to r/m16.
0F 01 /4	SMSW r32/m16						NO*	NO	V	V				Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined.
REX.W+ 0F 01 /4	SMSW r64/m16						NO*	NO	V	V				Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined.
														
66 0F 51 /r	SQRTPD xmm1, xmm2/m128	RM	W, R						V	V	SSE2	sqrtpd		Computes square roots of the packed double- precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.66.0F.WIG 51 /r	VSQRTPD xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vsqrtpd		Computes Square Roots of the packed double- precision floating-point values in xmm2/m128 and stores the result in xmm1.
VEX.256.66.0F.WIG 51 /r	VSQRTPD ymm1, ymm2/m256	RM	W, R						V	V	AVX	vsqrtpd		Computes Square Roots of the packed double- precision floating-point values in ymm2/m256 and stores the result in ymm1
														
0F 51 /r	SQRTPS xmm1, xmm2/m128	RM	W, R						V	V	SSE	sqrtps		Computes square roots of the packed single- precision floating-point values in xmm2/m128 and stores the results in xmm1.
VEX.128.0F.WIG 51 /r	VSQRTPS xmm1, xmm2/m128	RM	Z, R						V	V	AVX	vsqrtps		Computes Square Roots of the packed single- precision floating-point values in xmm2/m128 and stores the result in xmm1.
VEX.256.0F.WIG 51 /r	VSQRTPS ymm1, ymm2/m256	RM	W, R						V	V	AVX	vsqrtps		Computes Square Roots of the packed single- precision floating-point values in ymm2/m256 and stores the result in ymm1.
														
F2 0F 51 /r	SQRTSD xmm1, xmm2/m64	RM	W, R						V	V	SSE2	sqrtsd		Computes square root of the low double- precision floating-point value in xmm2/m64 and stores the results in xmm1.
VEX.NDS.LIG.F2.0F.WIG 51 /r	VSQRTSD xmm1,xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vsqrtsd		Computes square root of the low double- precision floating point value in xmm3/m64 and stores the results in xmm2. Also, upper double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
														
F3 0F 51 /r	SQRTSS xmm1, xmm2/m32	RM	W, R						V	V	SSE	sqrtss		Computes square root of the low single- precision floating-point value in xmm2/m32 and stores the results in xmm1.
VEX.NDS.LIG.F3.0F.WIG 51	VSQRTSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vsqrtss		Computes square root of the low single- precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
														
F9	STC	NP			E.CF				V	V		stc		Set CF flag.
														
FD	STD	NP			E.DF				V	V		std		Set DF flag.
														
FB	STI	NP		E.VM E.IOPL E.VIP	e.if e.vif				V	V		sti		Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction.
														
0F AE /3	STMXCSR m32	M	W	M.*					V	V	SSE	stmxcsr		Store contents of MXCSR register to m32. 
VEX.LZ.0F.WIG AE /3	VSTMXCSR m32	M	W	M.*					V	V	AVX	vstmxcsr		Store contents of MXCSR register to m32.
														
AA	STOS m8	NP	I	E.DF RDI AL					V	V		stosb		For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
AB	STOS m16	NP	I	E.DF RDI AX					V	V		stosw		For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
AB	STOS m32	NP	I	E.DF RDI EAX					V	V		stosl		For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
REX.W+ AB	STOS m64	NP	I	E.DF RDI RAX					V	NE		stosq		Store RAX at address RDI or EDI.
AA	STOSB	NP		E.DF RDI AL					V	V		stosb		For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
PREF.66+ AB	STOSW	NP		E.DF RDI AX					V	V		stosw		For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
AB	STOSD	NP		E.DF RDI EAX					V	V		stosl		For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
REX.W+ AB	STOSQ	NP		E.DF RDI RAX					V	NE		stosq		Store RAX at address RDI or EDI.
														
0F 00 /1	STR r/m16						NO	NO	V	V				Stores segment selector from TR in r/m16.
														
2C ib	SUB AL, imm8	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subb		Subtract imm8 from AL.
2D iw	SUB AX, imm16	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subw		Subtract imm16 from AX.
2D id	SUB EAX, imm32	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subl		Subtract imm32 from EAX.
REX.W+ 2D id	SUB RAX, imm32	I	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subq		Subtract imm32 sign-extended to 64-bits from RAX.
80 /5 ib	SUB r/m8, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subb		Subtract imm8 from r/m8.
REX+ 80 /5 ib	SUB r/m8, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subb		Subtract imm8 from r/m8.
81 /5 iw	SUB r/m16, imm16	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subw		Subtract imm16 from r/m16.
81 /5 id	SUB r/m32, imm32	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subl		Subtract imm32 from r/m32.
REX.W+ 81 /5 id	SUB r/m64, imm32	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subq		Subtract imm32 sign-extended to 64-bits from r/m64.
83 /5 ib	SUB r/m16, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subw	YES	Subtract sign-extended imm8 from r/m16.
83 /5 ib	SUB r/m32, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subl	YES	Subtract sign-extended imm8 from r/m32.
REX.W+ 83 /5 ib	SUB r/m64, imm8	MI	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subq	YES	Subtract sign-extended imm8 from r/m64.
28 /r	SUB r/m8, r8	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subb	YES	Subtract r8 from r/m8.
REX+ 28 /r	SUB r/m8, r8	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subb	YES	Subtract r8 from r/m8.
29 /r	SUB r/m16, r16	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subw	YES	Subtract r16 from r/m16.
29 /r	SUB r/m32, r32	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subl	YES	Subtract r32 from r/m32.
REX.W+ 29 /r	SUB r/m64, r64	MR	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subq	YES	Subtract r64 from r/m64.
2A /r	SUB r8, r/m8	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subb		Subtract r/m8 from r8.
REX+ 2A /r	SUB r8, r/m8	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subb		Subtract r/m8 from r8.
2B /r	SUB r16, r/m16	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subw		Subtract r/m16 from r16.
2B /r	SUB r32, r/m32	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		subl		Subtract r/m32 from r32.
REX.W+ 2B /r	SUB r64, r/m64	RM	RW, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		subq		Subtract r/m64 from r64.
														
66 0F 5C /r	SUBPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	subpd		Subtract packed double-precision floating- point values in xmm2/m128 from xmm1.
VEX.NDS.128.66.0F.WIG 5C /r	VSUBPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vsubpd		Subtract packed double-precision floating- point values in xmm3/mem from xmm2 and stores result in xmm1.
VEX.NDS.256.66.0F.WIG 5C /r	VSUBPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vsubpd		Subtract packed double-precision floating- point values in ymm3/mem from ymm2 and stores result in ymm1.
														
0F 5C /r	SUBPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	subps		Subtract packed single-precision floating-point values in xmm2/mem from xmm1.
VEX.NDS.128.0F.WIG 5C /r	VSUBPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vsubps		Subtract packed single-precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1.
VEX.NDS.256.0F.WIG 5C /r	VSUBPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vsubps		Subtract packed single-precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1.
														
F2 0F 5C /r	SUBSD xmm1, xmm2/m64	RM	RW, R						V	V	SSE2	subsd		Subtracts the low double-precision floating- point values in xmm2/mem64 from xmm1.
VEX.NDS.LIG.F2.0F.WIG 5C /r	VSUBSD xmm1, xmm2, xmm3/m64	RVM	Z, R, R						V	V	AVX	vsubsd		Subtract the low double-precision floating- point value in xmm3/mem from xmm2 and store the result in xmm1.
														
F3 0F 5C /r	SUBSS xmm1, xmm2/m32	RM	RW, R						V	V	SSE	subss		Subtract the lower single-precision floating- point values in xmm2/m32 from xmm1.
VEX.NDS.LIG.F3.0F.WIG 5C /r	VSUBSS xmm1, xmm2, xmm3/m32	RVM	Z, R, R						V	V	AVX	vsubss		Subtract the low single-precision floating- point value in xmm3/mem from xmm2 and store the result in xmm1.
														
0F 01 F8	SWAPGS	NP							V	I		swapgs		Exchanges the current GS base register value with the value contained in MSR address C0000102H.
														
0F 05	SYSCALL	NP		???	???	???			V	I	SYSCALL	syscall		Fast call to privilege level 0 system procedures.
														
0F 34	SYSENTER	NP		???	???	???			V	V	SEP	sysenter		Fast call to privilege level 0 system procedures.
														
0F 35	SYSEXIT	NP		???	???	???			V	V	SEP	sysexit		Fast return to privilege level 3 user code.
REX.W+ 0F 35	SYSEXIT pw	NP	I	???	???	???			V	V	SEP	sysexit		Fast return to 64-bit mode privilege level 3 user code.
														
0F 07	SYSRET	NP		???	???	???			V	I	SYSCALL	sysret		Return to compatibility mode from fast system call
REX.W+ 0F 07	SYSRET pw	NP	I						V	I	SYSCALL	sysret		Return to 64-bit mode from fast system call
														
A8 ib	TEST AL, imm8	I	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testb		AND imm8 with AL; set SF, ZF, PF according to result.
A9 iw	TEST AX, imm16	I	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testw		AND imm16 with AX; set SF, ZF, PF according to result.
A9 id	TEST EAX, imm32	I	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testl		AND imm32 with EAX; set SF, ZF, PF according to result.
REX.W+ A9 id	TEST RAX, imm32	I	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		testq		AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result.
F6 /0 ib	TEST r/m8, imm8	MI	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testb		AND imm8 with r/m8; set SF, ZF, PF according to result.
REX+ F6 /0 ib	TEST r/m8, imm8	MI	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		testb		AND imm8 with r/m8; set SF, ZF, PF according to result.
F7 /0 iw	TEST r/m16, imm16	MI	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testw		AND imm16 with r/m16; set SF, ZF, PF according to result.
F7 /0 id	TEST r/m32, imm32	MI	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testl		AND imm32 with r/m32; set SF, ZF, PF according to result.
REX.W+ F7 /0 id	TEST r/m64, imm32	MI	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		testq		AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result.
84 /r	TEST r/m8, r8	MR	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testb		AND r8 with r/m8; set SF, ZF, PF according to result.
REX+ 84 /r	TEST r/m8, r8	MR	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		testb		AND r8 with r/m8; set SF, ZF, PF according to result.
85 /r	TEST r/m16, r16	MR	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testw		AND r16 with r/m16; set SF, ZF, PF according to result.
85 /r	TEST r/m32, r32	MR	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		testl		AND r32 with r/m32; set SF, ZF, PF according to result.
REX.W+ 85 /r	TEST r/m64, r64	MR	R, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		testq		AND r64 with r/m64; set SF, ZF, PF according to result.
														
F3 0F BC /r	TZCNT r16, r/m16	RM	W, R		E.ZF E.CF	E.AF E.PF E.OF E.SF			V	V	BMI1	tzcntw		Count the number of trailing zero bits in r/m16, return result in r16.
F3 0F BC /r	TZCNT r32, r/m32	RM	W, R		E.ZF E.CF	E.AF E.PF E.OF E.SF			V	V	BMI1	tzcntl		Count the number of trailing zero bits in r/m32, return result in r32.
REX.W+ F3 0F BC /r	TZCNT r64, r/m64	RM	W, R		E.ZF E.CF	E.AF E.PF E.OF E.SF			V	NE	BMI1	tzcntq		Count the number of trailing zero bits in r/m64, return result in r64.
														
66 0F 2E /r	UCOMISD xmm1, xmm2/m64	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	SSE2	ucomisd		Compares (unordered) the low double- precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS accordingly.
VEX.LIG.66.0F.WIG 2E /r	VUCOMISD xmm1, xmm2/m64	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vucomisd		Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
														
0F 2E /r	UCOMISS xmm1, xmm2/m32	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	SSE	ucomiss		Compare lower single-precision floating-point value in xmm1 register with lower single- precision floating-point value in xmm2/mem and set the status flags accordingly.
VEX.LIG.0F.WIG 2E /r	VUCOMISS xmm1, xmm2/m32	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vucomiss		Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
														
0F 0B	UD2	NP							V	V		ud2		Raise invalid opcode exception.
														
66 0F 15 /r	UNPCKHPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	unpckhpd		Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 15 /r	VUNPCKHPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vunpckhpd		Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.66.0F.WIG 15 /r	VUNPCKHPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vunpckhpd		Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256.
														
0F 15 /r	UNPCKHPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	unpckhps		Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm1 and xmm2/mem into xmm1.
VEX.NDS.128.0F.WIG 15 /r	VUNPCKHPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vunpckhps		Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.0F.WIG 15 /r	VUNPCKHPS ymm1, ymm2,ymm3/m256	RVM	W, R, R						V	V	AVX	vunpckhps		Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256.
														
66 0F 14 /r	UNPCKLPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	unpcklpd		Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm1 and xmm2/m128.
VEX.NDS.128.66.0F.WIG 14 /r	VUNPCKLPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vunpcklpd		Unpacks and Interleaves double precision floating-point values low high quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.66.0F.WIG 14 /r	VUNPCKLPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vunpcklpd		Unpacks and Interleaves double precision floating-point values low high quadwords of ymm2 and ymm3/m256.
														
0F 14 /r	UNPCKLPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	unpcklps		Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm1 and xmm2/mem into xmm1.
VEX.NDS.128.0F.WIG 14 /r	VUNPCKLPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vunpcklps		Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/m128.
VEX.NDS.256.0F.WIG 14 /r	VUNPCKLPS ymm1, ymm2,ymm3/m256	RVM	W, R, R						V	V	AVX	vunpcklps		Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/m256.
														
VEX.128.66.0F38.W0 18 /r	VBROADCASTSS xmm1, m32	RM	Z, R						I	V	AVX	vbroadcastss		Broadcast single-precision floating-point element in mem to four locations in xmm1.
VEX.256.66.0F38.W0 18 /r	VBROADCASTSS ymm1, m32	RM	W, R						V	V	AVX	vbroadcastss		Broadcast single-precision floating-point element in mem to eight locations in ymm1.
VEX.256.66.0F38.W0 19 /r	VBROADCASTSD ymm1, m64	RM	W, R						V	V	AVX	vbroadcastsd		Broadcast double-precision floating-point element in mem to four locations in ymm1.
VEX.256.66.0F38.W0 1A /r	VBROADCASTF128 ymm1, m128	RM	W, R						V	V	AVX	vbroadcastf128		Broadcast 128 bits of floating-point data in mem to low and high 128-bits in ymm1.
VEX.128.66.0F38.W0 18 /r	VBROADCASTSS xmm1, xmm2	RM	Z, R						V	V	AVX2	vbroadcastss		Broadcast the low single-precision floating- point element in the source operand to four locations in xmm1.
VEX.256.66.0F38.W0 18 /r	VBROADCASTSS ymm1, xmm2	RM	W, R						V	V	AVX2	vbroadcastss		Broadcast low single-precision floating-point element in the source operand to eight locations in ymm1.
VEX.256.66.0F38.W0 19 /r	VBROADCASTSD ymm1, xmm2	RM	W, R						V	V	AVX2	vbroadcastsd		Broadcast low double-precision floating-point element in the source operand to four locations in ymm1.
														
VEX.256.66.0F38.W0 13 /r	VCVTPH2PS ymm1, xmm2/m128	RM	W, R	M.RC					V	V	F16C	vcvtph2ps		Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point value in ymm1.
VEX.128.66.0F38.W0 13 /r	VCVTPH2PS xmm1, xmm2/m64	RM	Z, R	M.RC					V	V	F16C	vcvtph2ps		Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point value in xmm1.
														
VEX.256.66.0F3A.W0 1D /r ib	VCVTPS2PH xmm1/m128, ymm2, imm8	MR	Z, R, R	M.RC					V	V	F16C	vcvtps2ph		Convert eight packed single-precision floating-point value in ymm2 to packed half-precision (16-bit) floating-point value in xmm1/mem. Imm8 provides rounding controls.
VEX.128.66.0F3A.W0 1D /r ib	VCVTPS2PH xmm1/m64, xmm2, imm8	MR	Z, R, R	M.RC					V	V	F16C	vcvtps2ph		Convert four packed single-precision float- ing-point value in xmm2 to packed half- precision (16-bit) floating-point value in xmm1/mem. Imm8 provides rounding con- trols.
														
0F 00 /4	VERR r/m16	M	R		E.ZF		YES	NO	V	V		verr		Set ZF=1 if segment specified with r/m16 can be read.
0F 00 /5	VERW r/m16	M	R		E.ZF		YES	NO	V	V		verw		Set ZF=1 if segment specified with r/m16 can be written.
														
VEX.256.66.0F3A.W0 19 /r ib	VEXTRACTF128 xmm1/m128, ymm2, imm8	MR	Z, R, R						V	V	AVX	vextractf128		Extract 128 bits of packed floating-point values from ymm2 and store results in xmm1/mem.
														
VEX.256.66.0F3A.W0 39 /r ib	VEXTRACTI128 xmm1/m128, ymm2, imm8	RMI	Z, R, R						V	V	AVX2	vextracti128		Extract 128 bits of integer data from ymm2 and store results in xmm1/mem.
														
VEX.DDS.128.66.0F38.W1 98 /r	VFMADD132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, add to xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 A8 /r	VFMADD213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, add to xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W1 B8 /r	VFMADD231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, add to xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W1 98 /r	VFMADD132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, add to ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 A8 /r	VFMADD213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, add to ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W1 B8 /r 	VFMADD231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, add to ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 98 /r 	VFMADD132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, add to xmm1 and put result in xmm0
VEX.DDS.128.66.0F38.W0 A8 /r	VFMADD213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, add to xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 B8 /r	VFMADD231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmadd231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, add to xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 98 /r	VFMADD132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, add to ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 A8 /r 	VFMADD213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, add to ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W0 B8 /r	VFMADD231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmadd231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, add to ymm0 and put result in ymm0.
														
VEX.DDS.LIG.128.66.0F38.W1 99 /r	VFMADD132SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmadd132sd		Multiply scalar double-precision floating-point value from xmm0 and xmm2/mem, add to xmm1 and put result in xmm0
VEX.DDS.LIG.128.66.0F38.W1 A9 /r	VFMADD213SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmadd213sd		Multiply scalar double-precision floating-point value from xmm0 and xmm1, add to xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 B9 /r	VFMADD231SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmadd231sd		Multiply scalar double-precision floating-point value from xmm1 and xmm2/mem, add to xmm0 and put result in xmm0.
														
VEX.DDS.LIG.128.66.0F38.W0 99 /r	VFMADD132SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmadd132ss		Multiply scalar single-precision floating-point value from xmm0 and xmm2/mem, add to xmm1 and put result in xmm0
VEX.DDS.LIG.128.66.0F38.W0 A9 /r	VFMADD213SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmadd213ss		Multiply scalar single-precision floating-point value from xmm0 and xmm1, add to xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 B9 /r	VFMADD231SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmadd231ss		Multiply scalar single-precision floating-point value from xmm1 and xmm2/mem, add to xmm0 and put result in xmm0.
														
VEX.DDS.128.66.0F38.W1 96 /r	VFMADDSUB132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, add/subtract elements in xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 A6 /r	VFMADDSUB213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, add/subtract elements in xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W1 B6 /r	VFMADDSUB231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, add/subtract elements in xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W1 96 /r	VFMADDSUB132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, add/subtract elements in ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 A6 /r	VFMADDSUB213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, add/subtract elements in ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W1 B6 /r	VFMADDSUB231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, add/subtract elements in ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 96 /r	VFMADDSUB132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, add/subtract elements in xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W0 A6 /r	VFMADDSUB213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, add/subtract elements in xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 B6 /r	VFMADDSUB231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmaddsub231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, add/subtract elements in xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 96 /r	VFMADDSUB132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, add/subtract elements in ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 A6 /r	VFMADDSUB213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, add/subtract elements in ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W0 B6 /r	VFMADDSUB231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmaddsub231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, add/subtract elements in ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W1 97 /r 	VFMSUBADD132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, subtract/add elements in xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 A7 /r	VFMSUBADD213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, subtract/add elements in xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W1 B7 /r	VFMSUBADD231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, subtract/add elements in xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W1 97 /r	VFMSUBADD132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, subtract/add elements in ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 A7 /r	VFMSUBADD213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, subtract/add elements in ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W1 B7 /r	VFMSUBADD231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, subtract/add elements in ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 97 /r	VFMSUBADD132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, subtract/add elements in xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W0 A7 /r	VFMSUBADD213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, subtract/add elements in xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 B7 /r	VFMSUBADD231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsubadd231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, subtract/add elements in xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 97 /r	VFMSUBADD132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, subtract/add elements in ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 A7 /r	VFMSUBADD213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, subtract/add elements in ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W0 B7 /r	VFMSUBADD231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsubadd231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, subtract/add elements in ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W1 9A /r	VFMSUB132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, subtract xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 AA /r	VFMSUB213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, subtract xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W1 BA /r	VFMSUB231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, subtract xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W1 9A /r 	VFMSUB132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, subtract ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 AA /r	VFMSUB213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, subtract ymm2/mem and put result in ymm0
VEX.DDS.256.66.0F38.W1 BA /r	VFMSUB231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, subtract ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 9A /r	VFMSUB132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, subtract xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W0 AA /r	VFMSUB213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, subtract xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 BA /r	VFMSUB231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfmsub231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, subtract xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 9A /r	VFMSUB132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, subtract ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 AA /r	VFMSUB213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, subtract ymm2/mem and put result in ymm0
VEX.DDS.256.66.0F38.W0 BA /r	VFMSUB231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfmsub231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, subtract ymm0 and put result in ymm0.
														
VEX.DDS.LIG.128.66.0F38.W1 9B /r	VFMSUB132SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmsub132sd		Multiply scalar double-precision floating-point value from xmm0 and xmm2/mem, subtract xmm1 and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 AB /r	VFMSUB213SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmsub213sd		Multiply scalar double-precision floating-point value from xmm0 and xmm1, subtract xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 BB /r	VFMSUB231SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfmsub231sd		Multiply scalar double-precision floating-point value from xmm1 and xmm2/mem, subtract xmm0 and put result in xmm0.
														
VEX.DDS.LIG.128.66.0F38.W0 9B /r	VFMSUB132SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmsub132ss		Multiply scalar single-precision floating-point value from xmm0 and xmm2/mem, subtract xmm1 and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 AB /r	VFMSUB213SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmsub213ss		Multiply scalar single-precision floating-point value from xmm0 and xmm1, subtract xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 BB /r 	VFMSUB231SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfmsub231ss		Multiply scalar single-precision floating-point value from xmm1 and xmm2/mem, subtract xmm0 and put result in xmm0.
														
VEX.DDS.128.66.0F38.W1 9C /r	VFNMADD132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, negate the multiplication result and add to xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 AC /r	VFNMADD213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, negate the multiplication result and add to xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W1 BC /r	VFNMADD231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, negate the multiplication result and add to xmm0 and put result in xmm0
VEX.DDS.256.66.0F38.W1 9C /r	VFNMADD132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, negate the multiplication result and add to ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 AC /r	VFNMADD213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, negate the multiplication result and add to ymm2/mem and put result in ymm0
VEX.DDS.256.66.0F38.W1 BC /r	VFNMADD231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, negate the multiplication result and add to ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 9C /r	VFNMADD132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, negate the multiplication result and add to xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W0 AC /r	VFNMADD213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, negate the multiplication result and add to xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 BC /r	VFNMADD231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmadd231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, negate the multiplication result and add to xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 9C /r 	VFNMADD132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, negate the multiplication result and add to ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 AC /r	VFNMADD213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, negate the multiplication result and add to ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.0 BC /r	VFNMADD231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmadd231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, negate the multiplication result and add to ymm0 and put result in ymm0.
														
VEX.DDS.LIG.128.66.0F38.W1 9D /r	VFNMADD132SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmadd132sd		Multiply scalar double-precision floating-point value from xmm0 and xmm2/mem, negate the multiplication result and add to xmm1 and put result in xmm0
VEX.DDS.LIG.128.66.0F38.W1 AD /r 	VFNMADD213SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmadd213sd		Multiply scalar double-precision floating-point value from xmm0 and xmm1, negate the multiplication result and add to xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 BD /r 	VFNMADD231SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmadd231sd		Multiply scalar double-precision floating-point value from xmm1 and xmm2/mem, negate the multiplication result and add to xmm0 and put result in xmm0.
														
VEX.DDS.LIG.128.66.0F38.W0 9D /r	VFNMADD132SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmadd132ss		Multiply scalar single-precision floating-point value from xmm0 and xmm2/mem, negate the multiplication result and add to xmm1 and put result in xmm0
VEX.DDS.LIG.128.66.0F38.W0 AD /r	VFNMADD213SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmadd213ss		Multiply scalar single-precision floating-point value from xmm0 and xmm1, negate the multiplication result and add to xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 BD /r	VFNMADD231SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmadd231ss		Multiply scalar single-precision floating-point value from xmm1 and xmm2/mem, negate the multiplication result and add to xmm0 and put result in xmm0.
														
VEX.DDS.128.66.0F38.W1 9E /r	VFNMSUB132PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub132pd		Multiply packed double-precision floating-point values from xmm0 and xmm2/mem, negate the multiplication result and subtract xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W1 AE /r	VFNMSUB213PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub213pd		Multiply packed double-precision floating-point values from xmm0 and xmm1, negate the multiplication result and subtract xmm2/mem and put result in xmm0
VEX.DDS.128.66.0F38.W1 BE /r	VFNMSUB231PD xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub231pd		Multiply packed double-precision floating-point values from xmm1 and xmm2/mem, negate the multiplication result and subtract xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W1 9E /r	VFNMSUB132PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub132pd		Multiply packed double-precision floating-point values from ymm0 and ymm2/mem, negate the multiplication result and subtract ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W1 AE /r	VFNMSUB213PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub213pd		Multiply packed double-precision floating-point values from ymm0 and ymm1, negate the multiplication result and subtract ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.W1 BE /r	VFNMSUB231PD ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub231pd		Multiply packed double-precision floating-point values from ymm1 and ymm2/mem, negate the multiplication result and subtract ymm0 and put result in ymm0.
														
VEX.DDS.128.66.0F38.W0 9E /r	VFNMSUB132PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub132ps		Multiply packed single-precision floating-point values from xmm0 and xmm2/mem, negate the multiplication result and subtract xmm1 and put result in xmm0.
VEX.DDS.128.66.0F38.W0 AE /r	VFNMSUB213PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub213ps		Multiply packed single-precision floating-point values from xmm0 and xmm1, negate the multiplication result and subtract xmm2/mem and put result in xmm0.
VEX.DDS.128.66.0F38.W0 BE /r	VFNMSUB231PS xmm0, xmm1, xmm2/m128	RVM	RZ, R, R						V	V	FMA	vfnmsub231ps		Multiply packed single-precision floating-point values from xmm1 and xmm2/mem, negate the multiplication result and subtract xmm0 and put result in xmm0.
VEX.DDS.256.66.0F38.W0 9E /r	VFNMSUB132PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub132ps		Multiply packed single-precision floating-point values from ymm0 and ymm2/mem, negate the multiplication result and subtract ymm1 and put result in ymm0.
VEX.DDS.256.66.0F38.W0 AE /r	VFNMSUB213PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub213ps		Multiply packed single-precision floating-point values from ymm0 and ymm1, negate the multiplication result and subtract ymm2/mem and put result in ymm0.
VEX.DDS.256.66.0F38.0 BE /r	VFNMSUB231PS ymm0, ymm1, ymm2/m256	RVM	RW, R, R						V	V	FMA	vfnmsub231ps		Multiply packed single-precision floating-point values from ymm1 and ymm2/mem, negate the multiplication result and subtract ymm0 and put result in ymm0.
														
VEX.DDS.LIG.128.66.0F38.W1 9F /r	VFNMSUB132SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmsub132sd		Multiply scalar double-precision floating-point value from xmm0 and xmm2/mem, negate the multiplication result and subtract xmm1 and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 AF /r	VFNMSUB213SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmsub213sd		Multiply scalar double-precision floating-point value from xmm0 and xmm1, negate the multiplication result and subtract xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W1 BF /r	VFNMSUB231SD xmm0, xmm1, xmm2/m64	RVM	RZ, R, R						V	V	FMA	vfnmsub231sd		Multiply scalar double-precision floating-point value from xmm1 and xmm2/mem, negate the multiplication result and subtract xmm0 and put result in xmm0.
														
VEX.DDS.LIG.128.66.0F38.W0 9F /r	VFNMSUB132SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmsub132ss		Multiply scalar single-precision floating-point value from xmm0 and xmm2/mem, negate the multiplication result and subtract xmm1 and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 AF /r	VFNMSUB213SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmsub213ss		Multiply scalar single-precision floating-point value from xmm0 and xmm1, negate the multiplication result and subtract xmm2/mem and put result in xmm0.
VEX.DDS.LIG.128.66.0F38.W0 BF /r	VFNMSUB231SS xmm0, xmm1, xmm2/m32	RVM	RZ, R, R						V	V	FMA	vfnmsub231ss		Multiply scalar single-precision floating-point value from xmm1 and xmm2/mem, negate the multiplication result and subtract xmm0 and put result in xmm0.
														
VEX.DDS.128.66.0F38.W1 92 /r	VGATHERDPD xmm1, vm32x, xmm2	RMV	Z, R, R						V	V	AVX2	vgatherdpd		Using dword indices specified in vm32x, gather double-precision FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.128.66.0F38.W1 93 /r	VGATHERQPD xmm1, vm64x, xmm2	RMV	Z, R, R						V	V	AVX2	vgatherqpd		Using qword indices specified in vm64x, gather double-precision FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.256.66.0F38.W1 92 /r	VGATHERDPD ymm1, vm32x, ymm2	RMV	W, R, R						V	V	AVX2	vgatherdpd		Using dword indices specified in vm32x, gather double-precision FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VEX.DDS.256.66.0F38.W1 93 /r	VGATHERQPD ymm1, vm64y, ymm2	RMV	W, R, R						V	V	AVX2	vgatherqpd		Using qword indices specified in vm64y, gather double-precision FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
														
VEX.DDS.128.66.0F38.W0 92 /r	VGATHERDPS xmm1, vm32x, xmm2	RMV	Z, R, R						V	V	AVX2	vgatherdps		Using dword indices specified in vm32x, gather single-precision FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.128.66.0F38.W0 93 /r	VGATHERQPS xmm1, vm64x, xmm2	RMV	Z, R, R						V	V	AVX2	vgatherqps		Using qword indices specified in vm64x, gather single-precision FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.256.66.0F38.W0 92 /r	VGATHERDPS ymm1, vm32y, ymm2	RMV	W, R, R						V	V	AVX2	vgatherdps		Using dword indices specified in vm32x, gather single-precision FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VEX.DDS.256.66.0F38.W0 93 /r	VGATHERQPS xmm1, vm64y, xmm2	RMV	Z, R, R						V	V	AVX2	vgatherqps		Using qword indices specified in vm64y, gather single-precision FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
														
VEX.DDS.128.66.0F38.W0 90 /r	VPGATHERDD xmm1, vm32x, xmm2	RMV	Z, R, R						V	V	AVX2	vpgatherdd		Using dword indices specified in vm32x, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.128.66.0F38.W0 91 /r	VPGATHERQD xmm1, vm64x, xmm2	RMV	Z, R, R						V	V	AVX2	vpgatherqd		Using qword indices specified in vm64x, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.256.66.0F38.W0 90 /r	VPGATHERDD ymm1, vm32y, ymm2	RMV	W, R, R						V	V	AVX2	vpgatherdd		Using dword indices specified in vm32y, gather dword from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VEX.DDS.256.66.0F38.W0 91 /r	VPGATHERQD xmm1, vm64y, xmm2	RMV	Z, R, R						V	V	AVX2	vpgatherqd		Using qword indices specified in vm64y, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
														
VEX.DDS.128.66.0F38.W1 90 /r	VPGATHERDQ xmm1, vm32x, xmm2	RMV	Z, R, R						V	V	AVX2	vpgatherdq		Using dword indices specified in vm32x, gather qword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.128.66.0F38.W1 91 /r	VPGATHERQQ xmm1, vm64x, xmm2	RMV	Z, R, R						V	V	AVX2	vpgatherqq		Using qword indices specified in vm64x, gather qword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VEX.DDS.256.66.0F38.W1 90 /r	VPGATHERDQ ymm1, vm32x, ymm2	RMV	W, R, R						V	V	AVX2	vpgatherdq		Using dword indices specified in vm32x, gather qword values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VEX.DDS.256.66.0F38.W1 91 /r	VPGATHERQQ ymm1, vm64y, ymm2	RMV	W, R, R						V	V	AVX2	vpgatherqq		Using qword indices specified in vm64y, gather qword values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
														
VEX.NDS.256.66.0F3A.W0 18 /r ib	VINSERTF128 ymm1, ymm2, xmm3/m128, imm8	RVMI	W, R, R, R						V	V	AVX	vinsertf128		Insert a single precision floating-point value selected by imm8 from xmm3/m128 into ymm2 at the specified destination element specified by imm8 and zero out destination elements in ymm1 as indicated in imm8.
														
VEX.NDS.256.66.0F3A.W0 38 /r ib	VINSERTI128 ymm1, ymm2, xmm3/m128, imm8	RVMI	W, R, R, R						V	V	AVX2	vinserti128		Insert 128-bits of integer data from xmm3/mem and the remaining values from ymm2 into ymm1.
														
VEX.NDS.128.66.0F38.W0 2C /r	VMASKMOVPS xmm1, xmm2, m128	RVM	Z, R, R						V	V	AVX	vmaskmovps		Conditionally load packed single-precision values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W0 2C /r	VMASKMOVPS ymm1, ymm2, m256	RVM	W, R, R						V	V	AVX	vmaskmovps		Conditionally load packed single-precision values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W0 2D /r	VMASKMOVPD xmm1, xmm2, m128	RVM	Z, R, R						V	V	AVX	vmaskmovpd		Conditionally load packed double-precision values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W0 2D /r	VMASKMOVPD ymm1, ymm2, m256	RVM	W, R, R						V	V	AVX	vmaskmovpd		Conditionally load packed double-precision values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W0 2E /r	VMASKMOVPS m128, xmm1, xmm2	MVR	W, R, R						V	V	AVX	vmaskmovps		Conditionally store packed single-precision values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W0 2E /r	VMASKMOVPS m256, ymm1, ymm2	MVR	W, R, R						V	V	AVX	vmaskmovps		Conditionally store packed single-precision values from ymm2 using mask in ymm1.
VEX.NDS.128.66.0F38.W0 2F /r	VMASKMOVPD m128, xmm1, xmm2	MVR	W, R, R						V	V	AVX	vmaskmovpd		Conditionally store packed double-precision values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W0 2F /r	VMASKMOVPD m256, ymm1, ymm2	MVR	W, R, R						V	V	AVX	vmaskmovpd		Conditionally store packed double-precision values from ymm2 using mask in ymm1.
														
VEX.NDS.128.66.0F3A.W0 02 /r ib	VPBLENDD xmm1, xmm2, xmm3/m128, imm8	RVMI	Z, R, R, R						V	V	AVX2	vpblendd		Select dwords from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VEX.NDS.256.66.0F3A.W0 02 /r ib	VPBLENDD ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX2	vpblendd		Select dwords from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
														
VEX.128.66.0F38.W0 78 /r	VPBROADCASTB xmm1, xmm2/m8	RM	Z, R						V	V	AVX2	vpbroadcastb		Broadcast a byte integer in the source operand to sixteen locations in xmm1.
VEX.256.66.0F38.W0 78 /r	VPBROADCASTB ymm1, xmm2/m8	RM	W, R						V	V	AVX2	vpbroadcastb		Broadcast a byte integer in the source operand to thirty two locations in ymm1.
VEX.128.66.0F38.W0 79 /r	VPBROADCASTW xmm1, xmm2/m16	RM	Z, R						V	V	AVX2	vpbroadcastw		Broadcast a word integer in the source operand to eight locations in xmm1.
VEX.256.66.0F38.W0 79 /r	VPBROADCASTW ymm1, xmm2/m16	RM	W, R						V	V	AVX2	vpbroadcastw		Broadcast a word integer in the source operand to sixteen locations in ymm1.
VEX.128.66.0F38.W0 58 /r	VPBROADCASTD xmm1, xmm2/m32	RM	Z, R						V	V	AVX2	vpbroadcastd		Broadcast a dword integer in the source operand to four locations in xmm1.
VEX.256.66.0F38.W0 58 /r	VPBROADCASTD ymm1, xmm2/m32	RM	W, R						V	V	AVX2	vpbroadcastd		Broadcast a dword integer in the source operand to eight locations in ymm1.
VEX.128.66.0F38.W0 59 /r	VPBROADCASTQ xmm1, xmm2/m64	RM	Z, R						V	V	AVX2	vpbroadcastq		Broadcast a qword element in mem to two locations in xmm1
VEX.256.66.0F38.W0 59 /r	VPBROADCASTQ ymm1, xmm2/m64	RM	W, R						V	V	AVX2	vpbroadcastq		Broadcast a qword element in mem to four locations in ymm1
VEX.256.66.0F38.W0 5A /r	VBROADCASTI128 ymm1, m128	RM	W, R						V	V	AVX2	vpbroadcasti128		Broadcast 128 bits of integer data in mem to low and high 128-bits in ymm1.
														
VEX.NDS.256.66.0F38.W0 36 /r	VPERMD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpermd		Permute doublewords in ymm3/m256 using indexes in ymm2 and store the result in ymm1.
														
VEX.256.66.0F3A.W1 01 /r ib	VPERMPD ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX2	vpermpd		Permute double-precision floating-point elements in ymm2/m256 using indexes in imm8 and store the result in ymm1.
														
VEX.NDS.256.66.0F38.W0 16 /r	VPERMPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpermps		Permute single-precision floating-point elements in ymm3/m256 using indexes in ymm2 and store the result in ymm1.
														
VEX.256.66.0F3A.W1 00 /r ib	VPERMQ ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX2	vpermq		Permute qwords in ymm2/m256 using indexes in imm8 and store the result in ymm1.
														
VEX.NDS.256.66.0F3A.W0 46 /r ib	VPERM2I128 ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX2	vperm2i128		Permute 128-bit integer data in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
														
VEX.NDS.128.66.0F38.W0 0D /r	VPERMILPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpermilpd		Permute double-precision floating-point values in xmm2 using controls from xmm3/mem and store result in xmm1.
VEX.NDS.256.66.0F38.W0 0D /r	VPERMILPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vpermilpd		Permute double-precision floating-point values in ymm2 using controls from ymm3/mem and store result in ymm1.
VEX.128.66.0F3A.W0 05 /r ib	VPERMILPD xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AVX	vpermilpd		Permute double-precision floating-point values in xmm2/mem using controls from imm8.
VEX.256.66.0F3A.W0 05 /r ib	VPERMILPD ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX	vpermilpd		Permute double-precision floating-point values in ymm2/mem using controls from imm8.
														
VEX.NDS.128.66.0F38.W0 0C /r	VPERMILPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vpermilps		Permute single-precision floating-point values in xmm2 using controls from xmm3/mem and store result in xmm1.
VEX.128.66.0F3A.W0 04 /r ib	VPERMILPS xmm1, xmm2/m128, imm8	RMI	Z, R, R						V	V	AVX	vpermilps		Permute single-precision floating-point values in xmm2/mem using controls from imm8 and store result in xmm1.
VEX.NDS.256.66.0F38.W0 0C /r	VPERMILPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vpermilps		Permute single-precision floating-point values in ymm2 using controls from ymm3/mem and store result in ymm1.
VEX.256.66.0F3A.W0 04 /r ib	VPERMILPS ymm1, ymm2/m256, imm8	RMI	W, R, R						V	V	AVX	vpermilps		Permute single-precision floating-point values in ymm2/mem using controls from imm8 and store result in ymm1.
														
VEX.NDS.256.66.0F3A.W0 06 /r ib	VPERM2F128 ymm1, ymm2, ymm3/m256, imm8	RVMI	W, R, R, R						V	V	AVX	vperm2f128		Permute 128-bit floating-point fields in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
														
VEX.NDS.128.66.0F38.W0 8C /r	VPMASKMOVD xmm1, xmm2, m128	RVM	Z, R, R						V	V	AVX2	vpmaskmovd		Conditionally load dword values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W0 8C /r	VPMASKMOVD ymm1, ymm2, m256	RVM	W, R, R						V	V	AVX2	vpmaskmovd		Conditionally load dword values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W1 8C /r	VPMASKMOVQ xmm1, xmm2, m128	RVM	Z, R, R						V	V	AVX2	vpmaskmovq		Conditionally load qword values from m128 using mask in xmm2 and store in xmm1.
VEX.NDS.256.66.0F38.W1 8C /r	VPMASKMOVQ ymm1, ymm2, m256	RVM	W, R, R						V	V	AVX2	vpmaskmovq		Conditionally load qword values from m256 using mask in ymm2 and store in ymm1.
VEX.NDS.128.66.0F38.W0 8E /r	VPMASKMOVD m128, xmm1, xmm2	MVR	W, R, R						V	V	AVX2	vpmaskmovd		Conditionally store dword values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W0 8E /r	VPMASKMOVD m256, ymm1, ymm2	MVR	W, R, R						V	V	AVX2	vpmaskmovd		Conditionally store dword values from ymm2 using mask in ymm1.
VEX.NDS.128.66.0F38.W1 8E /r	VPMASKMOVQ m128, xmm1, xmm2	MVR	W, R, R						V	V	AVX2	vpmaskmovq		Conditionally store qword values from xmm2 using mask in xmm1.
VEX.NDS.256.66.0F38.W1 8E /r	VPMASKMOVQ m256, ymm1, ymm2	MVR	W, R, R						V	V	AVX2	vpmaskmovq		Conditionally store qword values from ymm2 using mask in ymm1.
														
VEX.NDS.128.66.0F38.W0 47 /r	VPSLLVD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX2	vpsllvd		Shift bits in doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VEX.NDS.128.66.0F38.W1 47 /r	VPSLLVQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX2	vpsllvq		Shift bits in quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VEX.NDS.256.66.0F38.W0 47 /r	VPSLLVD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsllvd		Shift bits in doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VEX.NDS.256.66.0F38.W1 47 /r	VPSLLVQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsllvq		Shift bits in quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
														
VEX.NDS.128.66.0F38.W0 46 /r	VPSRAVD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX2	vpsravd		Shift bits in doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in the sign bits.
VEX.NDS.256.66.0F38.W0 46 /r	VPSRAVD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsravd		Shift bits in doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in the sign bits.
														
VEX.NDS.128.66.0F38.W0 45 /r	VPSRLVD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX2	vpsrlvd		Shift bits in doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VEX.NDS.128.66.0F38.W1 45 /r	VPSRLVQ xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX2	vpsrlvq		Shift bits in quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VEX.NDS.256.66.0F38.W0 45 /r	VPSRLVD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsrlvd		Shift bits in doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VEX.NDS.256.66.0F38.W1 45 /r	VPSRLVQ ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX2	vpsrlvq		Shift bits in quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
														
VEX.128.66.0F38.W0 0E /r	VTESTPS xmm1, xmm2/m128	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vtestps		Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating- point sources.
VEX.256.66.0F38.W0 0E /r	VTESTPS ymm1, ymm2/m256	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vtestps		Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating- point sources.
VEX.128.66.0F38.W0 0F /r	VTESTPD xmm1, xmm2/m128	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vtestpd		Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating- point sources.
VEX.256.66.0F38.W0 0F /r	VTESTPD ymm1, ymm2/m256	RM	R, R		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V	AVX	vtestpd		Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating- point sources.
														
VEX.256.0F.WIG 77	VZEROALL	NP			YMM*				V	V	AVX	vzeroall		Zero all YMM registers.
														
VEX.128.0F.WIG 77	VZEROUPPER	NP			YMM*				V	V	AVX	vzeroupper		Zero upper 128 bits of all YMM registers.
														
9B	WAIT	NP				S.C0 S.C1 S.C2 S.C3			V	V	FPU	wait		Check pending unmasked floating-point exceptions.
9B	FWAIT	NP				S.C0 S.C1 S.C2 S.C3			V	V	FPU	fwait		Check pending unmasked floating-point exceptions.
														
0F 09	WBINVD						NO	YES	V	V				Write back and flush Internal caches; initiate writing-back and flushing of external caches.
														
F3 0F AE /2	WRFSBASE r32	M	R						V	I	FSGSBASE	wrfsbase		Load the FS base address with the 32-bit value in the source register.
REX.W+ F3 0F AE /2	WRFSBASE r64	M	R						V	I	FSGSBASE	wrfsbase		Load the FS base address with the 64-bit value in the source register.
F3 0F AE /3	WRGSBASE r32	M	R						V	I	FSGSBASE	wrgsbase		Load the GS base address with the 32-bit value in the source register.
REX.W+ F3 0F AE /3	WRGSBASE r64	M	R						V	I	FSGSBASE	wrgsbase		Load the GS base address with the 64-bit value in the source register.
														
0F 30	WRMSR						NO	YES	V	V				Write the value in EDX:EAX to MSR specified by ECX.
														
F2	XACQUIRE	NP							V	V	HLE	xacquire		A hint used with an "XACQUIRE-enabled" instruction to start lock elision on the instruction memory operand address.
F3	XRELEASE	NP							V	V	HLE	xrelease		A hint used with an "XRELEASE-enabled" instruction to end lock elision on the instruction memory operand address.
														
C6 F8 ib	XABORT imm8	I	R						V	V	RTM	xabort		Causes an RTM abort if in RTM execution
														
0F C0 /r	XADD r/m8, r8	MR	RW, RW		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		xaddb		Exchange r8 and r/m8; load sum into r/m8.
REX+ 0F C0 /r	XADD r/m8, r8	MR	RW, RW		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		xaddb		Exchange r8 and r/m8; load sum into r/m8.
0F C1 /r	XADD r/m16, r16	MR	RW, RW		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		xaddw		Exchange r16 and r/m16; load sum into r/m16.
0F C1 /r	XADD r/m32, r32	MR	RW, RW		E.OF E.SF E.ZF E.AF E.CF E.PF				V	V		xaddl		Exchange r32 and r/m32; load sum into r/m32.
REX.W+ 0F C1 /r	XADD r/m64, r64	MR	RW, RW		E.OF E.SF E.ZF E.AF E.CF E.PF				V	NE		xaddq		Exchange r64 and r/m64; load sum into r/m64.
														
C7 F8	XBEGIN rel16	A	R						NS	V	RTM	xbegin		Specifies the start of an RTM region. Provides a 16-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
C7 F8	XBEGIN rel32	A	R						V	V	RTM	xbegin		Specifies the start of an RTM region. Provides a 32-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
														
90 +rw	XCHG AX, r16	O	RW, RW						V	V		xchgw		Exchange r16 with AX.
90 +rw	XCHG r16, AX	O	RW, RW						V	V		xchgw		Exchange AX with r16.
90 +rd	XCHG EAX, r32	O	RW, RW						V	V		xchgl		Exchange r32 with EAX.
REX.W+ 90 +rd	XCHG RAX, r64	O	RW, RW						V	NE		xchgq		Exchange r64 with RAX.
90 +rd	XCHG r32, EAX	O	RW, RW						V	V		xchgl		Exchange EAX with r32.
REX.W+ 90 +rd	XCHG r64, RAX	O	RW, RW						V	NE		xchgq		Exchange RAX with r64.
86 /r	XCHG r/m8, r8	MR	RW, RW						V	V		xchgb	YES	Exchange r8 (byte register) with byte from r/m8.
REX+ 86 /r	XCHG r/m8, r8	MR	RW, RW						V	NE		xchgb		Exchange r8 (byte register) with byte from r/m8.
86 /r	XCHG r8, r/m8	RM	RW, RW						V	V		xchgb		Exchange byte from r/m8 with r8 (byte register).
REX+ 86 /r	XCHG r8, r/m8	RM	RW, RW						V	NE		xchgb		Exchange byte from r/m8 with r8 (byte register).
87 /r	XCHG r/m16, r16	MR	RW, RW						V	V		xchgw	YES	Exchange r16 with word from r/m16.
87 /r	XCHG r16, r/m16	RM	RW, RW						V	V		xchgw		Exchange word from r/m16 with r16.
87 /r	XCHG r/m32, r32	MR	RW, RW						V	V		xchgl	YES	Exchange r32 with doubleword from r/m32.
REX.W+ 87 /r	XCHG r/m64, r64	MR	RW, RW						V	NE		xchgq	YES	Exchange r64 with quadword from r/m64.
87 /r	XCHG r32, r/m32	RM	RW, RW						V	V		xchgl		Exchange doubleword from r/m32 with r32.
REX.W+ 87 /r	XCHG r64, r/m64	RM	RW, RW						V	NE		xchgq		Exchange quadword from r/m64 with r64.
														
0F 01 D5	XEND	NP							V	V	RTM	xend		Specifies the end of an RTM code region.
														
0F 01 D0	XGETBV	NP		ECX	EAX EDX		YES	NO	V	V	XSAVE	xgetbv		Reads an XCR specified by ECX into EDX:EAX.
														
D7	XLAT m8	NP	I	AL DS BX ebx	AL				V	V		xlat		Set AL to memory byte DS:[(E)BX + unsigned AL].
D7	XLATB	NP		AL DS BX ebx	AL				V	V		xlatb	YES	Set AL to memory byte DS:[(E)BX + unsigned AL].
REX.W+ D7	XLATB	NP		AL RBX	AL				V	NE		xlatb		Set AL to memory byte [RBX + unsigned AL].
														
34 ib	XOR AL, imm8	I	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorb		AL XOR imm8.
35 iw	XOR AX, imm16	I	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorw		AX XOR imm16.
35 id	XOR EAX, imm32	I	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorl		EAX XOR imm32.
REX.W+ 35 id	XOR RAX, imm32	I	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorq		RAX XOR imm32 (sign-extended).
80 /6 ib	XOR r/m8, imm8	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorb		r/m8 XOR imm8.
REX+ 80 /6 ib	XOR r/m8, imm8	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorb		r/m8 XOR imm8.
81 /6 iw	XOR r/m16, imm16	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorw		r/m16 XOR imm16.
81 /6 id	XOR r/m32, imm32	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorl		r/m32 XOR imm32.
REX.W+ 81 /6 id	XOR r/m64, imm32	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorq		r/m64 XOR imm32 (sign-extended).
83 /6 ib	XOR r/m16, imm8	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorw	YES	r/m16 XOR imm8 (sign-extended).
83 /6 ib	XOR r/m32, imm8	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorl	YES	r/m32 XOR imm8 (sign-extended).
REX.W+ 83 /6 ib	XOR r/m64, imm8	MI	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorq	YES	r/m64 XOR imm8 (sign-extended).
30 /r	XOR r/m8, r8	MR	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorb	YES	r/m8 XOR r8.
REX+ 30 /r	XOR r/m8, r8	MR	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorb	YES	r/m8 XOR r8.
31 /r	XOR r/m16, r16	MR	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorw	YES	r/m16 XOR r16.
31 /r	XOR r/m32, r32	MR	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorl	YES	r/m32 XOR r32.
REX.W+ 31 /r	XOR r/m64, r64	MR	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorq	YES	r/m64 XOR r64.
32 /r	XOR r8, r/m8	RM	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorb		r8 XOR r/m8.
REX+ 32 /r	XOR r8, r/m8	RM	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorb		r8 XOR r/m8.
33 /r	XOR r16, r/m16	RM	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorw		r16 XOR r/m16.
33 /r	XOR r32, r/m32	RM	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	V		xorl		r32 XOR r/m32.
REX.W+ 33 /r	XOR r64, r/m64	RM	RW, R		E.OF E.SF E.ZF E.CF E.PF	E.AF			V	NE		xorq		r64 XOR r/m64.
														
66 0F 57 /r	XORPD xmm1, xmm2/m128	RM	RW, R						V	V	SSE2	xorpd		Bitwise exclusive-OR of xmm2/m128 and xmm1.
VEX.NDS.128.66.0F.WIG 57 /r	VXORPD xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vxorpd		Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.66.0F.WIG 57 /r	VXORPD ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vxorpd		Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/mem.
														
0F 57 /r	XORPS xmm1, xmm2/m128	RM	RW, R						V	V	SSE	xorps		Bitwise exclusive-OR of xmm2/m128 and xmm1.
VEX.NDS.128.0F.WIG 57 /r	VXORPS xmm1, xmm2, xmm3/m128	RVM	Z, R, R						V	V	AVX	vxorps		Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/mem.
VEX.NDS.256.0F.WIG 57 /r	VXORPS ymm1, ymm2, ymm3/m256	RVM	W, R, R						V	V	AVX	vxorps		Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/mem.
														
0F AE /5	XRSTOR mem	M	R	???	???	???			V	V	XSAVE	xrstor		Restore processor extended states from memory. The states are specified by EDX:EAX
REX.W+ 0F AE /5	XRSTOR64 mem	M	R	???	???	???			V	NE	XSAVE	xrstor64		Restore processor extended states from memory. The states are specified by EDX:EAX
														
0F AE /4	XSAVE mem	M	W	???	???	???			V	V	XSAVE	xsave		Save processor extended states to memory. The states are specified by EDX:EAX
REX.W+ 0F AE /4	XSAVE64 mem	M	W	???	???	???			V	NE	XSAVE	xsave64		Save processor extended states to memory. The states are specified by EDX:EAX
														
0F AE /6	XSAVEOPT mem	M	W	???	???	???			V	V	XSAVEOPT	xsaveopt		Save processor extended states specified in EDX:EAX to memory, optimizing the state save operation if possible.
REX.W+ 0F AE /6	XSAVEOPT64 mem	M	W	???	???	???			V	V	XSAVEOPT	xsaveopt64		Save processor extended states specified in EDX:EAX to memory, optimizing the state save operation if possible.
														
0F 01 D1	XSETBV						NO*	YES	V	V				Write the value in EDX:EAX to the XCR specified by ECX.
														
0F 01 D6	XTEST	NP							V	V	RTM	xtest		Test if executing in a transactional region
